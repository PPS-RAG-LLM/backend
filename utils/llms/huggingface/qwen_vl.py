from functools import lru_cache
from transformers import AutoProcessor, AutoModelForVision2Seq
from transformers.generation.streamers import TextIteratorStreamer
from qwen_vl_utils import process_vision_info
import torch
from utils import logger, free_torch_memory
from threading import Thread
from config import config

logger = logger(__name__)

# PyTorch Dynamo ë¹„í™œì„±í™” (í˜¸í™˜ì„± ì´ìŠˆ ìš°íšŒ)
torch._dynamo.config.disable = True


@lru_cache(maxsize=2)
def load_qwen3_vl_32b(model_dir):
    """
    Qwen3-VL-32B ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ë¡œë”© (ì´ë¯¸ì§€ + ë¹„ë””ì˜¤ + í…ìŠ¤íŠ¸)
    ì°¸ê³ : https://github.com/QwenLM/Qwen3-VL
    """
    logger.info(f"load Qwen3-VL-32B from `{model_dir}`")

    # Processor ë¡œë”© (ì´ë¯¸ì§€ + ë¹„ë””ì˜¤ + í…ìŠ¤íŠ¸ í†µí•© ì „ì²˜ë¦¬)
    processor = AutoProcessor.from_pretrained(
        model_dir,
        local_files_only=True,
        trust_remote_code=True,
    )

    # Vision-Language ëª¨ë¸ ë¡œë”© (AutoModelForVision2Seqê°€ ìë™ìœ¼ë¡œ Qwen3VL í´ë˜ìŠ¤ ì„ íƒ)
    model = AutoModelForVision2Seq.from_pretrained(
        model_dir,
        dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
        device_map="auto",
        local_files_only=True,
        trust_remote_code=True,
    )
    
    model.eval()
    return model, processor


def stream_chat(messages, **gen_kwargs):
    """
    Qwen3-VL ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… (ë©€í‹°ëª¨ë‹¬: ì´ë¯¸ì§€ + ë¹„ë””ì˜¤ + í…ìŠ¤íŠ¸)
    
    Args:
        messages: OpenAI í¬ë§· ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            ì˜ˆì‹œ: [{"role": "user", "content": [
                {"type": "image", "image": "url_or_path"},
                {"type": "text", "text": "ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"}
            ]}]
        **gen_kwargs: ìƒì„± íŒŒë¼ë¯¸í„° (temperature, top_p, model_path ë“±)
    
    Yields:
        str: ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì¡°ê°
    """
    logger.info(f"stream_chat: {gen_kwargs}")

    model_dir = gen_kwargs.get("model_path")
    if not model_dir:
        raise ValueError("ëˆ„ë½ëœ íŒŒë¼ë¯¸í„°: config.yamlì˜ model_path")

    model, processor = load_qwen3_vl_32b(model_dir)
    
    defaults = config.get("default", {}) or {}
    
    # 1ï¸âƒ£ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    # 2ï¸âƒ£ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ ì „ì²˜ë¦¬ (qwen_vl_utils ì‚¬ìš©)
    image_inputs, video_inputs = process_vision_info(messages)
    
    # 3ï¸âƒ£ ìµœì¢… ì…ë ¥ ì¤€ë¹„
    inputs = processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    ).to(model.device)
    
    # 4ï¸âƒ£ ìŠ¤íŠ¸ë¦¬ë¨¸ ìƒì„±
    streamer = TextIteratorStreamer(
        processor.tokenizer,
        skip_prompt=True,
        skip_special_tokens=True
    )

    generation_args = {
        **inputs,
        "temperature": gen_kwargs.get("temperature", defaults.get("temperature", 0.7)),
        "top_p": gen_kwargs.get("top_p", defaults.get("top_p", 0.8)),
        "top_k": defaults.get("top_k", 20),
        "max_new_tokens": defaults.get("max_tokens", 1024),
        "repetition_penalty": defaults.get("repetition_penalty", 1.0),
        "do_sample": True,
        "streamer": streamer,
    }

    try:
        thread = Thread(target=model.generate, kwargs=generation_args)
        thread.start()
        for text_token in streamer:
            if text_token:
                yield text_token
    finally:
        thread.join()
        free_torch_memory()


#####  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ (ë°˜ë“œì‹œ í•´ë‹¹ í´ë” ë‚´ì—ì„œ ì‹¤í–‰) #####
# python - <<'EOF'
# from transformers import AutoTokenizer, AutoProcessor, AutoModelForVision2Seq
# import torch

# model_id = "Qwen/Qwen3-VL-32B-Instruct"
# print(f"ğŸ”½ Downloading multimodal model: {model_id}")

# # 1ï¸âƒ£ Processor: (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ í†µí•© ì „ì²˜ë¦¬ê¸°)
# processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)

# # 2ï¸âƒ£ Vision-Language ëª¨ë¸ ë¡œë“œ
# model = AutoModelForVision2Seq.from_pretrained(
#     model_id,
#     trust_remote_code=True,
#     torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
#     device_map="auto",
# )

# # 3ï¸âƒ£ ì €ì¥ (í˜„ì¬ ë””ë ‰í† ë¦¬)
# processor.save_pretrained("./")
# model.save_pretrained("./")

# print("âœ… Done. Qwen3-VL-32B-Instruct ë‹¤ìš´ë¡œë“œ ì™„ë£Œ (í˜„ì¬ í´ë”ì— ì €ì¥ë¨)")
# EOF