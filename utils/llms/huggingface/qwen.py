from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.streamers import TextIteratorStreamer
# from transformers.generation.configuration_utils import GenerationConfig
from threading import Thread
from functools import lru_cache          # âœ… ëˆ„ë½ëœ ì„í¬íŠ¸ ì¶”ê°€
from config import config
# import time
from utils import logger, free_torch_memory
import torch

logger = logger(__name__)

@lru_cache(maxsize=2)
def load_qwen_instruct_7b(model_dir):
    logger.info(f"load qwen-instruct-model from `{model_dir}`")
    tokenizer = AutoTokenizer.from_pretrained(
        model_dir,
        local_files_only=True,
        trust_remote_code=True,
        use_fast=False,           # Qwen ê³„ì—´ì€ fast í† í¬ë‚˜ì´ì €ì—ì„œ í…œí”Œë¦¿ ì°¨ì´ê°€ ë‚˜ëŠ” ê²½ìš°ê°€ ìˆì–´ off ê¶Œì¥
    )
    # âœ… pad í† í° ë³´ì¥ (ì—†ìœ¼ë©´ EOSë¡œ ë§ì¶¤)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    # ğŸ¯ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ í˜¸í™˜ì„± ê°œì„  (FULL/LORA/QLORA ì§€ì›)
    model = AutoModelForCausalLM.from_pretrained(
        model_dir,
        device_map="auto",
        local_files_only=True,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,  # íŒŒì¸íŠœë‹ê³¼ ë™ì¼í•œ dtype ì‚¬ìš©
        low_cpu_mem_usage=True,      # ë©”ëª¨ë¦¬ ìµœì í™”
    )
    model.eval()
    return model, tokenizer

def stream_chat(messages, **gen_kwargs):
    logger.info(f"stream_chat: {gen_kwargs}")

    model_dir = gen_kwargs.get("model_path")
    if not model_dir:
        raise ValueError("ëˆ„ë½ëœ íŒŒë¼ë¯¸í„°: config.yamlì˜ model_path")

    model, tokenizer = load_qwen_instruct_7b(model_dir)

    # âœ… Qwen3 ê³„ì—´ ê¶Œì¥: chat í…œí”Œë¦¿ìœ¼ë¡œ ë°”ë¡œ ì¸ì½”ë”©
    #    add_generation_prompt=True ê°€ assistant ë‹µë³€ ì‹œì‘ í† í°ì„ ìë™ìœ¼ë¡œ ì¶”ê°€
    input_ids = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
    )
    
    # ğŸ¯ Multi-GPU ëª¨ë¸ ì²˜ë¦¬ ê°œì„ 
    try:
        # Multi-GPU ëª¨ë¸ì˜ ê²½ìš° ì²« ë²ˆì§¸ device ì‚¬ìš©
        if hasattr(model, 'hf_device_map') and model.hf_device_map:
            first_device = next(iter(model.hf_device_map.values()))
            input_ids = input_ids.to(first_device)
        else:
            input_ids = input_ids.to(model.device)
    except Exception:
        # Fallback: GPU 0 ì‚¬ìš©
        input_ids = input_ids.to("cuda:0" if torch.cuda.is_available() else "cpu")

    defaults = config.get("default", {}) or {}
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

    # ğŸ¯ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ í˜¸í™˜ generation parameters
    generation_args = {
        "input_ids": input_ids,
        "max_new_tokens": gen_kwargs.get("max_new_tokens", defaults.get("max_tokens", 512)),
        "do_sample": True,
        "temperature": gen_kwargs.get("temperature", defaults.get("temperature", 0.7)),
        "top_p": gen_kwargs.get("top_p", defaults.get("top_p", 0.8)),
        "repetition_penalty": gen_kwargs.get("repetition_penalty", defaults.get("repetition_penalty", 1.05)),
        "streamer": streamer,
        # âœ… íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì•ˆì •ì„± ê°œì„ 
        "eos_token_id": tokenizer.eos_token_id,
        "pad_token_id": tokenizer.pad_token_id,
        "use_cache": True,
    }
    
    # ğŸ›¡ï¸ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì—ì„œ ë¬¸ì œë˜ëŠ” íŒŒë¼ë¯¸í„° ì œê±°
    no_repeat_ngram = defaults.get("no_repeat_ngram_size", 0)
    if no_repeat_ngram and no_repeat_ngram > 0:
        generation_args["no_repeat_ngram_size"] = no_repeat_ngram

    # ğŸ¯ íŒŒì¸íŠœë‹ëœ Multi-GPU ëª¨ë¸ ì•ˆì •ì  ì²˜ë¦¬
    try:
        logger.debug(f"Starting generation with device: {input_ids.device}")
        thread = Thread(target=model.generate, kwargs=generation_args)
        thread.daemon = True  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ ê°™ì´ ì¢…ë£Œ
        thread.start()
        
        # ğŸ›¡ï¸ ë¬´í•œ ëŒ€ê¸° ë°©ì§€ë¥¼ ìœ„í•œ íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
        import time
        start_time = time.time()
        timeout_seconds = 300  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
        
        for text_token in streamer:
            if text_token:
                yield text_token
            
            # íƒ€ì„ì•„ì›ƒ ì²´í¬
            if time.time() - start_time > timeout_seconds:
                logger.warning(f"Generation timeout after {timeout_seconds}s - terminating")
                break
                
    except Exception as e:
        logger.error(f"Generation failed: {e}")
        yield f"[ì˜¤ë¥˜] ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    finally:
        try:
            # ìŠ¤ë ˆë“œ ì •ë¦¬ (íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜)
            thread.join(timeout=10)
            if thread.is_alive():
                logger.warning("Generation thread did not terminate cleanly")
        except Exception:
            pass
        free_torch_memory()


def build_prompt(messages):
    prompt = ""
    for msg in messages:
        if msg["role"] == "system":
            prompt += f"<|im_start|>system\n{msg['content']}<|im_end|>\n"
        elif msg["role"] == "user":
            prompt += f"<|im_start|>user\n{msg['content']}<|im_end|>\n"
        elif msg["role"] == "assistant":
            prompt += f"<|im_start|>assistant\n{msg['content']}<|im_end|>\n"
    prompt += "<|im_start|>assistant\n"  # ë‹µë³€ ì‹œì‘
    return prompt


if __name__ == "__main__":
    messages = [
        {"role": "system", "content": "You are a helpful assistant. Only Speak in Korean."},
        {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ ê¹€ë£¨ì•„ì…ë‹ˆë‹¤."},
        {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"},
        {"role": "user", "content": "ë‚´ì´ë¦„ì´ ë­ë¼ê³  í–ˆì§€?"},
    ]
    for chunk in stream_chat(messages):
        print(chunk, end="", flush=True)