
```
ssh -L 3007:localhost:3007 KT-Cloud

Funk-Enduring-Brutus-Thud-Absolument-Jeg

cd /home/work/CoreIQ/backend
conda activate coreIQ
uvicorn main:app --host 0.0.0.0 --port 3007 --reload

http://localhost:3007/docs

== 처음 설치 시 == 

conda create --name coreIQ python=3.13.5
conda activate coreIQ

pip install uv
uv pip install fastapi \
            streamlit \
            pymupdf \
            frontend \
            sentence_transformers\
            pandas\
            pyarrow\
            dill\
            aiohttp\
            numpy\
            accelerate\
            chardet\
            dotenv\
            bcrypt\
            pymilvus\
            uvicorn\
            sqlalchemy\
            tiktoken\
            python-multipart\
            unsloth

-- 안되면 실행 --
uv pip install --no-cache-dir "bitsandbytes==0.48.1"

# 2) bnb의 CUDA 버전 강제 해제(권장) 또는 torch와 맞추기(대안)
unset BNB_CUDA_VERSION
# (대안) torch와 맞추려면
# export BNB_CUDA_VERSION=128

# 3) CUDA 12.8 런타임 경로를 LD_LIBRARY_PATH에 추가(있으면 더 안전)
[ -d /usr/local/cuda-12.8/lib64 ] && export LD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:$LD_LIBRARY_PATH

# 4) (선택) transformers/accelerate도 coreIQ에 맞춰 정렬
pip install "transformers==4.55.0" "accelerate==1.10.0"

# 5) (강력 권장) PYTHONPATH 오염 제거
unset PYTHONPATH

# 6) 빠른 동작 확인
python - <<'PY'
import torch, bitsandbytes as bnb
print("torch.cuda.is_available():", torch.cuda.is_available(), "CUDA", torch.version.cuda)
print("bnb version:", bnb.__version__)
PY
--

pip install --user --pre pyhwp
conda install -y lxml
pip install --upgrade pyhwp olefile


conda install -c conda-forge pyyaml

pip install peft bitsandbytes accelerate
            
--upgrade --force-reinstall sentence-transformers
huggingface-hub==0.34.4

==
conda activate coreIQ
```

```
pip install transformers
oss 관련 
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

pip uninstall -y torch bitsandbytes
```



```
Qwen2.5-7B-Instruct-1M
gpt-oss-20b

bge_m3
qwen3_0_6b
qwen3_4b
```