from __future__ import annotations

from fastapi import APIRouter, Depends, Request, Body, status, Query, UploadFile, File, HTTPException, Form
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Literal
import json as _json
from collections import defaultdict
from pathlib import Path

from service.admin.manage_vator_DB import (
    TASK_TYPES,
    OverrideLevelsRequest,
    delete_collection,
    override_levels_and_ingest,
    # ì„¤ì •
    set_vector_settings,
    list_available_embedding_models,
    get_security_level_rules_all,
    upsert_security_level_for_task,
    get_security_level_rules_for_task,
    # íŒŒì´í”„ë¼ì¸
    ingest_embeddings,
    execute_search,
    # ê´€ë¦¬
    list_indexed_files,
    delete_files_by_names,
    # íŒŒì¼ ì €ì¥
    process_saved_raw_files,
)
from service.preprocessing.rag_preprocessing import extract_documents
from storage.db_models import DocumentType
from utils import logger
from utils.auth.session import get_user_id_from_cookie
from utils.documents import save_raw_file
from service.manage_documents.documents import upload_documents # [ì¶”ê°€] í†µí•© ì—…ë¡œë“œ í•¨ìˆ˜

router = APIRouter(
    prefix="/v1",
    tags=["Admin Document - RAG"],
    responses={
        status.HTTP_200_OK: {"description": "Successful Response"},
        status.HTTP_401_UNAUTHORIZED: {"description": "Unauthorized"},
        status.HTTP_403_FORBIDDEN: {"description": "Forbidden"},
        status.HTTP_404_NOT_FOUND: {"description": "Not found"},
    },
)
logger = logger(__name__)
# ============================
# Request/Response Models
# ============================
from config import config

ADMIN_RAW_DATA_DIR = Path(config.get("admin_raw_data_dir", "storage/raw_files/admin_raw_data"))

class VectorSettingsBody(BaseModel):
    embeddingModel: Optional[str] = Field(
        None,
        description="ì„ë² ë”© ëª¨ë¸ í‚¤ (ì˜ˆ: bge, embedding_bge_m3, qwen3_4b ë“±)"
    )
    searchType: Optional[Literal["hybrid", "semantic", "bm25"]] = Field(
        None,
        description="ê²€ìƒ‰ ë°©ì‹ (hybrid | semantic | bm25)"
    )
    chunkSize: Optional[int] = Field(
        None, ge=256, description="ì²­í¬ í† í° í¬ê¸° (ê¸°ë³¸ 512)"
    )
    overlap: Optional[int] = Field(
        None, ge=64, description="ì²­í¬ ê°„ ì˜¤ë²„ë© í† í° ìˆ˜ (ê¸°ë³¸ 64)"
    )


class TaskSecurityConfig(BaseModel):
    maxLevel: int = Field(..., ge=1, description="ìµœëŒ€ ë³´ì•ˆ ë ˆë²¨ (>=1)")
    # '@'ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë„ í—ˆìš©(ë ˆê±°ì‹œ í˜¸í™˜)
    levels: Dict[str, str | List[str]] = Field(
        default_factory=dict,
        description="ë ˆë²¨ë³„ í‚¤ì›Œë“œ ì„¤ì •. '@' ë¬¸ìì—´ ë˜ëŠ” í‚¤ì›Œë“œ ë°°ì—´ ëª¨ë‘ í—ˆìš©",
        examples=[{"1": "@ì¼ë°˜@ê³µê°œ", "2": "@ì—°êµ¬@ì—°ë´‰", "3": "@ë¶€ì •"}]
    )


class SecurityLevelsBody(BaseModel):
    service: Optional[str] = Field(default="global", description="ì„œë¹„ìŠ¤ ì´ë¦„(ë“œë¡­ë‹¤ìš´)")
    # ì‘ì—…ìœ í˜•ë³„(doc_gen, summary, qna) ë³´ì•ˆì„¤ì •
    doc_gen: TaskSecurityConfig
    summary: TaskSecurityConfig
    qna: TaskSecurityConfig


class ExecuteBody(BaseModel):
    question: str = Field(..., examples=["íšŒì‚¬ì˜ ë¶€ì •ì²­íƒ ì œë„ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"])
    topK: int = Field(50, gt=0, description="ì„ë² ë”© í›„ë³´ ê°œìˆ˜")
    rerank_topN: int = Field(5, gt=0, description="ë¦¬ë­í¬ í›„ ìµœì¢… ë°˜í™˜ ê°œìˆ˜")
    securityLevel: int = Field(1, ge=1)
    sourceFilter: Optional[List[str]] = None
    taskType: Literal["doc_gen", "summary", "qna"]
    searchMode: Optional[Literal["hybrid", "semantic", "bm25"]] = None
    
    model_config = {
        "json_schema_extra": {
            "example": {
                "question": "íšŒì‚¬ì˜ ë¶€ì •ì²­íƒ ì œë„ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
                "topK": 50,
                "rerank_topN": 5,
                "securityLevel": 3,
                "sourceFilter": ["íšŒì‚¬ê·œì •.pdf", "ë³µë¦¬í›„ìƒì•ˆë‚´.pdf"],
                "taskType": "qna",
                "searchMode": "hybrid",
            }
        }
    }


class SingleIngestBody(BaseModel):
    pdfPath: str
    taskTypes: Optional[List[Literal["doc_gen", "summary", "qna"]]] = None
    workspaceId: Optional[int] = None


class DeleteFilesBody(BaseModel):
    filesToDelete: List[str] = Field(
        ...,
        description="ì‚­ì œí•  íŒŒì¼ ì´ë¦„ ë°°ì—´ (ì˜ˆ: ['ì‚¬ê·œ.pdf','ë³´ë„ìë£Œ_20240101.pdf'])",
        examples=[["íšŒì‚¬ë‚´ê·œ.pdf", "20240835_ë³´ê³ ì„œ.pdf"]],
    )
    taskType: Optional[Literal["doc_gen", "summary", "qna"]] = Field(
        None,
        description="ì§€ì • ì‹œ í•´ë‹¹ ì‘ì—…ìœ í˜• ë°ì´í„°ë§Œ ì‚­ì œ. ë¯¸ì§€ì • ì‹œ ì „ì²´ ì‘ì—…ìœ í˜•ì—ì„œ ì‚­ì œ"
    )

    model_config = {
        "json_schema_extra": {
            "example": {
                "filesToDelete": [
                    "81._ë¶€ì •ì²­íƒë°ê¸ˆí’ˆë“±ìˆ˜ìˆ˜ì˜ì‹ ê³ ì‚¬ë¬´ì²˜ë¦¬ì—ê´€í•œë‚´ê·œ_20191128.pdf"
                ],
                "taskType": "qna"
            }
        }
    }


# ============================
# Vector Settings
# ============================

@router.post(
    "/admin/vector/settings",
    summary="0. ë²¡í„° ì„¤ì •(ëª¨ë¸/ê²€ìƒ‰/ì²­í¬) ì—…ë°ì´íŠ¸",
)
async def update_vector_settings(body: VectorSettingsBody):
    try:
        ret = set_vector_settings(
            embed_model_key=body.embeddingModel,
            search_type=body.searchType,
            chunk_size=body.chunkSize,
            overlap=body.overlap,
        )
        return {"message": "updated", **ret}

    except Exception as e:
        # ê¸°íƒ€ ì˜¤ë¥˜ (ëª¨ë¸ íŒŒì¼ ì—†ìŒ ë“±)
        return {"error": "ë°±í„° DBì„¤ì • ë¶ˆê°€(ë°±í„° DBë¥¼ ì „ë¶€ ì‚­ì œ)", "detail": str(e)}


from repository.rag_settings import get_rag_settings_row

@router.get(
    "/admin/vector/settings",
    summary="í˜„ì¬ ë²¡í„° ì„¤ì •(ì„ë² ë”© ëª¨ë¸/ê²€ìƒ‰ ë°©ì‹) ì¡°íšŒ",
)
async def read_vector_settings():
    row = get_rag_settings_row()
    return {
        "embeddingModel": row.get("embedding_key"),
        "searchType": row.get("search_type", "hybrid"),
        "chunkSize": int(row.get("chunk_size", 512)),
        "overlap": int(row.get("overlap", 64)),
    }


@router.get(
    "/admin/vector/embedding-models",
    summary="ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ ëª©ë¡ ì¡°íšŒ",
)
async def list_embedding_models():
    """
    ./storage/embedding-models í´ë” ë‚´ì˜ ëª¨ë¸ í´ë”ëª…ë“¤ì„ ë°˜í™˜.
    - embedding_ ì ‘ë‘ì‚¬ê°€ ìˆìœ¼ë©´ ì œê±° (ì˜ˆ: embedding_bge_m3 â†’ bge_m3)
    """
    models = list_available_embedding_models()
    return {
        "models": models,
        "count": len(models)
    }


# ============================
# Security Levels (per task type)
# ============================

from typing import List as _ListType, Dict as _DictType
from pydantic import conint

TaskLiteral = Literal["doc_gen", "summary", "qna"]

class SecurityLevelSingleBody(BaseModel):
    maxLevel: conint(ge=1) = Field(..., description="ìµœëŒ€ ë³´ì•ˆ ë ˆë²¨(>=1)")
    levels: _DictType[str, _ListType[str] | str] = Field(default_factory=dict)

@router.post(
    "/admin/vector/security-levels/{taskType}",
    summary="1. ì‘ì—…ìœ í˜•ë³„ ë³´ì•ˆë ˆë²¨ ê·œì¹™ 'ê°œë³„' ì €ì¥(doc_gen/summary/qna ì¤‘ í•˜ë‚˜)",
    status_code=status.HTTP_200_OK,
)
async def set_security_levels_one(taskType: TaskLiteral, body: SecurityLevelSingleBody):
    try:
        res = upsert_security_level_for_task(
            task_type=taskType,
            max_level=int(body.maxLevel),
            levels_raw=body.levels,
        )
        return res
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get(
    "/admin/vector/security-levels",
    summary="ë³´ì•ˆë ˆë²¨ ê·œì¹™ ì¡°íšŒ(ì „ì²´ ë˜ëŠ” íŠ¹ì • ì‘ì—…ìœ í˜•)"
)
async def get_security_levels(taskType: Optional[TaskLiteral] = None):
    if taskType:
        return get_security_level_rules_for_task(taskType)
    return get_security_level_rules_all()


# ============================
# Pipeline
# ============================

@router.post("/admin/vector/full-ingest", summary="ì „ì²´ íŒŒì¼ ì¶”ì¶œ ë° ì €ì¥ ì¸ì œìŠ¤íŠ¸") # TODO : MinIO ë§ˆì´ê·¸ë ˆì´ì…˜ í•„ìš”
async def rag_full_ingest(
    user_id: int = Depends(get_user_id_from_cookie), 
    files: List[UploadFile] = File(...)
    ):
    # 1) ì €ì¥ ê²½ë¡œ ì¤€ë¹„ (ê¸°ì¡´ ë¡œì§ ìœ ì§€: securityLevel1)
    target_folder = ADMIN_RAW_DATA_DIR / "securityLevel1"
    target_folder.mkdir(parents=True, exist_ok=True)

    raw_paths = []
    saved_original_names = []

    for f in files:
        filename = f.filename or "unknown"
        # upload_documents ë‚´ë¶€ì—ì„œ íŒŒì¼ì„ ì €ì¥í•˜ë¯€ë¡œ ê²½ë¡œë§Œ ì§€ì •
        file_path = target_folder / filename
        raw_paths.append(str(file_path))
        saved_original_names.append(filename)

    # 2) í†µí•© ì—…ë¡œë“œ í•¨ìˆ˜ í˜¸ì¶œ
    # ê¸°ì¡´ ë¡œì§ì´ securityLevel1 í´ë”ì— ì €ì¥í–ˆìœ¼ë¯€ë¡œ, ë³´ì•ˆ ë“±ê¸‰ì„ 1ë¡œ ê°•ì œ ì„¤ì •í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
    default_levels = {"qna": 1, "summary": 1, "doc_gen": 1}

    result = await upload_documents(
        user_id=user_id,
        files=files,
        raw_paths=raw_paths,
        add_to_workspaces=None,
        doc_type=DocumentType.ADMIN,  # ê´€ë¦¬ì ë¬¸ì„œ
        override_security_levels=default_levels
    )
    ingest_result = {"save": saved_original_names, "ingest": result}
    logger.info(f"[API] rag_full_ingest í˜¸ì¶œ ì™„ë£Œ, ê²°ê³¼ ingest_result=\n\n{ingest_result}\n")

    return ingest_result


@router.post("/admin/vector/execute",summary="ê´€ë¦¬ì ê²€ìƒ‰")
async def rag_search_endpoint(body: ExecuteBody):
    print(f"ğŸ¯ [API] ê´€ë¦¬ì ê²€ìƒ‰ ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ: question='{body.question}', topK={body.topK}, rerank_topN={body.rerank_topN}")
    
    model_key = get_rag_settings_row()["embedding_key"]
    print(f"ğŸ¯ [API] execute_search í˜¸ì¶œ ì‹œì‘...")
    
    result = await execute_search(
        question=body.question,
        top_k=body.topK,  # ì„ë² ë”© í›„ë³´ ê°œìˆ˜
        rerank_top_n=body.rerank_topN,  # ìµœì¢… ë°˜í™˜ ê°œìˆ˜
        security_level=body.securityLevel,
        source_filter=body.sourceFilter,
        task_type=body.taskType,
        model_key=model_key,
        search_type=body.searchMode,  # â† override
    )
    
    print(f"ğŸ¯ [API] execute_search í˜¸ì¶œ ì™„ë£Œ, ê²°ê³¼ hits={len(result.get('hits', []))}")
    return result


@router.post(
    "/user/vector/execute", summary="ì‚¬ìš©ì ê²€ìƒ‰"
)
async def user_rag_search_endpoint(body: ExecuteBody):
    model_key = get_rag_settings_row()["embedding_key"]
    return await execute_search(
        question=body.question,
        top_k=body.topK,  # ì„ë² ë”© í›„ë³´ ê°œìˆ˜
        rerank_top_n=body.rerank_topN,  # ìµœì¢… ë°˜í™˜ ê°œìˆ˜
        security_level=body.securityLevel,
        source_filter=body.sourceFilter,
        task_type=body.taskType,
        model_key=model_key,
        search_type=body.searchMode,
    )


# ============================
# Management
# ============================

@router.get(
    "/admin/vector/files",
    summary="ì¸ë±ì‹±ëœ íŒŒì¼ ëª©ë¡(ì‘ì—…ìœ í˜•ë³„ ì§‘ê³„) ì¡°íšŒ"
)
async def list_vector_files_endpoint(
    limit: int = Query(1000, ge=1, le=16384),
    offset: int = Query(0, ge=0),
    q: Optional[str] = Query(None, description="íŒŒì¼ëª… ë¶€ë¶„ê²€ìƒ‰"),
    taskType: Optional[Literal["doc_gen", "summary", "qna"]] = Query(None),
):
    return await list_indexed_files(limit=limit, offset=offset, query=q, task_type=taskType)


@router.get(
    "/admin/vector/files/overview",
    summary="ì‘ì—…ìœ í˜•Â·ë³´ì•ˆë ˆë²¨ë³„ ì§‘ê³„ + íŒŒì¼ ë¦¬ìŠ¤íŠ¸"
)
async def list_vector_files_overview():
    
    items = await list_indexed_files(limit=16384, offset=0, query=None, task_type=None)
    # agg: task_type -> level -> count
    agg: Dict[str, Dict[int, int]] = defaultdict(lambda: defaultdict(int))
    for it in items:
        agg[it["taskType"]][int(it["securityLevel"])] += it["chunkCount"]
    # ë³´ê¸° ì¢‹ê²Œ ë³€í™˜
    overview = {
        t: {str(lv): agg[t][lv] for lv in sorted(agg[t].keys())} for t in agg.keys()
    }
    return {"overview": overview, "items": items}


@router.delete(
    "/admin/vector/delete",
    summary="íŒŒì¼ ì´ë¦„ ëª©ë¡(doc_id ìŠ¤í…œ) ê¸°ë°˜ ì‚­ì œ. taskType ì§€ì • ì‹œ í•´ë‹¹ ì‘ì—…ìœ í˜•ë§Œ ì‚­ì œ"
)
async def delete_vector_files(body: DeleteFilesBody = Body(...)):
    return await delete_files_by_names(body.filesToDelete, task_type=body.taskType)


@router.post("/admin/vector/delete", summary="[POST] íŒŒì¼ ì´ë¦„ ëª©ë¡(doc_id ìŠ¤í…œ) ê¸°ë°˜ ì‚­ì œ. taskType ì§€ì • ì‹œ í•´ë‹¹ ì‘ì—…ìœ í˜•ë§Œ ì‚­ì œ")
async def delete_vector_files_post(body: DeleteFilesBody = Body(...)):
    return await delete_files_by_names(body.filesToDelete, task_type=body.taskType)


@router.post("/admin/vector/delete-admin-collection", summary="Milvus ì„œë²„ Admin ì»¬ë ‰ì…˜ ì‚­ì œ(ì´ˆê¸°í™”)")
async def rag_delete_admin_collection_endpoint(request: Request):
    logger.debug(f"[delete-admin-collection] from {request.client.host}")
    return await delete_collection(collection_key=DocumentType.ADMIN.value)

@router.post("/admin/vector/delete-all-collections", summary="Milvus ì„œë²„ ì „ì²´ ì»¬ë ‰ì…˜ ì‚­ì œ(ì´ˆê¸°í™”)")
async def rag_delete_db_endpoint(request: Request):
    logger.debug(f"[delete-all] from {request.client.host}")
    return await delete_collection(collection_key=None)


# ì¤‘ë³µ import ì œê±° ë° ìƒë‹¨ìœ¼ë¡œ ìŠ¹ê²©ë¨

def _parse_level_for_tasks_flex(
    raw: Optional[str],
    qna_level: Optional[str] = None,
    summary_level: Optional[str] = None,
    doc_gen_level: Optional[str] = None,
) -> Dict[str, int]:
    # 1) ê°œë³„ í•„ë“œê°€ ì˜¤ë©´ ìš°ì„  ì‚¬ìš© (ë¹ˆ ë¬¸ìì—´ì€ ë¬´ì‹œ)
    def _as_int(x: Optional[str]):
        if x is None:
            return None
        s = str(x).strip()
        if s == "":
            return None
        try:
            return int(s)
        except Exception:
            return None

    lvl_map: Dict[str, int] = {}
    q = _as_int(qna_level); s = _as_int(summary_level); d = _as_int(doc_gen_level)
    if q is not None: lvl_map["qna"] = max(1, q)
    if s is not None: lvl_map["summary"] = max(1, s)
    if d is not None: lvl_map["doc_gen"] = max(1, d)
    if lvl_map:
        return lvl_map

    if raw is None or str(raw).strip() == "":
        raise ValueError("level_for_tasks ê°’ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    s = str(raw).strip()

    # 2) ìˆ«ì í•˜ë‚˜ë§Œ ì˜¤ë©´ ëª¨ë“  task ë™ì¼ ì ìš©
    if s.isdigit():
        v = max(1, int(s))
        return {"qna": v, "summary": v, "doc_gen": v}

    # 3) JSON ì‹œë„
    try:
        obj = _json.loads(s)
        if isinstance(obj, dict):
            out = {}
            for k, v in obj.items():
                if k in ("qna", "summary", "doc_gen"):
                    out[k] = max(1, int(v))
            if out:
                return out
        elif isinstance(obj, list):
            # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° [1, 2, 3] -> qna=1, summary=2, doc_gen=3 (ìˆœì„œ ê°€ì •)
            # ë˜ëŠ” tasks ëª©ë¡ì„ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ì´ í•¨ìˆ˜ ë‹¨ë…ìœ¼ë¡œëŠ” ì²˜ë¦¬ê°€ ì–´ë µì§€ë§Œ
            # ì¼ë‹¨ ê°€ëŠ¥í•œ ê²½ìš°ë§Œ ì²˜ë¦¬
            # ì—¬ê¸°ì„œëŠ” task ìˆœì„œë¥¼ ê³ ì •(TASK_TYPES)í•œë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜, í˜¸ì¶œì²˜ì—ì„œ ì²˜ë¦¬í•´ì•¼ í•¨.
            # í•˜ì§€ë§Œ ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ [1,2,3] í˜•íƒœë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ ê°„ë‹¨íˆ ë§¤í•‘
            # (ì£¼ì˜: tasks íŒŒë¼ë¯¸í„°ì™€ ìˆœì„œê°€ ì¼ì¹˜í•œë‹¤ê³  ê°€ì •)
            pass 
    except Exception:
        pass

    # 4) "qna:2,summary:1" / "qna=2&summary=1" ë¥˜ íŒŒì‹±
    cand = s.replace("&", ",")
    parts = [p.strip() for p in cand.split(",") if p.strip()]
    out = {}
    for p in parts:
        if ":" in p:
            k, v = p.split(":", 1)
        elif "=" in p:
            k, v = p.split("=", 1)
        else:
            continue
        k = k.strip()
        if k in ("qna", "summary", "doc_gen"):
            try:
                out[k] = max(1, int(v))
            except Exception:
                pass
    if out:
        return out

    raise ValueError('level_for_tasks íŒŒì‹± ì‹¤íŒ¨. ì˜ˆ) {"qna":2,"summary":1} ë˜ëŠ” "qna:2,summary:1" ë˜ëŠ” "2"')
    
@router.post("/admin/vector/override-levels-upload", 
    summary="íŒŒì¼ ì—…ë¡œë“œ í›„ ë ˆë²¨ ì§€ì •í•˜ì—¬ ë°”ë¡œ ì „ì²˜ë¦¬ ë° ì¸ì œìŠ¤íŠ¸ (í†µí•© ì—…ë¡œë“œ ë°©ì‹)"
    )
async def override_levels_upload_form(
    user_id: int = Depends(get_user_id_from_cookie),
    files: List[UploadFile] = File(...),
    tasks: Optional[str] = Form(None),
    level_for_tasks: Optional[str] = Form(None),
    qna_level: Optional[str] = Form(None),
    summary_level: Optional[str] = Form(None),
    doc_gen_level: Optional[str] = Form(None),
):
    # 1) tasks, levels íŒŒì‹± (ì €ì¥ í´ë” ê²°ì •ì„ ìœ„í•´ ë¨¼ì € ìˆ˜í–‰)
    tlist = None
    if tasks:
        tlist = [t.strip() for t in tasks.split(",") if t.strip() in TASK_TYPES] or None

    lvmap = {}
    try:
        # [1,2,3] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´ ì²˜ë¦¬ ì‹œë„ (tasks ìˆœì„œì™€ ë§¤í•‘ ê°€ì •)
        s_lvl = str(level_for_tasks).strip()
        if s_lvl.startswith("[") and s_lvl.endswith("]"):
            try:
                lvl_arr = _json.loads(s_lvl)
                if isinstance(lvl_arr, list):
                    # tlistê°€ ìˆë‹¤ë©´ ìˆœì„œëŒ€ë¡œ ë§¤í•‘, ì—†ìœ¼ë©´ TASK_TYPES ìˆœì„œëŒ€ë¡œ? 
                    # ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: tasks=[qna, summary, doc_gen], level_for_tasks=[1,2,3]
                    # ë”°ë¼ì„œ tlistê°€ ìˆìœ¼ë©´ 1:1 ë§¤í•‘
                    mapping_target = tlist if tlist else TASK_TYPES
                    for i, t in enumerate(mapping_target):
                        if i < len(lvl_arr):
                             lvmap[t] = max(1, int(lvl_arr[i]))
            except Exception:
                pass

        if not lvmap:
            lvmap = _parse_level_for_tasks_flex(level_for_tasks, qna_level, summary_level, doc_gen_level)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    # ì €ì¥í•  í´ë” ê²°ì •: ì§€ì •ëœ ë ˆë²¨ ì¤‘ ê°€ì¥ ë†’ì€ ë ˆë²¨ í´ë”ì— ì €ì¥ (ë˜ëŠ” ë‹¨ì¼ ë ˆë²¨)
    effective_levels = [v for k, v in lvmap.items() if (not tlist) or (k in tlist)]
    max_lvl = max(effective_levels) if effective_levels else 1
    
    # 2) ì €ì¥ ê²½ë¡œ ì¤€ë¹„ (documents.pyì˜ upload_documentsëŠ” raw_pathsë¥¼ ë°›ìŒ)
    # ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼í•œ í´ë” êµ¬ì¡° ìœ ì§€: ADMIN_RAW_DATA_DIR / securityLevel{max_lvl}
    target_folder = ADMIN_RAW_DATA_DIR / f"securityLevel{max_lvl}"
    target_folder.mkdir(parents=True, exist_ok=True)

    raw_paths = []
    saved_original_names = []

    for f in files:
        filename = f.filename or "unknown"
        # upload_documents ë‚´ë¶€ì—ì„œ íŒŒì¼ì„ ì €ì¥í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ê²½ë¡œë§Œ ì§€ì •í•´ì¤Œ
        # íŒŒì¼ëª… ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ ê¸°ì¡´ save_raw_file ë¡œì§ì„ ë”°ë¥¼ ìˆ˜ë„ ìˆìœ¼ë‚˜,
        # upload_documentsëŠ” ì£¼ì–´ì§„ ê²½ë¡œì— íŒŒì¼ì„ ì”€.
        # ì—¬ê¸°ì„œëŠ” íŒŒì¼ëª… ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ í•„ìš”ì‹œ ì¤‘ë³µ ì²˜ë¦¬ í•„ìš”.
        # ì¼ë‹¨ íŒŒì¼ëª… ê·¸ëŒ€ë¡œ ì‚¬ìš©
        file_path = target_folder / filename
        raw_paths.append(str(file_path))
        saved_original_names.append(filename)
        
    # 3) í†µí•© ì—…ë¡œë“œ í•¨ìˆ˜ í˜¸ì¶œ
    # override_security_levels íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ ê°•ì œ ë ˆë²¨ ì ìš©
    result = await upload_documents(
        user_id=user_id,
        files=files,
        raw_paths=raw_paths,
        add_to_workspaces=None,
        doc_type=DocumentType.ADMIN,  # ê´€ë¦¬ì ë¬¸ì„œ
        override_security_levels=lvmap # ê°•ì œ ë ˆë²¨
    )
    
    return {"saved": saved_original_names, "ingest_result": result}
