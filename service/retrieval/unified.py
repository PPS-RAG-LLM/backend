"""í†µí•© ê²€ìƒ‰ ì—”ì§„."""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, Iterable, List, Tuple

from service.retrieval.adapters.base import RetrievalResult
from service.retrieval.adapters.local import LocalVectorAdapter
from service.retrieval.adapters.milvus import MilvusAdapter
from service.retrieval.adapters.workspace import WorkspaceAdapter
from service.retrieval.common import get_document_dirs
from service.retrieval.reranker import rerank_snippets
from utils import logger


LOGGER = logger(__name__)
DEFAULT_SOURCES = ("workspace", "local", "milvus")

def unified_search(query: str, config: Dict[str, Any]) -> List[RetrievalResult]:
    """
    ì—¬ëŸ¬ ê²€ìƒ‰ ì†ŒìŠ¤ë¥¼ í†µí•© í˜¸ì¶œ.
    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        config: ê²€ìƒ‰ ì„¤ì • (workspace_id, attachments, security_level ë“±)
    """
    logger.info(f"ğŸ” [UnifiedSearch] config: {config}")

    top_k = int(config.get("top_k"))
    threshold = float(config.get("threshold") or 0.0)
    sources = tuple(config.get("sources"))
    enable_rerank = bool(config.get("enable_rerank", False))
    rerank_top_n = int(config.get("rerank_top_n"))
    attachments = config.get("attachments") or []

    LOGGER.info(
        "[UnifiedSearch] query='%s' sources=%s top_k=%s workspace_id=%s attachments=%s",
        query,
        sources,
        top_k,
        config.get("workspace_id"),
        len(attachments),
    )

    results: List[RetrievalResult] = []

    if "workspace" in sources and config.get("workspace_id"):
        adapter = WorkspaceAdapter()
        workspace_hits = adapter.search(
            query,
            top_k,
            workspace_id=int(config["workspace_id"]),
            threshold=threshold,
        )
        LOGGER.info("[UnifiedSearch] workspace hits=%s", len(workspace_hits))
        results.extend(workspace_hits)
    elif "workspace" in sources:
        LOGGER.info("[UnifiedSearch] workspace source enabled but workspace_id missing")

    if "local" in sources:
        attachment_doc_ids = extract_doc_ids_from_attachments(config.get("attachments"))
        if attachment_doc_ids:
            adapter = LocalVectorAdapter()
            local_hits = adapter.search(
                query,
                top_k,
                doc_ids=attachment_doc_ids,
                threshold=threshold,
            )
            LOGGER.info(
                "[UnifiedSearch] local hits=%s (doc_ids=%s)",
                len(local_hits),
                attachment_doc_ids,
            )
            results.extend(local_hits)
        else:
            LOGGER.info("[UnifiedSearch] local source enabled but no attachment doc_ids")

    if "milvus" in sources:
        sec_level = int(config.get("security_level") or 1)
        adapter = MilvusAdapter()
        milvus_hits = adapter.search(
            query,
            top_k,
            security_level=sec_level,
            task_type=str(config.get("task_type") or "qna"),
            search_type=config.get("search_type"),
            model_key=config.get("model_key"),
            rerank_top_n=rerank_top_n,
        )
        LOGGER.info("[UnifiedSearch] milvus hits=%s", len(milvus_hits))
        results.extend(milvus_hits)

    if not results:
        LOGGER.info("[UnifiedSearch] no hits from any sources")
        return []

    merged = _deduplicate(results)
    LOGGER.info("[UnifiedSearch] merged hits=%s", len(merged))
    if enable_rerank and len(merged) > 1:
        reranked = rerank_snippets(merged, query=query, top_n=rerank_top_n)
        return reranked

    merged.sort(key=lambda r: r.score, reverse=True)
    return merged[: max(1, rerank_top_n if enable_rerank else top_k)]


# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------
def _deduplicate(results: Iterable[RetrievalResult]) -> List[RetrievalResult]:
    """
    doc_id + chunk_index ê¸°ì¤€ìœ¼ë¡œ dedup.
    ì ìˆ˜ê°€ ë” ë†’ì€ í•­ëª©ì„ ìœ ì§€í•œë‹¤.
    """
    dedup: Dict[Tuple[str, int], RetrievalResult] = {}
    for item in results:
        key = (str(item.doc_id or item.title), int(item.chunk_index or 0))
        if key not in dedup or item.score > dedup[key].score:
            dedup[key] = item
    return list(dedup.values())


def extract_doc_ids_from_attachments(attachments: Any) -> List[str]:
    """
    ì²¨ë¶€ íŒŒì¼ ë©”íƒ€ ì •ë³´ì—ì„œ doc_id ì¶”ì¶œ.
    ê¸°ì¡´ service.users.chat.retrieval.retrieval.extract_doc_ids_from_attachments ì™€ ë™ì¼í•œ ë¡œì§.
    """
    doc_info_dir, _ = get_document_dirs()
    doc_ids: List[str] = []

    for att in attachments or []:
        if isinstance(att, dict):
            location = str(att.get("contentString") or "").strip()
        else:
            location = str(getattr(att, "contentString", "")).strip()
        if not location:
            continue
        basename = location.split("/")[-1].split("?")[0]
        if not basename.endswith(".json"):
            continue

        base = basename[:-5].strip()

        # 1) íŒŒì¼ëª… ëì— UUIDê°€ ë¶™ì–´ ìˆìœ¼ë©´ doc_idë¡œ ì‚¬ìš©
        maybe_uuid = base.rsplit("-", 1)[-1].strip()
        if maybe_uuid and maybe_uuid.count("-") == 4:
            doc_ids.append(maybe_uuid)
            continue

        # 2) documents-info/<íŒŒì¼ëª…>.json ì—ì„œ idë¥¼ ì½ì–´ë³¸ë‹¤
        info_path = doc_info_dir / basename
        if info_path.exists():
            try:
                data = json.loads(info_path.read_text(encoding="utf-8"))
                doc_id = str(data.get("id") or "").strip()
                if doc_id:
                    doc_ids.append(doc_id)
            except Exception:
                continue

    seen = set()
    unique: List[str] = []
    for doc_id in doc_ids:
        if doc_id in seen:
            continue
        seen.add(doc_id)
        unique.append(doc_id)
    return unique

