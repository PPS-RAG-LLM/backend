"""ìŠ¤íŠ¸ë¦¬ë° ë° DB ì €ì¥ ë¡œì§"""
from typing import Dict, Any, List, Generator
from utils import logger
from utils.llms.registry import Streamer
from repository.workspace_chat import insert_chat_history
from repository.documents import delete_document_vectors_by_doc_ids
import json
import time

logger = logger(__name__)


def stream_and_persist(
    user_id: int,
    category: str,
    ws: Dict[str, Any],
    body: Dict[str, Any],
    runner: Streamer,
    messages: List[Dict[str, Any]],
    snippets: List[Dict[str, Any]],
    temp_doc_ids: List[str],
    thread_id: int | None = None,
) -> Generator[str, None, None]:
    """
    ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ë° DB ì €ì¥
    
    Args:
        user_id: ì‚¬ìš©ì ID
        category: ì¹´í…Œê³ ë¦¬ (qna, summary, doc_gen)
        ws: ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ì •ë³´
        body: ìš”ì²­ ë³¸ë¬¸
        runner: LLM streamer
        messages: ë©”ì‹œì§€ ëª©ë¡
        snippets: RAG ê²€ìƒ‰ ê²°ê³¼ (sourcesì— ì €ì¥ë  ë‚´ìš©)
        temp_doc_ids: ì„ì‹œ ë¬¸ì„œ ID ëª©ë¡ (ì •ë¦¬ìš©)
        thread_id: ìŠ¤ë ˆë“œ ID (QAë§Œ í•´ë‹¹)
    """
    temperature = ws.get("temperature")
    acc_text: List[str] = []
    t0 = time.perf_counter()

    sources = []
    for snippet in snippets:
        sources.append({
            "doc_id": snippet.get("doc_id"),
            "title": snippet.get("title"),
            "text": snippet.get("text"),
            "score": round(snippet.get("score", 0.0), 5),
            "page": snippet.get("page"),
            "chunk_index": snippet.get("chunk_index"),
            "source": snippet.get("source"),  # "milvus" ë˜ëŠ” "local"
        })
    # ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì „ì— ì†ŒìŠ¤ ë¨¼ì € ì „ì†¡
    if sources:
        yield f"__SOURCES__:{json.dumps(sources, ensure_ascii=False)}"
    
    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
    for chunk in runner.stream(messages, temperature=temperature):
        if chunk:
            acc_text.append(chunk)
            yield chunk
    duration = max(time.perf_counter() - t0, 0.0)
    
    # TODO : ì‘ë‹µ JSON êµ¬ì„± (TOKEN ì¹´ìš´íŠ¸ ì¶”ê°€)
    response_json = {
        "text": "".join(acc_text),
        "sources": sources,
        "type": "chat",  # ì¼ë‹¨ chatìœ¼ë¡œ ê³ ì • query ëª¨ë“œëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        "attachments": body.get("attachments") or [],
        "metrics": {
            "completion_tokens": 0,
            "prompt_tokens": 0,
            "total_tokens": 0,
            "output_tps": 0.0 if duration == 0 else len("".join(acc_text)) / max(duration, 1e-6),
            "duration": round(duration, 3),
        },
    }
    # DB ì €ì¥
    chat_id = insert_chat_history(
        user_id=user_id,
        category=category,
        workspace_id=ws["id"],
        prompt=body["message"],
        response=json.dumps(response_json, ensure_ascii=False),
        thread_id=thread_id,
        model=body["model"],
    )
    logger.debug(f"CHAT_ID : {chat_id}")
    try:
        if temp_doc_ids:
            delete_document_vectors_by_doc_ids(temp_doc_ids)
    except Exception as exc:
        logger.error(f"vector cleanup failed: {exc}")
        
    yield f"__CHAT_ID__: {chat_id}"


