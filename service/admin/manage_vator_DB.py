# === Vector DB Service (Milvus Server, Pro) ===
# - ì‘ì—…ìœ í˜•(task_type)ë³„ ë³´ì•ˆë ˆë²¨ ê´€ë¦¬: doc_gen | summary | qna
# - Milvus Docker ì„œë²„ ì „ìš© (Lite ì œê±°)
# - ë²¡í„°/í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì§€ì›, ì‹¤í–‰ ë¡œê·¸ ì ì¬

from __future__ import annotations
import asyncio
import json
import logging
import re
import shutil
import time
import uuid
from collections import defaultdict
from functools import partial
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from pydantic import BaseModel, Field
from config import config as app_config
from repository.rag_settings import get_rag_settings_row, set_rag_settings_row
from utils.database import get_session
from storage.db_models import (
    RagSettings,
    SecurityLevelConfigTask,
    SecurityLevelKeywordsTask,
)
from ..vector_db import (
    drop_all_collections,
    ensure_collection_and_index,
    get_milvus_client,
    milvus_has_data,
    run_dense_search,
    run_hybrid_search,
)

def split_for_varchar_bytes(
    text: str,
    hard_max_bytes: int = 32768,
    soft_max_bytes: int = 30000,   # ì—¬ìœ  ë²„í¼
    table_mark: str = "[[TABLE",
) -> list[str]:
    """
    VARCHAR ì´ˆê³¼ ë°©ì§€: UTF-8 ë°”ì´íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì•ˆì „ ë¶„í• .
    - í‘œ í…ìŠ¤íŠ¸ëŠ” í—¤ë”([[TABLE ...]])ë¥¼ ì²« ì¡°ê°ì—ë§Œ í¬í•¨.
    - ì´í›„ ì¡°ê°ì—” [[TABLE_CONT i/n]] ë§ˆì»¤ë¥¼ ë¶€ì—¬.
    - ê°œí–‰ ê²½ê³„ ìš°ì„ (backtrack), ê·¸ë˜ë„ ì•ˆë˜ë©´ í•˜ë“œì»·.
    """
    if not text:
        return [""]

    # í‘œ í—¤ë” ë¶„ë¦¬
    header = ""
    body = text
    if text.startswith(table_mark):
        head_end = text.find("]]")
        if head_end != -1:
            head_end += 2
            if head_end < len(text) and text[head_end] == "\n":
                head_end += 1
            header, body = text[:head_end], text[head_end:]

    def _split_body(b: str) -> list[str]:
        out: list[str] = []
        b_bytes = b.encode("utf-8")
        n = len(b_bytes)
        i = 0
        while i < n:
            j = min(i + soft_max_bytes, n)
            # ê°œí–‰ ê²½ê³„ë¡œ ë’¤ë¡œ ë¬¼ëŸ¬ë‚˜ê¸°
            k = j
            backtracked = False
            # jë¶€í„° iê¹Œì§€ ì—­ë°©í–¥ìœ¼ë¡œ \n ë°”ì´íŠ¸(0x0A) íƒìƒ‰
            while k > i and (j - k) < 2000:  # ìµœëŒ€ 2KBë§Œ ë°±íŠ¸ë™
                if b_bytes[k-1:k] == b"\n":
                    backtracked = True
                    break
                k -= 1
            if backtracked and (k - i) >= int(soft_max_bytes * 0.6):
                cut = k
            else:
                cut = j

            # í•˜ë“œ ì»·(ë©€í‹°ë°”ì´íŠ¸ ê²½ê³„ ë§ì¶”ê¸°)
            if cut - i > hard_max_bytes:
                cut = i + hard_max_bytes

            # UTF-8 ì•ˆì „ ë””ì½”ë“œ: ê²½ê³„ê°€ ë¬¸ìë¥¼ ë°˜ì¯¤ ìë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ë„‰ë„‰íˆ ì¡°ì •
            chunk = b_bytes[i:cut]
            # ë§Œì•½ ë””ì½”ë“œ ì—ëŸ¬ê°€ ë‚˜ë©´ í•œ ë°”ì´íŠ¸ì”© ì¤„ì´ë©° ì•ˆì „ ê²½ê³„ ì°¾ê¸°
            while True:
                try:
                    s = chunk.decode("utf-8")
                    break
                except UnicodeDecodeError:
                    cut -= 1
                    if cut <= i:
                        # ìµœì•…ì˜ ê²½ìš° í•œ ê¸€ìë¼ë„ ë””ì½”ë“œë˜ê²Œ í•œ ë°”ì´íŠ¸ ì•ë‹¹ê¹€
                        cut = i + 1
                    chunk = b_bytes[i:cut]
            out.append(s)
            i = cut
        return out

    if len(text.encode("utf-8")) <= hard_max_bytes:
        return [text]

    parts = _split_body(body)
    if header:
        total = len(parts)
        result = []
        for idx, c in enumerate(parts, start=1):
            if idx == 1:
                # ì²« ì¡°ê°ì€ í—¤ë” + ë³¸ë¬¸
                # ì „ì²´ê°€ í•˜ë“œë§¥ìŠ¤ë¥¼ ë„˜ì§€ ì•Šê²Œ í—¤ë”ì™€ í•©ì¹œ ë’¤ í•œë²ˆ ë” ìë¥´ê¸°
                first = header + c
                if len(first.encode("utf-8")) <= hard_max_bytes:
                    result.append(first)
                else:
                    # ë„ˆë¬´ í¬ë©´ í—¤ë”ëŠ” ìœ ì§€í•˜ê³  cë¥¼ ë‹¤ì‹œ ì˜ë¼ ë¶™ì„
                    # (í—¤ë”ê°€ ê¸¸ ë•Œ ë§¤ìš° ì˜ˆì™¸ì )
                    subparts = _split_body(c)
                    if subparts:
                        # ì²« ì¡°ê°ì€ í—¤ë” + ì²« sub
                        f = header + subparts[0]
                        if len(f.encode("utf-8")) > hard_max_bytes:
                            # í—¤ë” ìì²´ê°€ í° ê·¹ë‹¨: í—¤ë”ë§Œ ë„£ê³  ì´í›„ CONTë¡œ ì²˜ë¦¬
                            result.append(header[:0] + header)  # ê·¸ëŒ€ë¡œ
                            # ë‚˜ë¨¸ì§€ëŠ” CONT
                            for sidx, sp in enumerate(subparts, start=1):
                                tag = f"[[TABLE_CONT {sidx}/{len(subparts)}]]\n"
                                result.append(tag + sp)
                        else:
                            result.append(f)
                            # ë‚˜ë¨¸ì§€ëŠ” CONT
                            for sidx, sp in enumerate(subparts[1:], start=2):
                                tag = f"[[TABLE_CONT {sidx}/{len(subparts)}]]\n"
                                result.append(tag + sp)
                    else:
                        result.append(header)  # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ í—¤ë”ë§Œ
            else:
                tag = f"[[TABLE_CONT {idx}/{total}]]\n"
                # tag + c ê°€ í•˜ë“œë§¥ìŠ¤ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì¬ìë¥´ê¸°
                rest = tag + c
                if len(rest.encode("utf-8")) <= hard_max_bytes:
                    result.append(rest)
                else:
                    subs = _split_body(c)
                    for sidx, sp in enumerate(subs, start=1):
                        subt = f"[[TABLE_CONT {idx}.{sidx}/{total}]]\n" + sp
                        if len(subt.encode("utf-8")) <= hard_max_bytes:
                            result.append(subt)
                        else:
                            # ê·¸ë˜ë„ ë„˜ìœ¼ë©´ í•˜ë“œì»·ìœ¼ë¡œ ë§ˆì§€ë§‰ ë°©ì–´
                            bb = subt.encode("utf-8")[:hard_max_bytes]
                            result.append(bb.decode("utf-8", errors="ignore"))
        return result
    else:
        return parts


# KST ì‹œê°„ í¬ë§· ìœ í‹¸
from utils.time import now_kst, now_kst_string

from service.retrieval.common import hf_embed_text, chunk_text
from service.retrieval.pipeline import (
    DEFAULT_OUTPUT_FIELDS,
    build_dense_hits,
    # build_rrf_hits,
    build_rerank_payload,
    load_snippet_from_store,
)
from service.retrieval.reranker import rerank_snippets
from utils.model_load import (
    resolve_model_input,
    _get_or_load_embedder,
    _get_or_load_embedder_async,
    _invalidate_embedder_cache,
)

logger = logging.getLogger(__name__)

# -------------------------------------------------
# ê²½ë¡œ ìƒìˆ˜
# -------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent  # .../backend/service/admin
PROJECT_ROOT = BASE_DIR.parent.parent  # .../backend
_RETRIEVAL_CFG: Dict[str, Any] = app_config.get("retrieval", {}) or {}
_RETRIEVAL_PATHS: Dict[str, str] = _RETRIEVAL_CFG.get("paths", {}) or {}
_MILVUS_CFG: Dict[str, Any] = _RETRIEVAL_CFG.get("milvus", {}) or {}


def _cfg_path(key: str, fallback: str) -> Path:
    value = _RETRIEVAL_PATHS.get(key, fallback)
    return (PROJECT_ROOT / Path(value)).resolve()


STORAGE_DIR = _cfg_path("storage_dir", "storage")
USER_DATA_ROOT = _cfg_path("user_data_root", "storage/user_data")
RAW_DATA_DIR = _cfg_path("raw_data_dir", "storage/user_data/row_data")
LOCAL_DATA_ROOT = _cfg_path("local_data_root", "storage/user_data/preprocessed_data")
RESOURCE_DIR = _cfg_path("resources_dir", str(BASE_DIR / "resources"))
EXTRACTED_TEXT_DIR = _cfg_path("extracted_text_dir", "storage/extracted_texts")
META_JSON_PATH = _cfg_path("meta_json_path", "storage/extracted_texts/_extraction_meta.json")
MODEL_ROOT_DIR = _cfg_path("model_root_dir", "storage/embedding-models")
RERANK_MODEL_PATH = _cfg_path("rerank_model_path", "storage/rerank_model/Qwen3-Reranker-0.6B")

DATABASE_CFG = app_config.get("database", {}) or {}
SQLITE_DB_PATH = (PROJECT_ROOT / Path(DATABASE_CFG.get("path", "storage/pps_rag.db"))).resolve()

ADMIN_COLLECTION = _MILVUS_CFG.get("ADMIN_DOCS", "admin_docs_collection")

TASK_TYPES = tuple(_RETRIEVAL_CFG.get("task_types") or ("doc_gen", "summary", "qna"))
SUPPORTED_EXTS = set(_RETRIEVAL_CFG.get("supported_extensions"))

ZERO_WIDTH_RE = re.compile(r"[\u200B-\u200D\u2060\uFEFF]")
CONTROL_RE = re.compile(r"[\x00-\x08\x0B\x0C\x0E-\x1F]")
MULTISPACE_LINE_END_RE = re.compile(r"[ \t]+\n")
NEWLINES_RE = re.compile(r"\n{3,}")


# -------------------------------------------------
# í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ë‹¤ì¤‘ í™•ì¥ì ì§€ì›
# -------------------------------------------------

# í™•ì¥ìë³„ ì¶”ì¶œ í•¨ìˆ˜ë“¤ì€ service/preprocessing/rag_preprocessing.pyì™€ 
# service/preprocessing/extension/ í´ë”ë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤.
from service.preprocessing.rag_preprocessing import ext, extract_any


# -------------------------------------------------
# ì¸ì œìŠ¤íŠ¸ íŒŒë¼ë¯¸í„° ì„¤ì •
# -------------------------------------------------
def set_ingest_params(chunk_size: int | None = None, overlap: int | None = None):
    # rag_settings ë‹¨ì¼ ì†ŒìŠ¤ë¡œ ì €ì¥
    set_vector_settings(chunk_size=chunk_size, overlap=overlap)


def get_ingest_params():
    row = get_rag_settings_row()
    return {"chunkSize": row["chunk_size"], "overlap": row["overlap"]}


# -------------------------------------------------
# Pydantic ìŠ¤í‚¤ë§ˆ
# -------------------------------------------------
class RAGSearchRequest(BaseModel):
    query: str
    top_k: int = Field(5, gt=0)
    user_level: int = Field(1, ge=1)
    task_type: str = Field(..., description="doc_gen | summary | qna")
    model: Optional[str] = None  # ë‚´ë¶€ì ìœ¼ë¡œ settingsì—ì„œ ë¡œë“œ


class SinglePDFIngestRequest(BaseModel):
    pdf_path: str
    task_types: Optional[List[str]] = None  # ê¸°ë³¸ì€ ëª¨ë“  ì‘ì—…ìœ í˜•
    workspace_id: Optional[int] = None


# -------------------------------------------------
# SQLite ìœ í‹¸
# -------------------------------------------------


# ====== New helpers ======
def save_raw_file(filename: str, content: bytes) -> str:
    RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)
    out = RAW_DATA_DIR / filename
    out.parent.mkdir(parents=True, exist_ok=True)
    out.write_bytes(content)
    return str(out)


def save_raw_to_row_data(f):
    """Save FastAPI UploadFile to row_data and return relative path."""
    RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)
    name = Path(getattr(f, "filename", "uploaded"))
    dst = RAW_DATA_DIR / name.name
    if dst.exists():
        stem, ext = name.stem, name.suffix
        dst = RAW_DATA_DIR / f"{stem}_{int(time.time())}{ext}"
    with dst.open("wb") as out:
        data = f.file.read() if hasattr(f, "file") else b""
        out.write(data)
    try:
        return str(dst.relative_to(RAW_DATA_DIR))
    except Exception:
        return dst.name

def warmup_active_embedder(logger_func=print):
    """
    ì„œë²„ ê¸°ë™ ì‹œ í˜¸ì¶œìš©(ì„ íƒ). í™œì„± ëª¨ë¸ í‚¤ë¥¼ ì¡°íšŒí•´ ìºì‹œë¥¼ ì±„ì›€.
    ì‹¤íŒ¨í•´ë„ ì„œë¹„ìŠ¤ëŠ” ì‹¤ì œ ì‚¬ìš© ì‹œ ì§€ì—° ë¡œë”©ìœ¼ë¡œ ë³µêµ¬ë¨.
    """
    try:
        key = get_rag_settings_row().get("embedding_key")
        logger_func(f"[warmup] í™œì„± ì„ë² ë”© ëª¨ë¸: {key}. ë¡œë”© ì‹œë„...")
        _get_or_load_embedder(key, preload=True)
        logger_func(f"[warmup] ë¡œë”© ì™„ë£Œ: {key}")
    except Exception as e:
        logger_func(f"[warmup] ë¡œë”© ì‹¤íŒ¨(ì§€ì—° ë¡œë”©ìœ¼ë¡œ ë³µêµ¬ ì˜ˆì •): {e}")


def _update_vector_settings(
    search_type: Optional[str] = None,
    chunk_size: Optional[int] = None,
    overlap: Optional[int] = None,
):
    """ë ˆê±°ì‹œ API í˜¸í™˜: rag_settings(ì‹±ê¸€í†¤) ì—…ë°ì´íŠ¸"""
    cur = get_rag_settings_row()
    new_search = (search_type or cur["search_type"]).lower()
    if new_search == "vector":
        new_search = "semantic"
    if new_search not in {"hybrid", "semantic", "bm25"}:
        raise ValueError(
            "unsupported searchType; allowed: 'hybrid','semantic','bm25' (or 'vector' alias)"
        )
    new_chunk = int(chunk_size if chunk_size is not None else cur["chunk_size"])
    new_overlap = int(overlap if overlap is not None else cur["overlap"])
    if new_chunk <= 0 or new_overlap < 0 or new_overlap >= new_chunk:
        raise ValueError("invalid chunk/overlap (chunk>0, 0 <= overlap < chunk)")

    with get_session() as session:
        s = session.query(RagSettings).filter(RagSettings.id == 1).first()
        if not s:
            s = RagSettings(id=1)
            session.add(s)
        s.search_type = new_search
        s.chunk_size = new_chunk
        s.overlap = new_overlap
        s.updated_at = now_kst()
        session.commit()


# ---------------- Vector Settings ----------------
def set_vector_settings(embed_model_key: Optional[str] = None,
                        search_type: Optional[str] = None,
                        chunk_size: Optional[int] = None,
                        overlap: Optional[int] = None) -> Dict:
    """
    rag_settings ë‹¨ì¼ ì†ŒìŠ¤ë¡œ ì„¤ì • ì €ì¥.
    - ì„ë² ë”© ëª¨ë¸ ë³€ê²½ ì‹œ ê¸°ì¡´ ë°ì´í„° ì¡´ì¬í•˜ë©´ ì°¨ë‹¨, í™œì„± ëª¨ë¸ ê°±ì‹  ë° ìºì‹œ ë¬´íš¨í™”
    - search_type/ì²­í¬/ì˜¤ë²„ë©ì€ rag_settingsì—ë§Œ ë°˜ì˜
    """
    cur = get_vector_settings()
    key_now = cur.get("embeddingModel")
    st_now = (cur.get("searchType") or "hybrid").lower()
    cs_now = int(cur.get("chunkSize") or 512)
    ov_now = int(cur.get("overlap") or 64)

    new_key = embed_model_key or key_now
    new_st = (search_type or st_now).lower()
    # DB ì œì•½ê³¼ ì¼ì¹˜(semantic == vector)
    if new_st == "semantic":
        new_st = "vector"
    if new_st not in {"hybrid", "bm25", "vector"}:
        raise ValueError("unsupported searchType; allowed: 'hybrid','bm25','vector'")

    new_cs = int(chunk_size if chunk_size is not None else cs_now)
    new_ov = int(overlap if overlap is not None else ov_now)
    if new_cs <= 0 or new_ov < 0 or new_ov >= new_cs:
        raise ValueError("invalid chunk/overlap (chunk>0, 0 <= overlap < chunk)")

    if embed_model_key is not None:
        client = get_milvus_client()
        if milvus_has_data(client, collection_name=ADMIN_COLLECTION):
            raise RuntimeError("Milvus ì»¬ë ‰ì…˜ì— ê¸°ì¡´ ë°ì´í„°ê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤. ë¨¼ì € /v1/admin/vector/delete-all ì„ í˜¸ì¶œí•´ ì´ˆê¸°í™”í•˜ì„¸ìš”.")
        set_rag_settings_row(new_search=new_st, new_chunk=new_cs, new_overlap=new_ov, new_key=new_key)
        _invalidate_embedder_cache()

    with get_session() as session:
        s = session.query(RagSettings).filter(RagSettings.id == 1).first()
        if not s:
            s = RagSettings(id=1)
            session.add(s)
        s.embedding_key = new_key
        # search_type/chunk/overlapì€ _update_vector_settingsì—ì„œ ë°˜ì˜ë¨. ì—¬ê¸°ì„  ì¡´ì¬ ì‹œ ë³´ì¡´
        if search_type is not None:
            s.search_type = (
                (search_type or "hybrid").lower().replace("vector", "semantic")
            )
        if chunk_size is not None:
            s.chunk_size = int(chunk_size)
        if overlap is not None:
            s.overlap = int(overlap)
        s.updated_at = now_kst()
        session.commit()

    return get_vector_settings()


def get_vector_settings() -> Dict:
    # rag_settings ëŠ” ê²€ìƒ‰ íƒ€ì…/ì²­í¬/ì˜¤ë²„ë©ë§Œ ì‹ ë¢°
    try:
        row = get_rag_settings_row()
    except Exception:
        logger.error("get_rag_settings_row ì‹¤íŒ¨")
        return {
            "embeddingModel": "unknown",
            "searchType": "hybrid",
            "chunkSize": 512,
            "overlap": 64,
        }
    return {
        "embeddingModel": row.get("embedding_key"),                        # â† rag_settings.embedding_keyëŠ” ë¬´ì‹œ
        "searchType": row.get("search_type", "hybrid"),
        "chunkSize": int(row.get("chunk_size", 512)),
        "overlap": int(row.get("overlap", 64)),
    }


def list_available_embedding_models() -> List[str]:
    """
    ./storage/embedding-models í´ë” ë‚´ì˜ ëª¨ë¸ í´ë”ëª…ë“¤ì„ ë°˜í™˜.
    - embedding_ ì ‘ë‘ì‚¬ê°€ ìˆìœ¼ë©´ ì œê±° (ì˜ˆ: embedding_bge_m3 â†’ bge_m3)
    - í´ë”ë§Œ ë°˜í™˜ (íŒŒì¼ ì œì™¸)
    """
    models = []
    if not MODEL_ROOT_DIR.exists():
        return models
    
    for item in MODEL_ROOT_DIR.iterdir():
        if item.is_dir():
            model_name = item.name
            # embedding_ ì ‘ë‘ì‚¬ ì œê±°
            if model_name.startswith("embedding_"):
                model_name = model_name[len("embedding_"):]
            models.append(model_name)
    
    return sorted(models)

# ------------- Security Level (per task) ---------
def _parse_at_string_to_keywords(value: str) -> List[str]:
    if not value:
        return []
    toks = [t.strip() for t in value.split("@")]
    return [t for t in toks if t]


def _normalize_keywords(val: Any) -> List[str]:
    """
    ë¦¬ìŠ¤íŠ¸/íŠœí”Œ/ì…‹: ê° ì›ì†Œë¥¼ strë¡œ ìºìŠ¤íŒ…, ê³µë°±/í•´ì‹œ ì œê±°
    ë¬¸ìì—´: '@' ê¸°ì¤€ìœ¼ë¡œ í† í°í™”
    ë¹ˆ ê°’ ì œê±° ë° ì¤‘ë³µ ì œê±°
    """
    out: List[str] = []
    if isinstance(val, str):
        toks = [t.strip() for t in val.split("@")]
    elif isinstance(val, (list, tuple, set)):
        toks = [str(t).strip() for t in val]
    else:
        toks = []
    for t in toks:
        if not t:
            continue
        if t.startswith("#"):
            t = t[1:]
        if t and t not in out:
            out.append(t)
    return out


def _normalize_levels(
    levels_raw: Dict[str, Any], max_level: int
) -> Dict[int, List[str]]:
    norm: Dict[int, List[str]] = {}
    for k, v in (levels_raw or {}).items():
        try:
            lv = int(str(k).strip().replace("level_", ""))
        except Exception:
            continue
        if lv < 1 or lv > max_level:
            continue
        kws = _normalize_keywords(v)
        if kws:
            norm[lv] = kws
    return norm


def upsert_security_level_for_task(
    task_type: str, max_level: int, levels_raw: Dict[str, Any]
) -> Dict:
    if task_type not in TASK_TYPES:
        raise ValueError(f"invalid task_type: {task_type}")
    if max_level < 1:
        raise ValueError("maxLevel must be >= 1")

    levels_map = _normalize_levels(levels_raw, max_level)

    with get_session() as session:
        # upsert config
        cfg = (
            session.query(SecurityLevelConfigTask)
            .filter(SecurityLevelConfigTask.task_type == task_type)
            .first()
        )
        if not cfg:
            cfg = SecurityLevelConfigTask(task_type=task_type, max_level=int(max_level))
            session.add(cfg)
        else:
            cfg.max_level = int(max_level)
            cfg.updated_at = now_kst()
        # replace keywords
        session.query(SecurityLevelKeywordsTask).filter(
            SecurityLevelKeywordsTask.task_type == task_type
        ).delete()
        for lv, kws in levels_map.items():
            for kw in kws:
                session.add(
                    SecurityLevelKeywordsTask(
                        task_type=task_type, level=int(lv), keyword=str(kw)
                    )
                )
        session.commit()
        return get_security_level_rules_for_task(task_type)


def get_security_level_rules_for_task(task_type: str) -> Dict:
    with get_session() as session:
        cfg = (
            session.query(SecurityLevelConfigTask)
            .filter(SecurityLevelConfigTask.task_type == task_type)
            .first()
        )
        max_level = int(cfg.max_level) if cfg else 1
        res: Dict[str, Any] = {
            "taskType": task_type,
            "maxLevel": max_level,
            "levels": {str(i): [] for i in range(1, max_level + 1)},
        }
        rows = (
            session.query(
                SecurityLevelKeywordsTask.level, SecurityLevelKeywordsTask.keyword
            )
            .filter(SecurityLevelKeywordsTask.task_type == task_type)
            .order_by(
                SecurityLevelKeywordsTask.level.asc(),
                SecurityLevelKeywordsTask.keyword.asc(),
            )
            .all()
        )
        for lv, kw in rows:
            key = str(int(lv))
            res["levels"].setdefault(key, []).append(str(kw))
        return res


def set_security_level_rules_per_task(config: Dict[str, Dict]) -> Dict:
    """
    config = {
      "doc_gen": {"maxLevel": 3, "levels": {"2": "@ê¸ˆì•¡@ì—°ë´‰", "3": "@ë¶€ì •@í‡´ì§ê¸ˆ"}},
      "summary": {"maxLevel": 2, "levels": {"2": "@ì‚¬ë‚´ë¹„ë°€"}},
      "qna": {"maxLevel": 3, "levels": {"2": "@ì—°êµ¬", "3": "@ê°œì¸ì •ë³´"}}
    }
    """
    with get_session() as session:
        # ì „ì²´ ì‚­ì œ í›„ ì¬ì‚½ì…(ê°„ê²°/ëª…í™•)
        session.query(SecurityLevelConfigTask).delete()
        session.query(SecurityLevelKeywordsTask).delete()
        session.flush()

        for task in TASK_TYPES:
            entry = config.get(task) or {}
            max_level = int(entry.get("maxLevel", 1))
            session.add(
                SecurityLevelConfigTask(task_type=task, max_level=max(1, max_level))
            )
            levels = entry.get("levels", {}) or {}
            for lvl_str, at_str in levels.items():
                try:
                    lvl = int(str(lvl_str).strip().replace("level_", ""))
                except Exception:
                    continue
                if lvl <= 1 or lvl > max_level:
                    continue
                for kw in _parse_at_string_to_keywords(str(at_str)):
                    session.add(
                        SecurityLevelKeywordsTask(
                            task_type=task, level=int(lvl), keyword=str(kw)
                        )
                    )
        session.commit()
        return get_security_level_rules_all()


def get_security_level_rules_all() -> Dict:
    with get_session() as session:
        # ê¸°ë³¸ max_level=1
        max_map = {t: 1 for t in TASK_TYPES}
        for task, max_level in session.query(
            SecurityLevelConfigTask.task_type, SecurityLevelConfigTask.max_level
        ).all():
            max_map[task] = int(max_level)

        res: Dict[str, Dict] = {}
        for task in TASK_TYPES:
            res[task] = {
                "maxLevel": max_map.get(task, 1),
                "levels": {str(i): [] for i in range(1, max_map.get(task, 1) + 1)},
            }

        rows = (
            session.query(
                SecurityLevelKeywordsTask.task_type,
                SecurityLevelKeywordsTask.level,
                SecurityLevelKeywordsTask.keyword,
            )
            .order_by(
                SecurityLevelKeywordsTask.task_type.asc(),
                SecurityLevelKeywordsTask.level.asc(),
                SecurityLevelKeywordsTask.keyword.asc(),
            )
            .all()
        )
        for task, level, kw in rows:
            if task in res:
                lv = str(int(level))
                if lv not in res[task]["levels"]:
                    res[task]["levels"][lv] = []
                res[task]["levels"][lv].append(str(kw))
        return res


def determine_level_for_task(text: str, task_rules: Dict) -> int:
    max_level = int(task_rules.get("maxLevel", 1))
    levels = task_rules.get("levels", {})
    sel = 1
    # ìƒìœ„ ë ˆë²¨ ìš°ì„ 
    for lvl in range(1, max_level + 1):
        kws = levels.get(str(lvl), [])
        for kw in kws:
            if kw and kw in text:
                sel = max(sel, lvl)
    return sel


# -------------------------------------------------
# 1) PDF â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì‘ì—…ìœ í˜•ë³„ ë³´ì•ˆë ˆë²¨ ë™ì‹œ ì‚°ì •)
# -------------------------------------------------
# extract_pdfs() í•¨ìˆ˜ëŠ” service/preprocessing/pdf/pdf_preprocessing.pyë¡œ ì´ë™í–ˆìŠµë‹ˆë‹¤.


def parse_doc_version(stem: str) -> Tuple[str, int]:
    if "_" in stem:
        base, cand = stem.rsplit("_", 1)
        if cand.isdigit() and len(cand) in (4, 8):
            return base, int(cand)
    return stem, 0


# -------------------------------------------------
# 2) ì¸ì œìŠ¤íŠ¸ (bulk)
#   - ì‘ì—…ìœ í˜•ë³„ë¡œ ë™ì¼ ì²­í¬ë¥¼ ê°ê° ì €ì¥(task_type, security_level ë¶„ë¦¬)
# -------------------------------------------------
async def ingest_embeddings(
    model_key: str | None = None,
    chunk_size: int | None = None,
    overlap: int | None = None,
    target_tasks: list[str] | None = None,
    collection_name: str = ADMIN_COLLECTION,
    file_keys_filter: list[str] | None = None,  # â˜… ì¶”ê°€: íŠ¹ì • íŒŒì¼ë§Œ ì¸ì œìŠ¤íŠ¸
):
    """
    META_JSONì„ ì½ì–´ ì¶”ì¶œëœ í…ìŠ¤íŠ¸(.txt)ë“¤ì„ ì¸ì œìŠ¤íŠ¸í•œë‹¤.
    - VARCHAR(32768 bytes) ì´ˆê³¼ ë°©ì§€: split_for_varchar_bytes ë¡œ ì•ˆì „ ë¶„í• 
    - í‘œëŠ” [[TABLE ...]] ë¨¸ë¦¬ê¸€ ìœ ì§€, ì´ì–´ì§€ëŠ” ì¡°ê°ì€ [[TABLE_CONT i/n]] ë§ˆì»¤ë¡œ ì—°ì†ì„± í‘œì‹œ
    - collection_name íŒŒë¼ë¯¸í„°ë¥¼ ëê¹Œì§€ ì‚¬ìš©(ê¸°ë³¸/ì„¸ì…˜ ì»¬ë ‰ì…˜ ê³µìš©)
    - file_keys_filter ê°€ ì£¼ì–´ì§€ë©´ í•´ë‹¹ë˜ëŠ” íŒŒì¼(meta key/íŒŒì¼ëª…/ìŠ¤í…€)ì´ 'í¬í•¨'ëœ í•­ëª©ë§Œ ì¸ì œìŠ¤íŠ¸
    """
    # ==== ì„¤ì •/ëª¨ë¸ ====
    settings = get_vector_settings()
    MAX_TOKENS, OVERLAP = int(settings["chunkSize"]), int(settings["overlap"])

    if not META_JSON_PATH.exists():
        return {"error": "ë©”íƒ€ JSONì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € PDF/ë¬¸ì„œ ì¶”ì¶œì„ ìˆ˜í–‰í•˜ì„¸ìš”."}

    eff_model_key = model_key or settings["embeddingModel"]
    tok, model, device = await _get_or_load_embedder_async(eff_model_key)
    
    # ë²¡í„° ì°¨ì› ê²€ì¦
    probe_vec = hf_embed_text(tok, model, device, "probe")
    emb_dim = int(probe_vec.shape[0])
    logger.info(f"[Ingest] ì„ë² ë”© ëª¨ë¸: {eff_model_key}, ë²¡í„° ì°¨ì›: {emb_dim}")
    
    client = get_milvus_client()
    
    # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì°¨ì›ì„ í™•ì¸í•˜ê³ , ë‹¤ë¥´ë©´ ì‚­ì œ
    if collection_name in client.list_collections():
        try:
            # ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸
            desc = client.describe_collection(collection_name)
            existing_dim = None
            for field in desc.get("fields", []):
                if field.get("name") == "embedding":
                    existing_dim = field.get("params", {}).get("dim")
                    break
            
            if existing_dim and int(existing_dim) != emb_dim:
                logger.warning(f"[Ingest] ì°¨ì› ë¶ˆì¼ì¹˜: ê¸°ì¡´={existing_dim}, ìƒˆëª¨ë¸={emb_dim}. ì»¬ë ‰ì…˜ ì¬ìƒì„±.")
                client.drop_collection(collection_name)
        except Exception as e:
            logger.warning(f"[Ingest] ì»¬ë ‰ì…˜ ì •ë³´ í™•ì¸ ì‹¤íŒ¨: {e}. ì¬ìƒì„± ì‹œë„.")
            try:
                client.drop_collection(collection_name)
            except Exception:
                pass
    
    ensure_collection_and_index(client, emb_dim=emb_dim, metric="IP", collection_name=collection_name)

    # ==== META ë¡œë“œ ë° ëŒ€ìƒ í•„í„° êµ¬ì„± ====
    meta: dict = json.loads(META_JSON_PATH.read_text(encoding="utf-8"))
    tasks = [t for t in (target_tasks or TASK_TYPES) if t in TASK_TYPES]
    if not tasks:
        return {"error": f"ìœ íš¨í•œ ì‘ì—…ìœ í˜•ì´ ì—†ìŠµë‹ˆë‹¤. í—ˆìš©: {TASK_TYPES}"}

    filter_tokens = set()
    if file_keys_filter:
        # meta key / íŒŒì¼ëª… / ìŠ¤í…œì„ ëª¨ë‘ ë§¤ì¹­í•  ìˆ˜ ìˆë„ë¡ ì†Œë¬¸ì í† í°í™”
        for f in file_keys_filter:
            p = Path(str(f))
            filter_tokens.add(str(f).lower())
            filter_tokens.add(p.name.lower())
            filter_tokens.add(p.stem.lower())

    total_inserted = 0
    BATCH_SIZE = 128

    # ==== ì¸ì œìŠ¤íŠ¸ ====
    # ì£¼ì˜: EXTRACTED_TEXT_DIR ì•ˆì˜ *.txt ë¥¼ ëŒë©´ì„œ, í•´ë‹¹ txt ê°€ ì–´ë–¤ meta key(ì›ë³¸ í™•ì¥ì)ì™€ ë§¤ì¹­ë˜ëŠ”ì§€ ì°¾ëŠ”ë‹¤.
    for txt_path in EXTRACTED_TEXT_DIR.rglob("*.txt"):
        rel_txt = txt_path.relative_to(EXTRACTED_TEXT_DIR)

        # ë‹¤ì–‘í•œ í™•ì¥ì í›„ë³´ë¡œ META key ì°¾ê¸°
        cands = [rel_txt.with_suffix(ext).as_posix() for ext in SUPPORTED_EXTS]
        meta_key = next((k for k in cands if k in meta), None)
        if not meta_key:
            continue

        # â˜… ì—…ë¡œë“œí•œ ê²ƒë§Œ ì¸ì œìŠ¤íŠ¸ ì˜µì…˜: meta key / íŒŒì¼ëª… / ìŠ¤í…œ ê¸°ì¤€ í•„í„°ë§
        if filter_tokens:
            p = Path(meta_key)
            if (meta_key.lower() not in filter_tokens and
                p.name.lower() not in filter_tokens and
                p.stem.lower() not in filter_tokens):
                continue

        entry = meta.get(meta_key) or {}
        sec_map = entry.get("security_levels", {}) or {}

        # doc_id / version í™•ë³´(ì—†ìœ¼ë©´ íŒŒì¼ëª…ì—ì„œ ìœ ì¶”)
        doc_id = entry.get("doc_id")
        version = int(entry.get("version", 0) or 0)
        if not doc_id or version == 0:
            _id, _ver = parse_doc_version(Path(meta_key).stem)
            doc_id = doc_id or _id
            version = version or _ver
            entry["doc_id"] = doc_id
            entry["version"] = version
            meta[meta_key] = entry  # ë³€ê²½ì‚¬í•­ ë°˜ì˜

        # ê¸°ì¡´ ë™ì¼ ë¬¸ì„œ/ë²„ì „ ì‚­ì œ(ì‘ì—…ìœ í˜• ìƒê´€ ì—†ì´)
        try:
            client.delete(
                collection_name=collection_name,
                filter=f"doc_id == '{doc_id}' && version <= {int(version)}",
            )
        except Exception:
            pass

        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ë¡œë“œ ë° ì²­í¬í™”
        try:
            text = txt_path.read_text(encoding="utf-8")
        except Exception:
            # í˜¹ì‹œ ëª¨ë¥¼ ì¸ì½”ë”© ë¬¸ì œ í´ë°±
            text = txt_path.read_text(errors="ignore")
        
        # í†µí•© íŒŒì¼ì„ ì§ì ‘ íŒŒì‹±í•˜ì—¬ í˜ì´ì§€ë³„ë¡œ ë¶„í•  (í…ìŠ¤íŠ¸ì™€ í‘œê°€ í•¨ê»˜ ì €ì¥ëœ íŒŒì¼)
        # í˜ì´ì§€ êµ¬ë¶„ì„  "---" ê¸°ì¤€ìœ¼ë¡œ í˜ì´ì§€ ë¶„ë¦¬
        def _parse_integrated_file(text: str) -> list[tuple[int, str]]:
            """í†µí•© íŒŒì¼ì„ í˜ì´ì§€ë³„ë¡œ ë¶„í•  (í˜ì´ì§€ êµ¬ë¶„ì„  "---" ê¸°ì¤€)"""
            page_blocks: list[tuple[int, str]] = []
            lines = text.split('\n')
            current_page = 1
            current_content = []
            
            for line in lines:
                # í˜ì´ì§€ êµ¬ë¶„ì„  í™•ì¸: "---" (ë¹ˆ ì¤„ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ê²½ìš°)
                if line.strip() == "---":
                    # ì´ì „ í˜ì´ì§€ ì €ì¥
                    if current_content:
                        page_text = '\n'.join(current_content).strip()
                        if page_text:
                            page_blocks.append((current_page, page_text))
                    current_page += 1
                    current_content = []
                else:
                    current_content.append(line)
            
            # ë§ˆì§€ë§‰ í˜ì´ì§€ ì €ì¥
            if current_content:
                page_text = '\n'.join(current_content).strip()
                if page_text:
                    page_blocks.append((current_page, page_text))
            
            # í˜ì´ì§€ êµ¬ë¶„ì„ ì´ ì—†ìœ¼ë©´ ì „ì²´ë¥¼ 1í˜ì´ì§€ë¡œ ì²˜ë¦¬
            if not page_blocks:
                if text.strip():
                    page_blocks = [(1, text.strip())]
            
            return page_blocks
        
        # í†µí•© íŒŒì¼ íŒŒì‹±
        page_blocks = _parse_integrated_file(text)
        logger.info(f"[Ingest] í†µí•© íŒŒì¼ íŒŒì‹±: {len(page_blocks)}ê°œ í˜ì´ì§€ ë¸”ë¡ ë°œê²¬")
        
        # ì „ì²´ ë¬¸ì„œì—ì„œ ì²­í¬ ì¸ë±ìŠ¤ ëˆ„ì  (í˜ì´ì§€ë³„ë¡œ 0ë¶€í„° ì‹œì‘í•˜ì§€ ì•Šë„ë¡)
        chunks_with_page: list[tuple[int, int, str]] = []  # (page, chunk_idx, chunk_text)
        global_chunk_idx = 0  # ì „ì²´ ë¬¸ì„œì—ì„œ ëˆ„ì ë˜ëŠ” ì²­í¬ ì¸ë±ìŠ¤
        
        for page_num, page_text in page_blocks:
            if not page_text:
                continue
            page_chunks = chunk_text(page_text, max_tokens=MAX_TOKENS, overlap=OVERLAP)
            for chunk in page_chunks:
                if chunk.strip():  # ë¹ˆ ì²­í¬ ì œì™¸
                    chunks_with_page.append((page_num, global_chunk_idx, chunk))
                    global_chunk_idx += 1
        
        logger.info(f"[Ingest] ì´ {global_chunk_idx}ê°œ ì²­í¬ ìƒì„± (í˜ì´ì§€ë³„ ì²­í¬ ì¸ë±ìŠ¤ ëˆ„ì )")
        
        # í‘œ ë¸”ë¡ ì²˜ë¦¬
        # í†µí•© íŒŒì¼ì— ì´ë¯¸ í‘œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, í‘œë¥¼ ë³„ë„ë¡œ ì¸ì œìŠ¤íŠ¸í•˜ì§€ ì•ŠìŒ
        # (í†µí•© íŒŒì¼ì„ íŒŒì‹±í•  ë•Œ í‘œë„ í•¨ê»˜ ì²­í¬í™”ë˜ë¯€ë¡œ ì¤‘ë³µ ë°©ì§€)
        tables = entry.get("tables", []) or []
        logger.info(f"[Ingest] í‘œ ì •ë³´: {len(tables)}ê°œ (í†µí•© íŒŒì¼ì— ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë³„ë„ ì¸ì œìŠ¤íŠ¸ ì•ˆ í•¨)")

        batch: list[dict] = []

        for task in tasks:
            lvl = int(sec_map.get(task, 1))

            # 1) ë³¸ë¬¸ ì¡°ê° (í˜ì´ì§€ ì •ë³´ í¬í•¨, í…ìŠ¤íŠ¸ì™€ í‘œ ëª¨ë‘ í¬í•¨)
            for page_num, idx, c in chunks_with_page:
                # VARCHAR í•œë„ ì•ˆì „ ë¶„í• (ë°”ì´íŠ¸ ê¸°ì¤€)
                for part in split_for_varchar_bytes(c):
                    # ìµœì¢… ë°©ì–´(ì˜ˆì™¸ì ìœ¼ë¡œ ê²½ê³„ ì˜ë¦¼ ì‹¤íŒ¨ ì‹œ)
                    if len(part.encode("utf-8")) > 32768:
                        part = part.encode("utf-8")[:32768].decode("utf-8", errors="ignore")

                    vec = hf_embed_text(tok, model, device, part, max_len=MAX_TOKENS)
                    
                    # ë²¡í„° ì°¨ì› ê²€ì¦
                    if len(vec) != emb_dim:
                        logger.error(f"[Ingest] ë²¡í„° ì°¨ì› ë¶ˆì¼ì¹˜: ì˜ˆìƒ={emb_dim}, ì‹¤ì œ={len(vec)}, í…ìŠ¤íŠ¸='{part[:50]}...'")
                        continue  # ì´ ë²¡í„°ëŠ” ê±´ë„ˆë›°ê¸°
                    
                    batch.append({
                        "embedding": vec.tolist(),
                        "path": str(rel_txt.as_posix()),
                        "chunk_idx": int(idx),
                        "task_type": task,
                        "security_level": lvl,
                        "doc_id": str(doc_id),
                        "version": int(version),
                        "page": int(page_num),  # í˜ì´ì§€ ë²ˆí˜¸ ì¶”ê°€
                        "workspace_id": 0,
                        "text": part,
                    })
                    if len(batch) >= BATCH_SIZE:                        
                        client.insert(collection_name=collection_name, data=batch)
                        total_inserted += len(batch)
                        batch = []

            # 2) í‘œ ì¡°ê°ì€ í†µí•© íŒŒì¼ì— ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë³„ë„ ì¸ì œìŠ¤íŠ¸í•˜ì§€ ì•ŠìŒ
            # (í†µí•© íŒŒì¼ì„ íŒŒì‹±í•  ë•Œ í‘œë„ í•¨ê»˜ ì²­í¬í™”ë˜ë¯€ë¡œ ì¤‘ë³µ ë°©ì§€)

        if batch:
            client.insert(collection_name=collection_name, data=batch)
            total_inserted += len(batch)

    # ì¸ë±ìŠ¤/ë¡œë”© ì¬ë³´ì¥ ë° ë©”íƒ€ ì €ì¥(ìœ ì¶”ëœ doc_id/version ë°˜ì˜)
    try:
        client.flush(collection_name)
    except Exception:
        pass
    ensure_collection_and_index(client, emb_dim=emb_dim, metric="IP", collection_name=collection_name)

    # METAì— doc_id/version ë³´ì •ì´ ìˆì—ˆë‹¤ë©´ ì €ì¥
    try:
        META_JSON_PATH.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")
    except Exception:
        pass

    return {
        "message": f"Ingest ì™„ë£Œ(Milvus Server, collection={collection_name})",
        "inserted_chunks": int(total_inserted),
    }

# -------------------------------------------------
# 2-1) ë‹¨ì¼ íŒŒì¼ ì¸ì œìŠ¤íŠ¸(ì„ íƒ ì‘ì—…ìœ í˜•)
# -------------------------------------------------
async def ingest_single_pdf(req: SinglePDFIngestRequest):
    try:
        from repository.documents import insert_workspace_document
    except Exception:
        insert_workspace_document = None

    file_path = Path(req.pdf_path)
    if not file_path.exists():
        return {"error": f"íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}

    if ext(file_path) not in SUPPORTED_EXTS:
        return {"error": f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {ext(file_path)}"}

    # ë©”íƒ€ ë¡œë“œ
    if META_JSON_PATH.exists():
        meta = json.loads(META_JSON_PATH.read_text(encoding="utf-8"))
    else:
        meta = {}

    # ì¶”ì¶œ
    text_all, table_blocks_all = extract_any(file_path)

    # ë³´ì•ˆ ë ˆë²¨ íŒì •(ë³¸ë¬¸+í‘œ)
    all_rules = get_security_level_rules_all()
    whole_for_level = text_all + "\n\n" + "\n\n".join(t.get("text","") for t in (table_blocks_all or []))
    sec_map = {task: determine_level_for_task(whole_for_level, all_rules.get(task, {"maxLevel": 1, "levels": {}})) for task in TASK_TYPES}
    max_sec = max(sec_map.values()) if sec_map else 1
    sec_folder = f"securityLevel{int(max_sec)}"

    # ë³´ê´€ ë° í…ìŠ¤íŠ¸ ì €ì¥
    rel_file = Path(sec_folder) / file_path.name
    (LOCAL_DATA_ROOT / rel_file).parent.mkdir(parents=True, exist_ok=True)
    shutil.copy2(file_path, LOCAL_DATA_ROOT / rel_file)
    txt_path = EXTRACTED_TEXT_DIR / rel_file.with_suffix(".txt")
    txt_path.parent.mkdir(parents=True, exist_ok=True)
    txt_path.write_text(text_all, encoding="utf-8")

    from service.preprocessing.rag_preprocessing import _clean_text as clean_text

    doc_id, ver = parse_doc_version(file_path.stem)
    meta[str(rel_file)] = {
        "chars": len(text_all),
        "lines": len(text_all.splitlines()),
        "preview": (clean_text(text_all[:200].replace("\n", " ")) + "â€¦") if text_all else "",
        "security_levels": sec_map,
        "doc_id": doc_id,
        "version": ver,
        "tables": table_blocks_all or [],
        "sourceExt": ext(file_path),
    }
    META_JSON_PATH.parent.mkdir(parents=True, exist_ok=True)
    META_JSON_PATH.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

    # ì¸ì œìŠ¤íŠ¸
    settings = get_vector_settings()
    tok, model, device = await _get_or_load_embedder_async(settings["embeddingModel"])
    emb_dim = int(hf_embed_text(tok, model, device, "probe").shape[0])
    client = get_milvus_client()
    ensure_collection_and_index(client, emb_dim=emb_dim, metric="IP", collection_name=ADMIN_COLLECTION)

    s = get_vector_settings()
    max_token, overlab = int(s["chunkSize"]), int(s["overlap"])

    # ê¸°ì¡´ ì‚­ì œ
    try:
        client.delete(ADMIN_COLLECTION, filter=f"doc_id == '{doc_id}' && version <= {int(ver)}")
    except Exception:
        pass

    tasks = req.task_types or list(TASK_TYPES)
    chunks = chunk_text(text_all, max_tokens=max_token, overlab=overlab)
    batch, cnt = [], 0

    for task in tasks:
        lvl = int(sec_map.get(task, 1))

        # ë³¸ë¬¸: VARCHAR ì•ˆì „ ë¶„í• 
        for idx, c in enumerate(chunks):
            for part in split_for_varchar_bytes(c):
                if len(part.encode("utf-8")) > 32768:
                    part = part.encode("utf-8")[:32768].decode("utf-8", errors="ignore")
                vec = hf_embed_text(tok, model, device, part, max_len=max_token)
                batch.append({
                    "embedding": vec.tolist(),
                    "path": str(rel_file.with_suffix(".txt")),
                    "chunk_idx": int(idx),
                    "task_type": task,
                    "security_level": lvl,
                    "doc_id": str(doc_id),
                    "version": int(ver),
                    "workspace_id": 0,
                    "text": part,
                })
                if len(batch) >= 128:
                    client.insert(collection_name=ADMIN_COLLECTION, data=batch)
                    cnt += len(batch)
                    batch = []

        # í‘œ: VARCHAR ì•ˆì „ ë¶„í• 
        base_idx = len(chunks)
        for t_i, t in enumerate(table_blocks_all or []):
            md = (t.get("text") or "").strip()
            if not md:
                continue
            page = int(t.get("page", 0))
            bbox = t.get("bbox") or []
            bbox_str = ",".join(str(x) for x in bbox) if bbox else ""
            table_text = f"[[TABLE page={page} bbox={bbox_str}]]\n{md}"

            for sub_j, part in enumerate(split_for_varchar_bytes(table_text)):
                if len(part.encode("utf-8")) > 32768:
                    part = part.encode("utf-8")[:32768].decode("utf-8", errors="ignore")
                vec = hf_embed_text(tok, model, device, part, max_len=max_token)
                batch.append({
                    "embedding": vec.tolist(),
                    "path": str(rel_file.with_suffix(".txt")),
                    "chunk_idx": int(base_idx + t_i * 1000 + sub_j),
                    "task_type": task,
                    "security_level": lvl,
                    "doc_id": str(doc_id),
                    "version": int(ver),
                    "workspace_id": 0,
                    "text": part,
                })
                if len(batch) >= 128:
                    client.insert(collection_name=ADMIN_COLLECTION, data=batch)
                    cnt += len(batch)
                    batch = []

    if batch:
        client.insert(collection_name=ADMIN_COLLECTION, data=batch)
        cnt += len(batch)

    try:
        client.flush(ADMIN_COLLECTION)
    except Exception:
        pass
    ensure_collection_and_index(client, emb_dim=emb_dim, metric="IP", collection_name=ADMIN_COLLECTION)

    return {
        "message": f"ë‹¨ì¼ íŒŒì¼ ì¸ì œìŠ¤íŠ¸ ì™„ë£Œ(Milvus Server) - {ext(file_path)}",
        "doc_id": doc_id,
        "version": ver,
        "chunks": cnt,
        "sourceExt": ext(file_path),
    }

async def ingest_specific_files_with_levels(
    uploads: Optional[List[Any]] = None,          # FastAPI UploadFile ë¦¬ìŠ¤íŠ¸
    paths: Optional[List[str]] = None,            # ë¡œì»¬ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    tasks: Optional[List[str]] = None,            # ì—†ìœ¼ë©´ ëª¨ë“  TASK_TYPES
    level_for_tasks: Optional[Dict[str, int]] = None,  # {"qna":2,"summary":1} ìš°ì„ 
    level: Optional[int] = None,                  # ê³µí†µ ë ˆë²¨. ìœ„ map ìˆìœ¼ë©´ ë¬´ì‹œ
    collection_name: Optional[str] = None,
):
    if not uploads and not paths:
        return {"error": "ëŒ€ìƒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. uploads ë˜ëŠ” paths ì¤‘ í•˜ë‚˜ëŠ” í•„ìš”í•©ë‹ˆë‹¤."}

    tasks_eff = [t for t in (tasks or TASK_TYPES) if t in TASK_TYPES]
    if not tasks_eff:
        return {"error": f"ìœ íš¨í•œ ì‘ì—…ìœ í˜•ì´ ì—†ìŠµë‹ˆë‹¤. í—ˆìš©: {TASK_TYPES}"}

    lvl_map: Dict[str, int] = {}
    if level_for_tasks:
        for k, v in level_for_tasks.items():
            if k in TASK_TYPES:
                lvl_map[k] = max(1, int(v))
    elif level is not None:
        for t in tasks_eff:
            lvl_map[t] = max(1, int(level))

    # ì—…ë¡œë“œ ì €ì¥(ì„ì‹œ) + ê²½ë¡œ í•©ì¹˜ê¸°
    run_id = uuid.uuid4().hex[:8]
    tmp_root = (VAL_SESSION_ROOT / "adhoc" / run_id).resolve()
    tmp_root.mkdir(parents=True, exist_ok=True)

    saved: List[Path] = []
    if uploads:
        for f in uploads:
            fname = Path(getattr(f, "filename", "uploaded")).name
            tmp_path = tmp_root / fname
            try:
                data = await f.read()
            except Exception:
                data = getattr(getattr(f, "file", None), "read", lambda: b"")()
            tmp_path.write_bytes(data or b"")
            saved.append(tmp_path)
    for p in (paths or []):
        pp = Path(str(p)).resolve()
        if pp.exists() and pp.is_file():
            saved.append(pp)

    if not saved:
        return {"error": "ì €ì¥/ìœ íš¨ì„± ê²€ì‚¬ í›„ ë‚¨ì€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}

    # ì„ë² ë”/ì»¬ë ‰ì…˜ ì¤€ë¹„
    settings = get_vector_settings()
    eff_model_key = settings["embeddingModel"]
    tok, model, device = await _get_or_load_embedder_async(eff_model_key)
    emb_dim = int(hf_embed_text(tok, model, device, "probe").shape[0])

    coll = collection_name or ADMIN_COLLECTION
    client = get_milvus_client()
    ensure_collection_and_index(client, emb_dim=emb_dim, metric="IP", collection_name=coll)

    MAX_TOKENS, OVERLAP = int(settings["chunkSize"]), int(settings["overlap"])

    processed, total = [], 0
    for src in saved:
        try:
            text, tables = extract_any(src)

            # ë ˆë²¨ ê²°ì •(ê°•ì œ > ê·œì¹™)
            if lvl_map:
                sec_map = {t: int(lvl_map.get(t, 1)) for t in tasks_eff}
            else:
                all_rules = get_security_level_rules_all()
                whole = text + "\n\n" + "\n\n".join(t.get("text", "") for t in (tables or []))
                sec_map = {
                    t: determine_level_for_task(whole, all_rules.get(t, {"maxLevel": 1, "levels": {}}))
                    for t in tasks_eff
                }
            max_sec = max(sec_map.values()) if sec_map else 1

            # ìŠ¤ë‹ˆí« ë¡œë”©ìš© í…ìŠ¤íŠ¸ ì €ì¥(ë©”ì¸ê³¼ ë¶„ë¦¬: __adhoc__)
            rel_txt = Path("__adhoc__") / run_id / f"securityLevel{int(max_sec)}" / src.with_suffix(".txt").name
            abs_txt = EXTRACTED_TEXT_DIR / rel_txt
            abs_txt.parent.mkdir(parents=True, exist_ok=True)
            abs_txt.write_text(text, encoding="utf-8")

            # ë¬¸ì„œ ID/ë²„ì „
            doc_id, ver = parse_doc_version(src.stem)

            # ê¸°ì¡´ ì‚­ì œ
            try:
                client.delete(collection_name=coll, filter=f"doc_id == '{doc_id}' && version <= {int(ver)}")
            except Exception:
                pass

            # ë³¸ë¬¸
            chunks = chunk_text(text, max_tokens=MAX_TOKENS, overlap=OVERLAP)
            batch, cnt = [], 0
            for t in tasks_eff:
                lvl = int(sec_map.get(t, 1))

                for idx, c in enumerate(chunks):
                    for part in split_for_varchar_bytes(c):
                        if len(part.encode("utf-8")) > 32768:
                            part = part.encode("utf-8")[:32768].decode("utf-8", errors="ignore")
                        vec = hf_embed_text(tok, model, device, part, max_len=MAX_TOKENS)
                        batch.append({
                            "embedding": vec.tolist(),
                            "path": str(rel_txt.as_posix()),
                            "chunk_idx": int(idx),
                            "task_type": t,
                            "security_level": lvl,
                            "doc_id": str(doc_id),
                            "version": int(ver),
                            "workspace_id": 0,
                            "text": part,
                        })
                        if len(batch) >= 128:
                            client.insert(collection_name=coll, data=batch); cnt += len(batch); batch = []

                # í‘œ
                base_idx = len(chunks)
                for t_i, tb in enumerate(tables or []):
                    md = (tb.get("text") or "").strip()
                    if not md:
                        continue
                    page = int(tb.get("page", 0)); bbox = tb.get("bbox") or []
                    bbox_str = ",".join(str(x) for x in bbox) if bbox else ""
                    table_text = f"[[TABLE page={page} bbox={bbox_str}]]\n{md}"
                    for sub_j, part in enumerate(split_for_varchar_bytes(table_text)):
                        if len(part.encode("utf-8")) > 32768:
                            part = part.encode("utf-8")[:32768].decode("utf-8", errors="ignore")
                        vec = hf_embed_text(tok, model, device, part, max_len=MAX_TOKENS)
                        batch.append({
                            "embedding": vec.tolist(),
                            "path": str(rel_txt.as_posix()),
                            "chunk_idx": int(base_idx + t_i * 1000 + sub_j),
                            "task_type": t,
                            "security_level": lvl,
                            "doc_id": str(doc_id),
                            "version": int(ver),
                            "workspace_id": 0,
                            "text": part,
                        })
                        if len(batch) >= 128:
                            client.insert(collection_name=coll, data=batch); cnt += len(batch); batch = []

            if batch:
                client.insert(collection_name=coll, data=batch); cnt += len(batch); batch = []

            processed.append({
                "file": src.name, "doc_id": doc_id, "version": int(ver),
                "levels": sec_map, "chunks": cnt
            })
            total += cnt

        except Exception:
            logger.exception("[upload-and-ingest] failed: %s", src)

    try:
        client.flush(coll)
    except Exception:
        pass
    ensure_collection_and_index(client, emb_dim=emb_dim, metric="IP", collection_name=coll)

    return {
        "message": "Upload & Ingest ì™„ë£Œ",
        "collection": coll,
        "runId": run_id,
        "processed": processed,
        "inserted_chunks": int(total),
    }

async def search_documents(req: RAGSearchRequest, search_type_override: Optional[str] = None,
                           collection_name: str = ADMIN_COLLECTION, rerank_top_n: Optional[int] = None) -> Dict:
    t0 = time.perf_counter()
    print(f"ğŸ” [Search] ê²€ìƒ‰ ì‹œì‘: query='{req.query}', topK={req.top_k}, rerank_topN={rerank_top_n}, task={req.task_type}")
    
    if req.task_type not in TASK_TYPES:
        return {
            "error": f"invalid task_type: {req.task_type}. choose one of {TASK_TYPES}"
        }

    settings = get_vector_settings()
    model_key = req.model or settings["embeddingModel"]
    raw_st = (search_type_override or settings.get("searchType") or "").lower()
    # alias normalization: 'semantic'/'sementic' -> 'vector'; default 'hybrid' if empty
    search_type = (raw_st.replace("semantic", "vector").replace("sementic", "vector") or "hybrid")

    tok, model, device = await _get_or_load_embedder_async(model_key)
    q_emb = hf_embed_text(tok, model, device, req.query)
    client = get_milvus_client()
    ensure_collection_and_index(client, emb_dim=len(q_emb), metric="IP", collection_name=ADMIN_COLLECTION)

    if ADMIN_COLLECTION not in client.list_collections():
        return {"error": "ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„° ì €ì¥(ì¸ì œìŠ¤íŠ¸)ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”."}

    # ê³µí†µ íŒŒë¼ë¯¸í„°
    embedding_candidates = int(req.top_k)  # ì„ë² ë”©ì—ì„œ ì°¾ì„ í›„ë³´ ê°œìˆ˜
    final_results = int(rerank_top_n) if rerank_top_n is not None else 5  # ìµœì¢… ë°˜í™˜ ê°œìˆ˜
    candidate = max(embedding_candidates, final_results * 2)  # ì¶©ë¶„í•œ í›„ë³´ í™•ë³´
    filter_expr = f"task_type == '{req.task_type}' && security_level <= {int(req.user_level)}"
    snippet_loader = partial(
        load_snippet_from_store,
        EXTRACTED_TEXT_DIR,
        max_tokens=512,
        overlap=64,
    )

    # === ë¶„ê¸°: ê²€ìƒ‰ ë°©ì‹ ===
    if search_type == "vector":
        res_dense = run_dense_search(
            client,
            collection_name=ADMIN_COLLECTION,
            query_vector=q_emb.tolist(),
            limit=candidate,
            filter_expr=filter_expr,
            output_fields=DEFAULT_OUTPUT_FIELDS,
        )
        hits_raw = build_dense_hits(res_dense, snippet_loader=snippet_loader)
    else:
        res_hybrid = run_hybrid_search(
            client,
            collection_name=ADMIN_COLLECTION,
            query_vector=q_emb.tolist(),
            query_text=req.query,
            limit=candidate,
            filter_expr=filter_expr,
            output_fields=DEFAULT_OUTPUT_FIELDS,
        )
        hits_raw = build_dense_hits(res_hybrid, snippet_loader=snippet_loader)
        # hits_raw = build_rrf_hits(
        #     res_dense,
        #     res_sparse,
        #     snippet_loader=snippet_loader,
        #     limit=candidate,
        # )

    # ê²€ìƒ‰ ê²°ê³¼ ìƒíƒœ ë¡œê·¸
    logger.info(f"ğŸ“Š [Search] ë²¡í„°/BM25 ê²€ìƒ‰ ì™„ë£Œ: í›„ë³´ {len(hits_raw)}ê°œ ë°œê²¬")
    if hits_raw:
        logger.info(f"ğŸ“Š [Search] ì²« ë²ˆì§¸ í›„ë³´: doc_id={hits_raw[0].get('doc_id')}, path={hits_raw[0].get('path')}")

    rerank_candidates = build_rerank_payload(hits_raw)

    if rerank_candidates:
        reranked = rerank_snippets(rerank_candidates, query=req.query, top_n=final_results)
        hits_sorted = []
        for res in reranked:
            original = res.metadata or {}
            hits_sorted.append(
                {
                    "score": float(res.score),
                    "path": original.get("path"),
                    "chunk_idx": int(original.get("chunk_idx", 0)),
                    "task_type": original.get("task_type"),
                    "security_level": int(original.get("security_level", 1)),
                    "doc_id": original.get("doc_id"),
                    "page": int(original.get("page", 0)),
                    "snippet": res.text,
                }
            )
    else:
        hits_sorted = sorted(
            hits_raw,
            key=lambda x: x.get("score_fused", x.get("score_vec", x.get("score_sparse", 0.0))),
            reverse=True,
        )[:final_results]
    
    # ë¦¬ë­í¬ í›„ ì¤‘ë³µ ì œê±°
    # 1) snippet_text ê¸°ì¤€: ë™ì¼í•œ ë‚´ìš©ì˜ ìŠ¤ë‹ˆí«ì€ í•˜ë‚˜ë§Œ (ìµœê³  ì ìˆ˜ë§Œ ìœ ì§€, doc_id ë¬´ê´€)
    # 2) (doc_id, chunk_idx) ê¸°ì¤€: ê°™ì€ ë¬¸ì„œì˜ ê°™ì€ ì²­í¬ëŠ” í•˜ë‚˜ë§Œ (chunk_idx ì¤‘ë³µ ë°©ì§€)
    # ë¬¸ì„œë‹¹ ì œí•œ ì—†ìŒ - rerank_topNë§Œí¼ ëª¨ë‘ ë°˜í™˜
    seen_by_snippet: dict[str, dict] = {}  # snippet_text -> hit (ìµœê³  ì ìˆ˜ë§Œ ìœ ì§€)
    seen_by_chunk: dict[tuple[str, int], dict] = {}  # (doc_id, chunk_idx) -> hit
    
    original_count = len(hits_sorted)
    
    for hit in hits_sorted:
        doc_id = hit.get("doc_id", "")
        chunk_idx = int(hit.get("chunk_idx", 0))
        snippet = hit.get("snippet", "").strip()
        
        if not snippet:
            continue
        
        chunk_key = (doc_id, chunk_idx)
        
        # 1) snippet_text ì¤‘ë³µ ì²´í¬ - ë™ì¼í•œ ë‚´ìš©ì´ë©´ ì¤‘ë³µ (ë‹¤ë¥¸ ë¬¸ì„œ/ì²­í¬ì—¬ë„)
        if snippet in seen_by_snippet:
            # ë™ì¼í•œ ìŠ¤ë‹ˆí«ì´ ì´ë¯¸ ìˆìœ¼ë©´ ë” ë†’ì€ ì ìˆ˜ë¡œ êµì²´
            existing = seen_by_snippet[snippet]
            if hit.get("score", 0.0) > existing.get("score", 0.0):
                # ê¸°ì¡´ í•­ëª©ì˜ chunk_keyë„ ì œê±°
                old_doc_id = existing.get("doc_id", "")
                old_chunk_idx = int(existing.get("chunk_idx", 0))
                old_chunk_key = (old_doc_id, old_chunk_idx)
                if old_chunk_key in seen_by_chunk:
                    del seen_by_chunk[old_chunk_key]
                # ìƒˆ í•­ëª©ìœ¼ë¡œ êµì²´
                seen_by_snippet[snippet] = hit
                seen_by_chunk[chunk_key] = hit
            continue  # ì¤‘ë³µì´ë¯€ë¡œ ìŠ¤í‚µ
        
        # 2) (doc_id, chunk_idx) ì¤‘ë³µ ì²´í¬ - ê°™ì€ ë¬¸ì„œì˜ ê°™ì€ ì²­í¬ëŠ” í•˜ë‚˜ë§Œ
        if chunk_key in seen_by_chunk:
            # ê°™ì€ (doc_id, chunk_idx)ê°€ ì´ë¯¸ ìˆìœ¼ë©´ ë” ë†’ì€ ì ìˆ˜ë¡œ êµì²´
            existing = seen_by_chunk[chunk_key]
            if hit.get("score", 0.0) > existing.get("score", 0.0):
                # ê¸°ì¡´ í•­ëª©ì˜ snippetë„ ì œê±°
                old_snippet = existing.get("snippet", "").strip()
                if old_snippet in seen_by_snippet and seen_by_snippet[old_snippet] == existing:
                    del seen_by_snippet[old_snippet]
                # ìƒˆ í•­ëª©ìœ¼ë¡œ êµì²´
                seen_by_chunk[chunk_key] = hit
                seen_by_snippet[snippet] = hit
            continue  # ì¤‘ë³µì´ë¯€ë¡œ ìŠ¤í‚µ
        
        # ìƒˆë¡œìš´ í•­ëª© ì¶”ê°€
        seen_by_snippet[snippet] = hit
        seen_by_chunk[chunk_key] = hit
    
    # ì¤‘ë³µ ì œê±°ëœ ê²°ê³¼ë¥¼ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  rerank_topNë§Œí¼ë§Œ ë°˜í™˜
    deduplicated = sorted(seen_by_snippet.values(), key=lambda x: x.get("score", 0.0), reverse=True)
    hits_sorted = deduplicated[:final_results]
    
    logger.info(f"ğŸ” [Deduplication] ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(hits_sorted)}ê°œ ê²°ê³¼ (ì›ë³¸: {original_count}ê°œ, ì œê±°: {original_count - len(hits_sorted)}ê°œ)")

    # ë¦¬ë­í¬ ê²°ê³¼ ë¡œê·¸ ì¶œë ¥
    if hits_sorted:
        top_hit = hits_sorted[0]
        logger.info(f"âœ¨ [Rerank] ì™„ë£Œ! ìµœê³  ì ìˆ˜: {top_hit.get('score', 0):.4f}")
        logger.info(f"ğŸ† [Rerank] ìµœê³  ìŠ¤ë‹ˆí« (doc_id: {top_hit.get('doc_id', 'unknown')}): {top_hit.get('snippet', '')[:100]}...")

    # í”„ë¡¬í”„íŠ¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    context = "\n---\n".join(h["snippet"] for h in hits_sorted if h.get("snippet"))
    prompt = f"ì‚¬ìš©ì ì§ˆì˜: {req.query}\n:\n{context}\n\nìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•´ ì£¼ì„¸ìš”."

    elapsed = round(time.perf_counter() - t0, 4)

    # query_logs ì‚­ì œ: INSERT ì œê±°
    return {
        "elapsed_sec": elapsed,
        "settings_used": {"model": model_key, "searchType": search_type},
        "hits": [
            {
                "score": float(h["score"]),
                "path": h["path"],
                "chunk_idx": int(h["chunk_idx"]),
                "task_type": h["task_type"],
                "security_level": int(h["security_level"]),
                "doc_id": h.get("doc_id"),
                "page": int(h.get("page", 0)),  # í˜ì´ì§€ ì •ë³´ ì¶”ê°€
                "snippet": h["snippet"],
            }
            for h in hits_sorted
        ],
        "prompt": prompt,
    }
    

async def execute_search(
    question: str,
    top_k: int = 20,   # ì„ë² ë”© í›„ë³´ ê°œìˆ˜
    rerank_top_n: int = 5,    # ìµœì¢… ë°˜í™˜ ê°œìˆ˜  
    security_level: int = 1,
    source_filter: Optional[List[str]] = None,
    task_type: str = "qna",
    model_key: Optional[str] = None,
    search_type: Optional[str] = None,
) -> Dict:
    print(f"â­ [ExecuteSearch] í•¨ìˆ˜ í˜¸ì¶œ: question='{question}', topK={top_k}, rerank_topN={rerank_top_n}")
    req = RAGSearchRequest(
        query=question,
        top_k=top_k,
        user_level=security_level,
        task_type=task_type,
        model=model_key,
    )
    logger.info(f"ğŸ“ [ExecuteSearch] search_documents í˜¸ì¶œ ì „: req ìƒì„± ì™„ë£Œ")
    res = await search_documents(req, search_type_override=search_type, rerank_top_n=rerank_top_n)
    logger.info(f"ğŸ“ [ExecuteSearch] search_documents í˜¸ì¶œ í›„: ê²°ê³¼ hits ìˆ˜={len(res.get('hits', []))}")
    # Build check_file BEFORE optional source_filter so it reflects original candidates
    check_files: List[str] = []
    logger.debug(f"\n###########################\nres: {res}")
    try:
        for h in res.get("hits", []):
            # Prefer doc_id when available; fallback to path-derived filename
            doc_id_val = h.get("doc_id")
            if doc_id_val:
                check_files.append(f"{str(doc_id_val)}.pdf")
                continue
            p = Path(h.get("path", ""))
            if str(p):
                check_files.append(p.with_suffix(".pdf").name)
    except Exception:
        pass

    if source_filter and "hits" in res:
        names = {Path(n).stem for n in source_filter}
        res["hits"] = [h for h in res["hits"] if Path(h["path"]).stem in names]

    res["check_file"] = sorted(list(set(check_files)))
    return res

# -------------------------------------------------
# 4) ê´€ë¦¬ ìœ í‹¸
# -------------------------------------------------
async def delete_db():
    # ëª¨ë¸ ìºì‹œ í´ë¦¬ì–´
    _invalidate_embedder_cache()

    client = get_milvus_client()
    cols = drop_all_collections(client)
    return {"message": "ì‚­ì œ ì™„ë£Œ(Milvus Server)", "dropped_collections": cols}

async def list_indexed_files(
    limit: int = 16384,
    offset: int = 0,
    query: Optional[str] = None,
    task_type: Optional[str] = None,
):
    limit = max(1, min(limit, 16384))
    client = get_milvus_client()
    if ADMIN_COLLECTION not in client.list_collections():
        return []

    # ë©”íƒ€ ë¡œë“œ(ì›ë³¸ í™•ì¥ì ë³µì›ìš©)
    try:
        meta = json.loads(META_JSON_PATH.read_text(encoding="utf-8"))
    except Exception:
        meta = {}

    flt = ""
    if task_type and task_type in TASK_TYPES:
        flt = f"task_type == '{task_type}'"
    try:
        rows = client.query(
            collection_name=ADMIN_COLLECTION,
            filter=flt,
            output_fields=["path", "chunk_idx", "security_level", "task_type"],
            limit=limit,
            offset=offset,
            consistency_level="Strong",
        )
    except Exception:
        rows = []

    counts: Dict[Tuple[str, str], int] = defaultdict(int)
    level_map: Dict[Tuple[str, str], int] = {}
    for r in rows:
        path = r.get("path") if isinstance(r, dict) else r["path"]
        ttype = r.get("task_type") if isinstance(r, dict) else r["task_type"]
        lvl = int((r.get("security_level") if isinstance(r, dict) else r["security_level"]) or 1)
        key = (path, ttype)
        counts[key] += 1
        level_map.setdefault(key, lvl)

    items = []
    for (path, ttype), cnt in counts.items():
        txt_rel = Path(path)

        # ë©”íƒ€ì—ì„œ ì›ë˜ í™•ì¥ìë¥¼ ë³µì›
        cands = [txt_rel.with_suffix(ext).as_posix() for ext in SUPPORTED_EXTS]
        meta_key = next((k for k in cands if k in meta), None)
        if meta_key:
            source_ext = meta.get(meta_key, {}).get("sourceExt") or Path(meta_key).suffix
            orig_rel = txt_rel.with_suffix(source_ext)
        else:
            # í´ë°±(êµ¬ë²„ì „ ë°ì´í„°): pdf ê°€ì •
            orig_rel = txt_rel.with_suffix(".pdf")

        file_name = orig_rel.name
        file_path = str(orig_rel)

        txt_abs = EXTRACTED_TEXT_DIR / txt_rel
        try:
            stat = txt_abs.stat()
            size = stat.st_size
            indexed_at = now_kst_string()
        except FileNotFoundError:
            size = None
            indexed_at = None
        items.append(
            {
                "taskType": ttype,
                "fileName": file_name,
                "filePath": file_path,
                "chunkCount": int(cnt),
                "indexedAt": indexed_at,
                "fileSize": size,
                "securityLevel": int(level_map.get((path, ttype), 1)),
            }
        )

    if query:
        q = str(query)
        items = [it for it in items if q in it["fileName"]]
    return items

async def delete_files_by_names(file_names: List[str], task_type: Optional[str] = None, collection_name: str = ADMIN_COLLECTION):
    """
    íŒŒì¼ëª…(= doc_id stem) ë°°ì—´ì„ ë°›ì•„ ë²¡í„° DBì—ì„œ ì‚­ì œ.
    - task_type ê°€ None ì´ë©´ ëª¨ë“  ì‘ì—…ìœ í˜•(doc_gen/summary/qna)ì—ì„œ ì‚­ì œ (ê¸°ì¡´ ë™ì‘ê³¼ ë™ì¼)
    - task_type ê°€ ì§€ì •ë˜ë©´ í•´ë‹¹ ì‘ì—…ìœ í˜• ë ˆì½”ë“œë§Œ ì‚­ì œ
    """
    if not file_names:
        return {"deleted": 0, "requested": 0}

    try:
        from repository.documents import delete_workspace_documents_by_filenames
    except Exception:
        delete_workspace_documents_by_filenames = None

    client = get_milvus_client()
    if ADMIN_COLLECTION not in client.list_collections():
        deleted_sql = None
        if delete_workspace_documents_by_filenames:
            deleted_sql = delete_workspace_documents_by_filenames(file_names)
        return {"deleted": 0, "deleted_sql": deleted_sql, "requested": len(file_names)}

    # ë¡œë“œ ë³´ì¥
    try:
        client.load_collection(collection_name=ADMIN_COLLECTION)
    except Exception:
        pass

    # ìœ íš¨í•œ task_type ì¸ì§€ ê²€ì¦
    task_filter = ""
    if task_type:
        if task_type not in TASK_TYPES:
            return {
                "deleted": 0,
                "requested": len(file_names),
                "error": f"invalid taskType: {task_type}",
            }
        task_filter = f" && task_type == '{task_type}'"

    deleted_total = 0
    per_file: dict[str, int] = {}

    for name in file_names:
        stem = Path(name).stem
        # Align fileName -> doc_id by stripping version suffix if present
        try:
            base_id, _ver = parse_doc_version(stem)
        except Exception:
            base_id = stem
        try:
            # doc_id == 'stem' [&& task_type == 'xxx']
            filt = f"doc_id == '{base_id}'{task_filter}"
            client.delete(collection_name=ADMIN_COLLECTION, filter=filt)
            deleted_total += 1
            per_file[name] = per_file.get(name, 0) + 1
        except Exception:
            logger.exception("Failed to delete from Milvus for file: %s", name)
            per_file[name] = per_file.get(name, 0)

    # Ensure deletion is visible to subsequent queries (file lists/overview)
    try:
        client.flush(ADMIN_COLLECTION)
    except Exception:
        logger.exception("Failed to flush Milvus after deletion")
    # Force reload to avoid any stale cache/state on the server side
    try:
        client.release_collection(collection_name=ADMIN_COLLECTION)
    except Exception:
        pass
    try:
        client.load_collection(collection_name=ADMIN_COLLECTION)
    except Exception:
        logger.exception("Failed to reload collection after deletion")

    deleted_sql = None
    if delete_workspace_documents_by_filenames:
        try:
            # SQLì€ ì‘ì—…ìœ í˜• êµ¬ë¶„ì´ ì—†ë‹¤ê³  ê°€ì •(ê¸°ì¡´ ê·¸ëŒ€ë¡œ)
            deleted_sql = delete_workspace_documents_by_filenames(file_names)
        except Exception:
            logger.exception("Failed to delete workspace documents in SQL")
            deleted_sql = None

    return {
        "deleted": deleted_total,  # ìš”ì²­ íŒŒì¼ ê¸°ì¤€ ì„±ê³µ ê±´ìˆ˜(ì‘ì—…ìœ í˜• ê¸°ì¤€ ë‹¨ìˆœ ì¹´ìš´íŠ¸)
        "deleted_sql": deleted_sql,
        "requested": len(file_names),
        "taskType": task_type,
        "perFile": per_file,  # íŒŒì¼ë³„ ì²˜ë¦¬í˜„í™©
    }


async def list_indexed_files_overview(collection_name: str = ADMIN_COLLECTION):
    items = await list_indexed_files(limit=16384, offset=0, query=None, task_type=None)
    # agg: task_type -> level -> count
    agg: Dict[str, Dict[int, int]] = defaultdict(lambda: defaultdict(int))
    for it in items:
        agg[it["taskType"]][int(it["securityLevel"])] += it["chunkCount"]
    # ë³´ê¸° ì¢‹ê²Œ ë³€í™˜
    overview = {
        t: {str(lv): agg[t][lv] for lv in sorted(agg[t].keys())} for t in agg.keys()
    }
    return {"overview": overview, "items": items}



# === ìƒˆ API: í‚¤ì›Œë“œ ì—†ì´ ë ˆë²¨ ì˜¤ë²„ë¼ì´ë“œ í›„ ì¸ì œìŠ¤íŠ¸ ===
class OverrideLevelsRequest(BaseModel):
    """
    ì—…ë¡œë“œ(or ê¸°ì¡´) íŒŒì¼ë“¤ì— ëŒ€í•´ ì‘ì—…ìœ í˜•ë³„ ë ˆë²¨ì„ ê°•ì œë¡œ ì„¸íŒ…í•˜ê³  ì¸ì œìŠ¤íŠ¸.
    - files: ëŒ€ìƒ íŒŒì¼ ì´ë¦„/ê²½ë¡œ(ë¹„ìš°ë©´ META ì „ì²´ ëŒ€ìƒì´ì§€ë§Œ, ë³¸ ì—”ë“œí¬ì¸íŠ¸ì—ì„œëŠ” ì—…ë¡œë“œ íŒŒì¼ë§Œ ì „ë‹¬)
    - level_for_tasks: {"qna":2,"summary":1,"doc_gen":3} (í•„ìˆ˜)
    - tasks: ì‘ì—…ìœ í˜• ì œí•œ (ë¯¸ì§€ì • ì‹œ ëª¨ë“  TASK_TYPES)
    """
    files: Optional[List[str]] = None
    level_for_tasks: Dict[str, int]
    tasks: Optional[List[str]] = None


async def override_levels_and_ingest(req: OverrideLevelsRequest):
    if not META_JSON_PATH.exists():
        return {"error": "ë©”íƒ€ JSONì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € /v1/admin/vector/extract ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”."}

    target_tasks = [t for t in (req.tasks or TASK_TYPES) if t in TASK_TYPES]
    if not target_tasks:
        return {"error": "ìœ íš¨í•œ ì‘ì—…ìœ í˜•ì´ ì—†ìŠµë‹ˆë‹¤. (í—ˆìš©: doc_gen|summary|qna)"}

    level_map = {t: int(max(1, lv)) for t, lv in (req.level_for_tasks or {}).items() if t in TASK_TYPES}
    if not level_map:
        return {"error": "ì ìš©í•  ë³´ì•ˆë ˆë²¨ì´ ì—†ìŠµë‹ˆë‹¤. level_for_tasks ë¥¼ ì§€ì •í•˜ì„¸ìš”."}

    meta = json.loads(META_JSON_PATH.read_text(encoding="utf-8"))

    # ëŒ€ìƒ íŒŒì¼ ì…‹(ë©”íƒ€í‚¤/íŒŒì¼ëª…/ìŠ¤í…€ ëª¨ë‘ í—ˆìš©)
    def _to_keyset(files: List[str]) -> set:
        out = set()
        for f in files:
            p = Path(f)
            out.update({str(f), p.name, p.stem})
        return out

    all_keys = list(meta.keys())  # "securityLevelX/.../íŒŒì¼ëª….í™•ì¥ì"
    if req.files:
        ks = _to_keyset(req.files)
        targets = [k for k in all_keys if (k in ks or Path(k).name in ks or Path(k).stem in ks)]
    else:
        targets = all_keys

    if not targets:
        return {"updated": 0, "ingested": 0, "message": "ëŒ€ìƒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}

    updated = 0
    for k in targets:
        entry = meta.get(k) or {}
        sec = entry.get("security_levels") or {}
        for t in target_tasks:
            if t in level_map:
                sec[t] = int(level_map[t])
        entry["security_levels"] = sec
        meta[k] = entry
        updated += 1

    META_JSON_PATH.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

    # â˜… ì—…ë¡œë“œí•œ(ë˜ëŠ” ì§€ì •í•œ) íŒŒì¼ë§Œ ì¸ì œìŠ¤íŠ¸
    res = await ingest_embeddings(
        model_key=None,
        chunk_size=None,
        overlap=None,
        target_tasks=target_tasks,
        collection_name=ADMIN_COLLECTION,
        file_keys_filter=targets,
    )
    return {
        "message": "ë ˆë²¨ ì˜¤ë²„ë¼ì´ë“œ í›„ ì¸ì œìŠ¤íŠ¸ ì™„ë£Œ",
        "collection": ADMIN_COLLECTION,
        "updated_meta_entries": updated,
        "inserted_chunks": int(res.get("inserted_chunks", 0)),
        "target_count": len(targets),
    }