# === Vector DB Service (Milvus Server, Pro) ===
# - ì‘ì—…ìœ í˜•(task_type)ë³„ ë³´ì•ˆë ˆë²¨ ê´€ë¦¬: doc_gen | summary | qna
# - Milvus Docker ì„œë²„ ì „ìš© (Lite ì œê±°)
# - ë²¡í„°/í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì§€ì›, ì‹¤í–‰ ë¡œê·¸ ì ì¬

from __future__ import annotations
import asyncio
import re
import time
import uuid
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List, Optional
from pydantic import BaseModel, Field
from config import config as app_config
from repository.rag_settings import get_rag_settings_row, set_rag_settings_row
from repository.documents import (
    bulk_upsert_document_metadata,
    delete_document_vectors,
    delete_documents_by_type_and_ids,
    document_has_vectors,
    fetch_metadata_by_vector_ids,
    get_document_by_source_path,
    get_list_indexed_files,
    insert_document_vectors,
    list_documents_by_type,
    purge_documents_by_collection,
    upsert_document,
    fetch_document_metadata_by_doc_ids, 
)
from utils.database import get_session
from utils.documents import generate_doc_id
from storage.db_models import (
    DocumentType,
    DocumentVector,
    RagSettings,
    SecurityLevelConfigTask,
    SecurityLevelKeywordsTask,
)
from ..vector_db import (
    ensure_collection_and_index,
    get_milvus_client,
    milvus_has_data,
    run_dense_search,
    run_hybrid_search,
)
from service.retrieval.common import (
    extract_insert_ids,
    hf_embed_text, 
    parse_doc_version, 
    determine_level_for_task, 
)
from service.retrieval.ingestion import ingest_common
from service.retrieval.pipeline import (
    DEFAULT_OUTPUT_FIELDS,
    build_dense_hits,
    build_rerank_payload,
)
from service.retrieval.reranker import rerank_snippets
from utils.model_load import (
    _get_or_load_embedder,
    _get_or_load_embedder_async,
    _invalidate_embedder_cache,
)
from utils import now_kst, now_kst_string, logger
logger = logger(__name__)


# -------------------------------------------------
# ê²½ë¡œ ìƒìˆ˜
# -------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent  # .../backend/service/admin
PROJECT_ROOT = BASE_DIR.parent.parent  # .../backend
_RETRIEVAL_CFG: Dict[str, Any] = app_config.get("retrieval", {}) or {}
_RETRIEVAL_PATHS: Dict[str, str] = _RETRIEVAL_CFG.get("paths", {}) or {}
_MILVUS_CFG: Dict[str, Any] = _RETRIEVAL_CFG.get("milvus", {}) or {}


def _cfg_path(key: str, fallback: str) -> Path:
    value = _RETRIEVAL_PATHS.get(key, fallback)
    return (PROJECT_ROOT / Path(value)).resolve()


RAW_DATA_DIR = _cfg_path("raw_data_dir", "storage/user_data/row_data")
MODEL_ROOT_DIR = _cfg_path("model_root_dir", "storage/embedding-models")
VAL_SESSION_ROOT = _cfg_path("val_session_root", "storage/val_data")

DATABASE_CFG = app_config.get("database", {}) or {}
SQLITE_DB_PATH = (PROJECT_ROOT / Path(DATABASE_CFG.get("path", "storage/pps_rag.db"))).resolve()
ADMIN_COLLECTION = _MILVUS_CFG.get("ADMIN_DOCS", "admin_docs_collection")
TASK_TYPES = tuple(_RETRIEVAL_CFG.get("task_types") or ("doc_gen", "summary", "qna"))
SUPPORTED_EXTS = set(_RETRIEVAL_CFG.get("supported_extensions"))

ZERO_WIDTH_RE = re.compile(r"[\u200B-\u200D\u2060\uFEFF]")
CONTROL_RE = re.compile(r"[\x00-\x08\x0B\x0C\x0E-\x1F]")
MULTISPACE_LINE_END_RE = re.compile(r"[ \t]+\n")
NEWLINES_RE = re.compile(r"\n{3,}")
ADMIN_DOC_TYPE = DocumentType.ADMIN.value


def _ext(value: Path | str) -> str:
    """Path helper returning lowercase suffix."""
    return Path(value).suffix.lower()


def _max_security_level(sec_map: Dict[str, int]) -> int:
    if not sec_map:
        return 1
    levels = [int(v) for v in sec_map.values() if isinstance(v, (int, float, str))]
    parsed = []
    for lv in levels:
        try:
            parsed.append(int(lv))
        except Exception:
            continue
    return max(parsed or [1])


def _build_admin_payload(
    *,
    sec_map: Dict[str, int],
    version: int,
    preview: str,
    tables: List[Dict[str, Any]],
    total_pages: int,
    saved_files: Dict[str, str],
    pages: Dict[str, Any],
    source_ext: str,
    extraction_info: Dict[str, Any],
    rel_key: str,
) -> Dict[str, Any]:
    return {
        "security_levels": sec_map,
        "version": int(version),
        "preview": preview,
        "tables": tables or [],
        "total_pages": int(total_pages or 0),
        "saved_files": saved_files,
        "pages": pages or {},
        "source_ext": source_ext,
        "doc_rel_key": rel_key,
        "extraction_info": extraction_info,
    }


def register_admin_document(
    *,
    doc_id: str,
    filename: str,
    rel_text_path: str,
    rel_source_path: str,
    sec_map: Dict[str, int],
    version: int,
    preview: str,
    tables: List[Dict[str, Any]],
    total_pages: int,
    pages: Dict[str, Any],
    source_ext: str,
    extraction_info: Dict[str, Any],
) -> None:
    payload = _build_admin_payload(
        sec_map=sec_map,
        version=version,
        preview=preview,
        tables=tables,
        total_pages=total_pages,
        saved_files={"text": rel_text_path, "source": rel_source_path},
        pages=pages,
        source_ext=source_ext,
        extraction_info=extraction_info,
        rel_key=rel_source_path,
    )
    upsert_document(
        doc_id=doc_id,
        doc_type=ADMIN_DOC_TYPE,
        filename=filename,
        source_path=rel_source_path,
        security_level=_max_security_level(sec_map),
        payload=payload,
    )


def _doc_matches_tokens(doc: Dict[str, Any], tokens: set[str]) -> bool:
    if not tokens:
        return True
    payload = doc.get("payload") or {}
    candidates = [
        doc.get("doc_id"),
        doc.get("filename"),
        Path(str(doc.get("filename") or "")).stem,
        doc.get("source_path"),
        Path(str(doc.get("source_path") or "")).name,
        payload.get("doc_rel_key"),
    ]
    if payload.get("doc_rel_key"):
        candidates.append(Path(str(payload.get("doc_rel_key"))).name)
        candidates.append(Path(str(payload.get("doc_rel_key"))).stem)
    for value in candidates:
        if value and str(value).lower() in tokens:
            return True
    return False


def _load_admin_documents(file_keys_filter: Optional[List[str]] = None) -> List[Dict[str, Any]]:
    docs = list_documents_by_type(ADMIN_DOC_TYPE)
    if not file_keys_filter:
        return docs
    tokens = {str(f).lower() for f in file_keys_filter if str(f).strip()}
    if not tokens:
        return docs
    return [doc for doc in docs if _doc_matches_tokens(doc, tokens)]


def _doc_name_tokens(doc: Dict[str, Any]) -> set[str]:
    tokens: set[str] = set()

    def _push(value: Any) -> None:
        if not value:
            return
        try:
            s = str(value).strip()
        except Exception:
            return
        if not s:
            return
        tokens.add(s.lower())
        try:
            p = Path(s)
            tokens.add(p.name.lower())
            tokens.add(p.stem.lower())
        except Exception:
            pass

    _push(doc.get("doc_id"))
    _push(doc.get("filename"))
    _push(doc.get("source_path"))
    payload = doc.get("payload") or {}
    _push(payload.get("doc_rel_key"))
    saved = payload.get("saved_files") or {}
    for path in saved.values():
        _push(path)
    return {t for t in tokens if t}


def _build_doc_name_index() -> Dict[str, str]:
    docs = list_documents_by_type(ADMIN_DOC_TYPE)
    index: Dict[str, str] = {}
    for doc in docs:
        doc_id = str(doc.get("doc_id") or "").strip()
        if not doc_id:
            continue
        for token in _doc_name_tokens(doc):
            index.setdefault(token, doc_id)
    return index
# -------------------------------------------------
# Pydantic ìŠ¤í‚¤ë§ˆ
# -------------------------------------------------
class RAGSearchRequest(BaseModel):
    query: str
    top_k: int = Field(5, gt=0)
    user_level: int = Field(1, ge=1)
    task_type: str = Field(..., description="doc_gen | summary | qna")
    model: Optional[str] = None  # ë‚´ë¶€ì ìœ¼ë¡œ settingsì—ì„œ ë¡œë“œ



# -------------------------------------------------
# SQLite ìœ í‹¸
# -------------------------------------------------

# ====== New helpers ======
def save_raw_file(filename: str, content: bytes) -> str:
    RAW_DATA_DIR.mkdir(parents=True, exist_ok=True)
    name = Path(filename or "uploaded").name or f"uploaded_{uuid.uuid4().hex}"
    dst = RAW_DATA_DIR / name
    if dst.exists():
        stem, ext = dst.stem, dst.suffix
        dst = RAW_DATA_DIR / f"{stem}_{int(time.time())}{ext}"
    dst.write_bytes(content)
    return str(dst.relative_to(RAW_DATA_DIR).as_posix())


def _write_combined_text_file(
    output_path: Path,
    *,
    text: str,
    tables: List[Dict[str, Any]],
    pages_text_dict: Dict[int, str],
) -> None:
    def _write_tables(handle, items):
        for tbl in items:
            table_text = (tbl.get("text") or "").strip()
            if table_text:
                handle.write(table_text)
                handle.write("\n\n")

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8") as handle:
        if pages_text_dict:
            pages_tables: Dict[int, List[Dict[str, Any]]] = defaultdict(list)
            for tbl in tables or []:
                page_num = int(tbl.get("page", 0))
                if page_num > 0:
                    pages_tables[page_num].append(tbl)
            ordered_pages = sorted({*pages_text_dict.keys(), *pages_tables.keys()})
            if ordered_pages:
                for idx, page_num in enumerate(ordered_pages):
                    page_text = pages_text_dict.get(page_num, "")
                    if page_text:
                        handle.write(page_text)
                        handle.write("\n\n")
                    _write_tables(handle, pages_tables.get(page_num, []))
                    if idx < len(ordered_pages) - 1:
                        handle.write("\n---\n\n")
            else:
                if text.strip():
                    handle.write(text)
                    handle.write("\n\n")
                _write_tables(handle, tables or [])
        else:
            if text.strip():
                handle.write(text)
                handle.write("\n\n")
            _write_tables(handle, tables or [])

async def process_saved_raw_files(rel_paths: List[str]) -> List[Dict[str, Any]]:
    if not rel_paths:
        return []
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, _process_saved_raw_files_sync, rel_paths)

def _process_saved_raw_files_sync(rel_paths: List[str]) -> List[Dict[str, Any]]:
    level_rules = get_security_level_rules_all()
    results: List[Dict[str, Any]] = []
    for rel in rel_paths:
        info = _process_single_raw_file(rel, level_rules)
        if info:
            results.append(info)
    return results

def _process_single_raw_file(rel_path: str, level_rules: Dict[str, Dict]) -> Optional[Dict[str, Any]]:
    raw_path = (RAW_DATA_DIR / rel_path).resolve()
    if not raw_path.exists():
        logger.warning("[ProcessRaw] RAW íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: %s", rel_path)
        return None

    try:
        rel_from_raw = raw_path.relative_to(RAW_DATA_DIR)
    except ValueError:
        rel_from_raw = raw_path

    file_ext = _ext(raw_path)
    pages_text_dict: Dict[int, str] = {}
    total_pages = 0

    try:
        if file_ext == ".pdf":
            from service.preprocessing.extension.pdf_preprocessing import _extract_pdf_with_tables
            text, tables, pages_text_dict, total_pages = _extract_pdf_with_tables(raw_path)
        else:
            from service.preprocessing.rag_preprocessing import extract_any
            text, tables = extract_any(raw_path)
    except Exception:
        logger.exception("[ProcessRaw] ì¶”ì¶œ ì‹¤íŒ¨: %s", raw_path)
        return None

    tables = tables or []
    text = text or ""
    combined_for_level = text + "\n\n" + "\n\n".join(t.get("text", "") for t in tables)
    sec_map = {
        task: determine_level_for_task(
            combined_for_level,
            level_rules.get(task, {"maxLevel": 1, "levels": {}}),
        )
        for task in TASK_TYPES
    }
    max_sec = max(sec_map.values()) if sec_map else 1
    rel_text_path = Path(f"securityLevel{int(max_sec)}") / rel_from_raw.with_suffix(".txt")

    from service.preprocessing.rag_preprocessing import _clean_text as clean_text

    preview = (clean_text(text[:200].replace("\n", " ")) + "â€¦") if text else ""
    rel_source_path = Path(rel_path).as_posix()
    source_entry = str(Path("row_data") / rel_source_path)
    _, parsed_version = parse_doc_version(raw_path.stem)
    version = int(parsed_version) if parsed_version else 0
    extraction_info = {
        "original_file": raw_path.name,
        "text_length": len(text),
        "table_count": len(tables),
        "extracted_at": now_kst_string(),
    }

    existing = get_document_by_source_path(ADMIN_DOC_TYPE, source_entry)
    doc_id = existing["doc_id"] if existing else generate_doc_id()

    register_admin_document(
        doc_id=doc_id,
        filename=raw_path.name,
        rel_text_path=rel_text_path.as_posix(),
        rel_source_path=source_entry,
        sec_map=sec_map,
        version=int(version),
        preview=preview,
        tables=tables,
        total_pages=total_pages,
        pages=pages_text_dict if pages_text_dict else {},
        source_ext=file_ext,
        extraction_info=extraction_info,
    )

    return {
        "doc_id": doc_id,
        "filename": raw_path.name,
        "source_path": source_entry,
        "text_path": rel_text_path.as_posix(),
        "security_levels": sec_map,
        "version": int(version),
    }
    
def warmup_active_embedder(logger_func=print):
    """
    ì„œë²„ ê¸°ë™ ì‹œ í˜¸ì¶œìš©(ì„ íƒ). í™œì„± ëª¨ë¸ í‚¤ë¥¼ ì¡°íšŒí•´ ìºì‹œë¥¼ ì±„ì›€.
    ì‹¤íŒ¨í•´ë„ ì„œë¹„ìŠ¤ëŠ” ì‹¤ì œ ì‚¬ìš© ì‹œ ì§€ì—° ë¡œë”©ìœ¼ë¡œ ë³µêµ¬ë¨.
    """
    try:
        key = get_rag_settings_row().get("embedding_key")
        logger_func(f"[warmup] í™œì„± ì„ë² ë”© ëª¨ë¸: {key}. ë¡œë”© ì‹œë„...")
        _get_or_load_embedder(key, preload=True)
        logger_func(f"[warmup] ë¡œë”© ì™„ë£Œ: {key}")
    except Exception as e:
        logger_func(f"[warmup] ë¡œë”© ì‹¤íŒ¨(ì§€ì—° ë¡œë”©ìœ¼ë¡œ ë³µêµ¬ ì˜ˆì •): {e}")


def _update_vector_settings(
    search_type: Optional[str] = None,
    chunk_size: Optional[int] = None,
    overlap: Optional[int] = None,
):
    """ë ˆê±°ì‹œ API í˜¸í™˜: rag_settings(ì‹±ê¸€í†¤) ì—…ë°ì´íŠ¸"""
    cur = get_rag_settings_row()
    new_search = (search_type or cur["search_type"]).lower()
    if new_search == "vector":
        new_search = "semantic"
    if new_search not in {"hybrid", "semantic", "bm25"}:
        raise ValueError(
            "unsupported searchType; allowed: 'hybrid','semantic','bm25' (or 'vector' alias)"
        )
    new_chunk = int(chunk_size if chunk_size is not None else cur["chunk_size"])
    new_overlap = int(overlap if overlap is not None else cur["overlap"])
    if new_chunk <= 0 or new_overlap < 0 or new_overlap >= new_chunk:
        raise ValueError("invalid chunk/overlap (chunk>0, 0 <= overlap < chunk)")

    with get_session() as session:
        s = session.query(RagSettings).filter(RagSettings.id == 1).first()
        if not s:
            s = RagSettings(id=1)
            session.add(s)
        s.search_type = new_search
        s.chunk_size = new_chunk
        s.overlap = new_overlap
        s.updated_at = now_kst()
        session.commit()


# ---------------- Vector Settings ----------------
def set_vector_settings(embed_model_key: Optional[str] = None,
                        search_type: Optional[str] = None,
                        chunk_size: Optional[int] = None,
                        overlap: Optional[int] = None) -> Dict:
    """
    rag_settings ë‹¨ì¼ ì†ŒìŠ¤ë¡œ ì„¤ì • ì €ì¥.
    - ì„ë² ë”© ëª¨ë¸ ë³€ê²½ ì‹œ ê¸°ì¡´ ë°ì´í„° ì¡´ì¬í•˜ë©´ ì°¨ë‹¨, í™œì„± ëª¨ë¸ ê°±ì‹  ë° ìºì‹œ ë¬´íš¨í™”
    - search_type/ì²­í¬/ì˜¤ë²„ë©ì€ rag_settingsì—ë§Œ ë°˜ì˜
    """
    cur = get_vector_settings()
    key_now = cur.get("embeddingModel")
    st_now = (cur.get("searchType") or "hybrid").lower()
    cs_now = int(cur.get("chunkSize") or 512)
    ov_now = int(cur.get("overlap") or 64)

    new_key = embed_model_key or key_now
    new_st = (search_type or st_now).lower()
    # DB ì œì•½ê³¼ ì¼ì¹˜(semantic == vector)
    if new_st == "semantic":
        new_st = "vector"
    if new_st not in {"hybrid", "bm25", "vector"}:
        raise ValueError("unsupported searchType; allowed: 'hybrid','bm25','vector'")

    new_cs = int(chunk_size if chunk_size is not None else cs_now)
    new_ov = int(overlap if overlap is not None else ov_now)
    if new_cs <= 0 or new_ov < 0 or new_ov >= new_cs:
        raise ValueError("invalid chunk/overlap (chunk>0, 0 <= overlap < chunk)")

    if embed_model_key is not None:
        client = get_milvus_client()
        if milvus_has_data(client, collection_name=ADMIN_COLLECTION):
            raise RuntimeError("Milvus ì»¬ë ‰ì…˜ì— ê¸°ì¡´ ë°ì´í„°ê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤. ë¨¼ì € /v1/admin/vector/delete-all ì„ í˜¸ì¶œí•´ ì´ˆê¸°í™”í•˜ì„¸ìš”.")
        set_rag_settings_row(new_search=new_st, new_chunk=new_cs, new_overlap=new_ov, new_key=new_key)
        _invalidate_embedder_cache()

    with get_session() as session:
        s = session.query(RagSettings).filter(RagSettings.id == 1).first()
        if not s:
            s = RagSettings(id=1)
            session.add(s)
        s.embedding_key = new_key
        # search_type/chunk/overlapì€ _update_vector_settingsì—ì„œ ë°˜ì˜ë¨. ì—¬ê¸°ì„  ì¡´ì¬ ì‹œ ë³´ì¡´
        if search_type is not None:
            s.search_type = (
                (search_type or "hybrid").lower().replace("vector", "semantic")
            )
        if chunk_size is not None:
            s.chunk_size = int(chunk_size)
        if overlap is not None:
            s.overlap = int(overlap)
        s.updated_at = now_kst()
        session.commit()

    return get_vector_settings()


def get_vector_settings() -> Dict:
    # rag_settings ëŠ” ê²€ìƒ‰ íƒ€ì…/ì²­í¬/ì˜¤ë²„ë©ë§Œ ì‹ ë¢°
    try:
        row = get_rag_settings_row()
    except Exception:
        logger.error("get_rag_settings_row ì‹¤íŒ¨")
        return {
            "embeddingModel": "unknown",
            "searchType": "hybrid",
            "chunkSize": 512,
            "overlap": 64,
        }
    return {
        "embeddingModel": row.get("embedding_key"),                        # â† rag_settings.embedding_keyëŠ” ë¬´ì‹œ
        "searchType": row.get("search_type", "hybrid"),
        "chunkSize": int(row.get("chunk_size", 512)),
        "overlap": int(row.get("overlap", 64)),
    }


def list_available_embedding_models() -> List[str]:
    """
    ./storage/embedding-models í´ë” ë‚´ì˜ ëª¨ë¸ í´ë”ëª…ë“¤ì„ ë°˜í™˜.
    - embedding_ ì ‘ë‘ì‚¬ê°€ ìˆìœ¼ë©´ ì œê±° (ì˜ˆ: embedding_bge_m3 â†’ bge_m3)
    - í´ë”ë§Œ ë°˜í™˜ (íŒŒì¼ ì œì™¸)
    """
    models = []
    if not MODEL_ROOT_DIR.exists():
        return models
    
    for item in MODEL_ROOT_DIR.iterdir():
        if item.is_dir():
            model_name = item.name
            # embedding_ ì ‘ë‘ì‚¬ ì œê±°
            if model_name.startswith("embedding_"):
                model_name = model_name[len("embedding_"):]
            models.append(model_name)
    
    return sorted(models)

# ------------- Security Level (per task) ---------
def _parse_at_string_to_keywords(value: str) -> List[str]:
    if not value:
        return []
    toks = [t.strip() for t in value.split("@")]
    return [t for t in toks if t]


def _normalize_keywords(val: Any) -> List[str]:
    """
    ë¦¬ìŠ¤íŠ¸/íŠœí”Œ/ì…‹: ê° ì›ì†Œë¥¼ strë¡œ ìºìŠ¤íŒ…, ê³µë°±/í•´ì‹œ ì œê±°
    ë¬¸ìì—´: '@' ê¸°ì¤€ìœ¼ë¡œ í† í°í™”
    ë¹ˆ ê°’ ì œê±° ë° ì¤‘ë³µ ì œê±°
    """
    out: List[str] = []
    if isinstance(val, str):
        toks = [t.strip() for t in val.split("@")]
    elif isinstance(val, (list, tuple, set)):
        toks = [str(t).strip() for t in val]
    else:
        toks = []
    for t in toks:
        if not t:
            continue
        if t.startswith("#"):
            t = t[1:]
        if t and t not in out:
            out.append(t)
    return out


def _normalize_levels(
    levels_raw: Dict[str, Any], max_level: int
) -> Dict[int, List[str]]:
    norm: Dict[int, List[str]] = {}
    for k, v in (levels_raw or {}).items():
        try:
            lv = int(str(k).strip().replace("level_", ""))
        except Exception:
            continue
        if lv < 1 or lv > max_level:
            continue
        kws = _normalize_keywords(v)
        if kws:
            norm[lv] = kws
    return norm


def upsert_security_level_for_task(
    task_type: str, max_level: int, levels_raw: Dict[str, Any]
) -> Dict:
    if task_type not in TASK_TYPES:
        raise ValueError(f"invalid task_type: {task_type}")
    if max_level < 1:
        raise ValueError("maxLevel must be >= 1")

    levels_map = _normalize_levels(levels_raw, max_level)

    with get_session() as session:
        # upsert config
        cfg = (
            session.query(SecurityLevelConfigTask)
            .filter(SecurityLevelConfigTask.task_type == task_type)
            .first()
        )
        if not cfg:
            cfg = SecurityLevelConfigTask(task_type=task_type, max_level=int(max_level))
            session.add(cfg)
        else:
            cfg.max_level = int(max_level)
            cfg.updated_at = now_kst()
        # replace keywords
        session.query(SecurityLevelKeywordsTask).filter(
            SecurityLevelKeywordsTask.task_type == task_type
        ).delete()
        for lv, kws in levels_map.items():
            for kw in kws:
                session.add(
                    SecurityLevelKeywordsTask(
                        task_type=task_type, level=int(lv), keyword=str(kw)
                    )
                )
        session.commit()
        return get_security_level_rules_for_task(task_type)


def get_security_level_rules_for_task(task_type: str) -> Dict:
    with get_session() as session:
        cfg = (
            session.query(SecurityLevelConfigTask)
            .filter(SecurityLevelConfigTask.task_type == task_type)
            .first()
        )
        max_level = int(cfg.max_level) if cfg else 1
        res: Dict[str, Any] = {
            "taskType": task_type,
            "maxLevel": max_level,
            "levels": {str(i): [] for i in range(1, max_level + 1)},
        }
        rows = (
            session.query(
                SecurityLevelKeywordsTask.level, SecurityLevelKeywordsTask.keyword
            )
            .filter(SecurityLevelKeywordsTask.task_type == task_type)
            .order_by(
                SecurityLevelKeywordsTask.level.asc(),
                SecurityLevelKeywordsTask.keyword.asc(),
            )
            .all()
        )
        for lv, kw in rows:
            key = str(int(lv))
            res["levels"].setdefault(key, []).append(str(kw))
        return res


def set_security_level_rules_per_task(config: Dict[str, Dict]) -> Dict:
    """
    config = {
      "doc_gen": {"maxLevel": 3, "levels": {"2": "@ê¸ˆì•¡@ì—°ë´‰", "3": "@ë¶€ì •@í‡´ì§ê¸ˆ"}},
      "summary": {"maxLevel": 2, "levels": {"2": "@ì‚¬ë‚´ë¹„ë°€"}},
      "qna": {"maxLevel": 3, "levels": {"2": "@ì—°êµ¬", "3": "@ê°œì¸ì •ë³´"}}
    }
    """
    with get_session() as session:
        # ì „ì²´ ì‚­ì œ í›„ ì¬ì‚½ì…(ê°„ê²°/ëª…í™•)
        session.query(SecurityLevelConfigTask).delete()
        session.query(SecurityLevelKeywordsTask).delete()
        session.flush()

        for task in TASK_TYPES:
            entry = config.get(task) or {}
            max_level = int(entry.get("maxLevel", 1))
            session.add(
                SecurityLevelConfigTask(task_type=task, max_level=max(1, max_level))
            )
            levels = entry.get("levels", {}) or {}
            for lvl_str, at_str in levels.items():
                try:
                    lvl = int(str(lvl_str).strip().replace("level_", ""))
                except Exception:
                    continue
                if lvl <= 1 or lvl > max_level:
                    continue
                for kw in _parse_at_string_to_keywords(str(at_str)):
                    session.add(
                        SecurityLevelKeywordsTask(
                            task_type=task, level=int(lvl), keyword=str(kw)
                        )
                    )
        session.commit()
        return get_security_level_rules_all()


def get_security_level_rules_all() -> Dict:
    with get_session() as session:
        # ê¸°ë³¸ max_level=1
        max_map = {t: 1 for t in TASK_TYPES}
        for task, max_level in session.query(
            SecurityLevelConfigTask.task_type, SecurityLevelConfigTask.max_level
        ).all():
            max_map[task] = int(max_level)

        res: Dict[str, Dict] = {}
        for task in TASK_TYPES:
            res[task] = {
                "maxLevel": max_map.get(task, 1),
                "levels": {str(i): [] for i in range(1, max_map.get(task, 1) + 1)},
            }

        rows = (
            session.query(
                SecurityLevelKeywordsTask.task_type,
                SecurityLevelKeywordsTask.level,
                SecurityLevelKeywordsTask.keyword,
            )
            .order_by(
                SecurityLevelKeywordsTask.task_type.asc(),
                SecurityLevelKeywordsTask.level.asc(),
                SecurityLevelKeywordsTask.keyword.asc(),
            )
            .all()
        )
        for task, level, kw in rows:
            if task in res:
                lv = str(int(level))
                if lv not in res[task]["levels"]:
                    res[task]["levels"][lv] = []
                res[task]["levels"][lv].append(str(kw))
        return res


# -------------------------------------------------
# 2) ì¸ì œìŠ¤íŠ¸ (bulk)
#   - ì‘ì—…ìœ í˜•ë³„ë¡œ ë™ì¼ ì²­í¬ë¥¼ ê°ê° ì €ì¥(task_type, security_level ë¶„ë¦¬)
# -------------------------------------------------
async def ingest_embeddings(
    model_key: str | None = None,
    target_tasks: list[str] | None = None,
    max_token: int = 512,
    overlab: int = 64,
    collection_name: str = ADMIN_COLLECTION,
    file_keys_filter: list[str] | None = None,
):
    """
    documents í…Œì´ë¸”ì— ì €ì¥ëœ ê´€ë¦¬ì ë¬¸ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸(.txt)ë¥¼ ì¸ì œìŠ¤íŠ¸í•œë‹¤.
    - VARCHAR(32768 bytes) ì´ˆê³¼ ë°©ì§€: split_for_varchar_bytes ë¡œ ì•ˆì „ ë¶„í• 
    - í‘œëŠ” [[TABLE ...]] ë¨¸ë¦¬ê¸€ ìœ ì§€, ì´ì–´ì§€ëŠ” ì¡°ê°ì€ [[TABLE_CONT i/n]] ë§ˆì»¤ë¡œ ì—°ì†ì„± í‘œì‹œ
    - file_keys_filter ì „ë‹¬ ì‹œ doc_id/íŒŒì¼ëª…/ìŠ¤í† ë¦¬ì§€ ê²½ë¡œê°€ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œë§Œ ì¸ì œìŠ¤íŠ¸
    """
    tok, model, device = await _get_or_load_embedder_async(model_key)
    probe_vec = hf_embed_text(tok, model, device, "probe")
    emb_dim = int(probe_vec.shape[0])
    logger.info("[Ingest] ì„ë² ë”© ëª¨ë¸: %s, ë²¡í„° ì°¨ì›: %s", model_key, emb_dim)

    client = get_milvus_client()
    ensure_collection_and_index(client, emb_dim=emb_dim, metric="IP", collection_name=collection_name)

    tasks = [t for t in (target_tasks or TASK_TYPES) if t in TASK_TYPES]
    if not tasks:
        return {"error": f"ìœ íš¨í•œ ì‘ì—…ìœ í˜•ì´ ì—†ìŠµë‹ˆë‹¤. í—ˆìš©: {TASK_TYPES}"}

    documents = _load_admin_documents(file_keys_filter)
    if not documents:
        return {"error": "ê´€ë¦¬ì ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”."}

    doc_ids = [doc["doc_id"] for doc in documents if doc.get("doc_id")]
    metadata_by_doc = fetch_document_metadata_by_doc_ids(doc_ids)
    total_inserted = 0
    BATCH_SIZE = 128

    for doc in documents:
        doc_id = str(doc.get("doc_id") or "").strip()
        if not doc_id:
            continue
        meta_chunks = metadata_by_doc.get(doc_id) or []
        if not meta_chunks:
            logger.warning("[Ingest] metadata missing: doc_id=%s", doc_id)
            continue

        payload = doc.get("payload") or {}
        sec_map = payload.get("security_levels", {}) or {}
        version = int(payload.get("version") or 0)
        chunk_entries: list[dict[str, Any]] = [
            {
                "page": int(entry.get("page") or 0),
                "chunk_idx": int(entry.get("chunk_index") or 0),
                "text": entry.get("text") or "",
                "is_table": bool((entry.get("payload") or {}).get("table")),
            }
            for entry in meta_chunks
            if entry.get("text")
        ]

        for task in tasks:
            lvl = int(sec_map.get(task, 1))
            batch: List[Dict[str, Any]] = []
            batch_meta: List[Dict[str, int]] = []
            vector_records: List[Dict[str, Any]] = []

            def flush_batch_for_task() -> None:
                nonlocal batch, batch_meta, total_inserted
                if not batch:
                    return
                result = client.insert(collection_name=collection_name, data=batch)
                # [Refactor] _extract_insert_ids -> extract_insert_ids (common)
                ids = extract_insert_ids(result)
                for vec_id, meta in zip(ids or [], batch_meta):
                    vector_records.append(
                        {
                            "vector_id": vec_id,
                            "page": meta["page"],
                            "chunk_index": meta["chunk_idx"],
                            "task_type": task,
                        }
                    )
                total_inserted += len(batch)
                batch.clear()
                batch_meta.clear()

            for entry in chunk_entries:
                part = entry["text"]
                vec = hf_embed_text(tok, model, device, part, max_len=max_token)
                if len(vec) != emb_dim:
                    continue
                batch.append(
                    {
                        "embedding": vec.tolist(),
                        "path": "",  # íŒŒì¼ ê²½ë¡œ ëŒ€ì‹  ë¹ˆ ê°’ ë˜ëŠ” doc_id ì‚¬ìš©
                        "chunk_idx": entry["chunk_idx"],
                        "task_type": task,
                        "security_level": lvl,
                        "doc_id": doc_id,
                        "version": version,
                        "page": entry["page"],
                        "workspace_id": 0,
                        "text": part,
                    }
                )
                batch_meta.append(
                    {
                        "page": entry["page"],
                        "chunk_idx": entry["chunk_idx"],
                    }
                )
                if len(batch) >= BATCH_SIZE:
                    flush_batch_for_task()

            flush_batch_for_task()

            if vector_records:
                insert_document_vectors(
                    doc_id=doc_id,
                    collection=collection_name,
                    embedding_version=str(model_key),
                    vectors=vector_records,
                )
    client.flush(collection_name)
    ensure_collection_and_index(client, emb_dim=emb_dim, metric="IP", collection_name=collection_name)

    return {
        "message": f"Ingest ì™„ë£Œ(Milvus Server, collection={collection_name})",
        "inserted_chunks": int(total_inserted),
    }

async def ingest_specific_files_with_levels(
    uploads: Optional[List[Any]] = None,          # FastAPI UploadFile ë¦¬ìŠ¤íŠ¸
    paths: Optional[List[str]] = None,            # ë¡œì»¬ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    tasks: Optional[List[str]] = None,            # ì—†ìœ¼ë©´ ëª¨ë“  TASK_TYPES
    level_for_tasks: Optional[Dict[str, int]] = None,  # {"qna":2,"summary":1} ìš°ì„ 
    level: Optional[int] = None,                  # ê³µí†µ ë ˆë²¨. ìœ„ map ìˆìœ¼ë©´ ë¬´ì‹œ
    collection_name: Optional[str] = None,
):
    if not uploads and not paths:
        return {"error": "ëŒ€ìƒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. uploads ë˜ëŠ” paths ì¤‘ í•˜ë‚˜ëŠ” í•„ìš”í•©ë‹ˆë‹¤."}

    tasks_eff = [t for t in (tasks or TASK_TYPES) if t in TASK_TYPES]
    if not tasks_eff:
        return {"error": f"ìœ íš¨í•œ ì‘ì—…ìœ í˜•ì´ ì—†ìŠµë‹ˆë‹¤. í—ˆìš©: {TASK_TYPES}"}

    lvl_map: Dict[str, int] = {}
    if level_for_tasks:
        for k, v in level_for_tasks.items():
            if k in TASK_TYPES:
                lvl_map[k] = max(1, int(v))
    elif level is not None:
        for t in tasks_eff:
            lvl_map[t] = max(1, int(level))

    # ì—…ë¡œë“œ ì €ì¥(ì„ì‹œ) + ê²½ë¡œ í•©ì¹˜ê¸°
    run_id = uuid.uuid4().hex[:8]
    tmp_root = (VAL_SESSION_ROOT / "adhoc" / run_id).resolve()
    tmp_root.mkdir(parents=True, exist_ok=True)

    saved: List[Path] = []
    if uploads:
        for f in uploads:
            fname = Path(getattr(f, "filename", "uploaded")).name
            tmp_path = tmp_root / fname
            try:
                data = await f.read()
            except Exception:
                data = getattr(getattr(f, "file", None), "read", lambda: b"")()
            tmp_path.write_bytes(data or b"")
            saved.append(tmp_path)
    for p in (paths or []):
        pp = Path(str(p)).resolve()
        if pp.exists() and pp.is_file():
            saved.append(pp)

    if not saved:
        return {"error": "ì €ì¥/ìœ íš¨ì„± ê²€ì‚¬ í›„ ë‚¨ì€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}

    # [Refactor] ingest_commonì„ ì‚¬ìš©í•˜ì—¬ ë¡œì§ ê°„ì†Œí™”
    settings = get_vector_settings()
    coll_eff = collection_name or ADMIN_COLLECTION

    # Callback to handle vector insertion (equivalent to insert_document_vectors)
    def _batch_callback(records: List[Dict[str, Any]], doc_id: str):
        if not records:
            return
        try:
            insert_document_vectors(
                doc_id=doc_id,
                collection=coll_eff,
                embedding_version=settings["embeddingModel"],
                vectors=records,
            )
        except Exception:
            logger.exception(f"document_vectors ê¸°ë¡ ì‹¤íŒ¨(doc_id={doc_id})")

    res = await ingest_common(
        files=saved,
        collection_name=coll_eff,
        task_types=tasks_eff,
        settings=settings,
        # lvl_mapì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ security_level_config ì‚¬ìš©
        override_level_map=lvl_map if lvl_map else None,
        security_level_config=get_security_level_rules_all() if not lvl_map else None,
        doc_id_generator=lambda _base: generate_doc_id(),
        batch_callback=_batch_callback,
    )

    return {
        "message": "Upload & Ingest ì™„ë£Œ",
        "collection": coll_eff,
        "runId": run_id,
        "processed": res.get("processed", []),
        "inserted_chunks": int(res.get("inserted_chunks", 0)),
    }

async def search_documents(req: RAGSearchRequest, 
                            search_type_override: Optional[str] = None,
                            rerank_top_n: Optional[int] = None) -> Dict:
    t0 = time.perf_counter()
    print(f"ğŸ” [Search] ê²€ìƒ‰ ì‹œì‘: query='{req.query}', topK={req.top_k}, rerank_topN={rerank_top_n}, task={req.task_type}")
    
    if req.task_type not in TASK_TYPES:
        return {
            "error": f"invalid task_type: {req.task_type}. choose one of {TASK_TYPES}"
        }

    settings = get_vector_settings()
    model_key = req.model or settings["embeddingModel"]
    raw_st = (search_type_override or settings.get("searchType") or "").lower()
    # alias normalization: 'semantic'/'sementic' -> 'vector'; default 'hybrid' if empty
    search_type = (raw_st.replace("semantic", "vector").replace("sementic", "vector") or "hybrid")

    tok, model, device = await _get_or_load_embedder_async(model_key)
    q_emb = hf_embed_text(tok, model, device, req.query)
    client = get_milvus_client()
    ensure_collection_and_index(client, emb_dim=len(q_emb), metric="IP", collection_name=ADMIN_COLLECTION)

    if ADMIN_COLLECTION not in client.list_collections():
        return {"error": "ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„° ì €ì¥(ì¸ì œìŠ¤íŠ¸)ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”."}

    # ê³µí†µ íŒŒë¼ë¯¸í„°
    embedding_candidates = int(req.top_k)  # ì„ë² ë”©ì—ì„œ ì°¾ì„ í›„ë³´ ê°œìˆ˜
    final_results = int(rerank_top_n) if rerank_top_n is not None else 5  # ìµœì¢… ë°˜í™˜ ê°œìˆ˜
    candidate = max(embedding_candidates, final_results * 2)  # ì¶©ë¶„í•œ í›„ë³´ í™•ë³´
    filter_expr = f"task_type == '{req.task_type}' && security_level <= {int(req.user_level)}"
  
    # === ë¶„ê¸°: ê²€ìƒ‰ ë°©ì‹ ===
    if search_type == "vector":
        raw_results = run_dense_search(
            client,
            collection_name=ADMIN_COLLECTION,
            query_vector=q_emb.tolist(),
            limit=candidate,
            filter_expr=filter_expr,
            output_fields=DEFAULT_OUTPUT_FIELDS,
        )
    else:
        raw_results = run_hybrid_search(
            client,
            collection_name=ADMIN_COLLECTION,
            query_vector=q_emb.tolist(),
            query_text=req.query,
            limit=candidate,
            filter_expr=filter_expr,
            output_fields=DEFAULT_OUTPUT_FIELDS,
        )
    hits_raw = build_dense_hits(raw_results, snippet_loader=lambda _path, _idx: "")

    vector_ids = [str(h["vector_id"]) for h in hits_raw if h.get("vector_id")]
    meta_map = fetch_metadata_by_vector_ids(vector_ids)
    for hit in hits_raw:
        vid = str(hit.get("vector_id") or "")
        meta = meta_map.get(vid)
        if meta:
            hit["doc_id"] = hit.get("doc_id") or meta.get("doc_id")
            hit["chunk_idx"] = meta.get("chunk_index")
            hit["text"] = meta.get("text")
        else:
            hit["snippet"] = ""
            
    rerank_candidates = build_rerank_payload(hits_raw)

    if rerank_candidates:
        reranked = rerank_snippets(rerank_candidates, query=req.query, top_n=final_results)
        hits_sorted = []
        for res in reranked:
            original = res.metadata or {}
            hits_sorted.append(
                {
                    "score": float(res.score),
                    "path": original.get("path"),
                    "chunk_idx": int(original.get("chunk_idx", 0)),
                    "task_type": original.get("task_type"),
                    "security_level": int(original.get("security_level", 1)),
                    "doc_id": original.get("doc_id"),
                    "page": int(original.get("page", 0)),
                    "snippet": res.text,
                }
            )
    else:
        hits_sorted = sorted(
            hits_raw,
            key=lambda x: x.get("score_fused", x.get("score_vec", x.get("score_sparse", 0.0))),
            reverse=True,
        )[:final_results]
    
    # ë¦¬ë­í¬ í›„ ì¤‘ë³µ ì œê±°
    # 1) snippet_text ê¸°ì¤€: ë™ì¼í•œ ë‚´ìš©ì˜ ìŠ¤ë‹ˆí«ì€ í•˜ë‚˜ë§Œ (ìµœê³  ì ìˆ˜ë§Œ ìœ ì§€, doc_id ë¬´ê´€)
    # 2) (doc_id, chunk_idx) ê¸°ì¤€: ê°™ì€ ë¬¸ì„œì˜ ê°™ì€ ì²­í¬ëŠ” í•˜ë‚˜ë§Œ (chunk_idx ì¤‘ë³µ ë°©ì§€)
    # ë¬¸ì„œë‹¹ ì œí•œ ì—†ìŒ - rerank_topNë§Œí¼ ëª¨ë‘ ë°˜í™˜
    seen_by_snippet: dict[str, dict] = {}  # snippet_text -> hit (ìµœê³  ì ìˆ˜ë§Œ ìœ ì§€)
    seen_by_chunk: dict[tuple[str, int], dict] = {}  # (doc_id, chunk_idx) -> hit
    
    original_count = len(hits_sorted)
    
    for hit in hits_sorted:
        doc_id = hit.get("doc_id", "")
        chunk_idx = int(hit.get("chunk_idx", 0))
        snippet = hit.get("snippet", "").strip()
        
        if not snippet:
            continue
        
        chunk_key = (doc_id, chunk_idx)
        
        # 1) snippet_text ì¤‘ë³µ ì²´í¬ - ë™ì¼í•œ ë‚´ìš©ì´ë©´ ì¤‘ë³µ (ë‹¤ë¥¸ ë¬¸ì„œ/ì²­í¬ì—¬ë„)
        if snippet in seen_by_snippet:
            # ë™ì¼í•œ ìŠ¤ë‹ˆí«ì´ ì´ë¯¸ ìˆìœ¼ë©´ ë” ë†’ì€ ì ìˆ˜ë¡œ êµì²´
            existing = seen_by_snippet[snippet]
            if hit.get("score", 0.0) > existing.get("score", 0.0):
                # ê¸°ì¡´ í•­ëª©ì˜ chunk_keyë„ ì œê±°
                old_doc_id = existing.get("doc_id", "")
                old_chunk_idx = int(existing.get("chunk_idx", 0))
                old_chunk_key = (old_doc_id, old_chunk_idx)
                if old_chunk_key in seen_by_chunk:
                    del seen_by_chunk[old_chunk_key]
                # ìƒˆ í•­ëª©ìœ¼ë¡œ êµì²´
                seen_by_snippet[snippet] = hit
                seen_by_chunk[chunk_key] = hit
            continue  # ì¤‘ë³µì´ë¯€ë¡œ ìŠ¤í‚µ
        
        # 2) (doc_id, chunk_idx) ì¤‘ë³µ ì²´í¬ - ê°™ì€ ë¬¸ì„œì˜ ê°™ì€ ì²­í¬ëŠ” í•˜ë‚˜ë§Œ
        if chunk_key in seen_by_chunk:
            # ê°™ì€ (doc_id, chunk_idx)ê°€ ì´ë¯¸ ìˆìœ¼ë©´ ë” ë†’ì€ ì ìˆ˜ë¡œ êµì²´
            existing = seen_by_chunk[chunk_key]
            if hit.get("score", 0.0) > existing.get("score", 0.0):
                # ê¸°ì¡´ í•­ëª©ì˜ snippetë„ ì œê±°
                old_snippet = existing.get("snippet", "").strip()
                if old_snippet in seen_by_snippet and seen_by_snippet[old_snippet] == existing:
                    del seen_by_snippet[old_snippet]
                # ìƒˆ í•­ëª©ìœ¼ë¡œ êµì²´
                seen_by_chunk[chunk_key] = hit
                seen_by_snippet[snippet] = hit
            continue  # ì¤‘ë³µì´ë¯€ë¡œ ìŠ¤í‚µ
        
        # ìƒˆë¡œìš´ í•­ëª© ì¶”ê°€
        seen_by_snippet[snippet] = hit
        seen_by_chunk[chunk_key] = hit
    
    # ì¤‘ë³µ ì œê±°ëœ ê²°ê³¼ë¥¼ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  rerank_topNë§Œí¼ë§Œ ë°˜í™˜
    deduplicated = sorted(seen_by_snippet.values(), key=lambda x: x.get("score", 0.0), reverse=True)
    hits_sorted = deduplicated[:final_results]
    
    logger.info(f"ğŸ” [Deduplication] ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(hits_sorted)}ê°œ ê²°ê³¼ (ì›ë³¸: {original_count}ê°œ, ì œê±°: {original_count - len(hits_sorted)}ê°œ)")

    # ë¦¬ë­í¬ ê²°ê³¼ ë¡œê·¸ ì¶œë ¥
    if hits_sorted:
        top_hit = hits_sorted[0]
        logger.info(f"âœ¨ [Rerank] ì™„ë£Œ! ìµœê³  ì ìˆ˜: {top_hit.get('score', 0):.4f}")
        logger.info(f"ğŸ† [Rerank] ìµœê³  ìŠ¤ë‹ˆí« (doc_id: {top_hit.get('doc_id', 'unknown')}): {top_hit.get('snippet', '')[:100]}...")

    # í”„ë¡¬í”„íŠ¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    context = "\n---\n".join(h["snippet"] for h in hits_sorted if h.get("snippet"))
    prompt = f"ì‚¬ìš©ì ì§ˆì˜: {req.query}\n:\n{context}\n\nìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•´ ì£¼ì„¸ìš”."

    elapsed = round(time.perf_counter() - t0, 4)

    # query_logs ì‚­ì œ: INSERT ì œê±°
    return {
        "elapsed_sec": elapsed,
        "settings_used": {"model": model_key, "searchType": search_type},
        "hits": [
            {
                "score": float(h["score"]),
                "path": h["path"],
                "chunk_idx": int(h["chunk_idx"]),
                "task_type": h["task_type"],
                "security_level": int(h["security_level"]),
                "doc_id": h.get("doc_id"),
                "page": int(h.get("page", 0)),  # í˜ì´ì§€ ì •ë³´ ì¶”ê°€
                "snippet": h["snippet"],
            }
            for h in hits_sorted
        ],
        "prompt": prompt,
    }
    

async def execute_search(
    question: str,
    top_k: int = 20,   # ì„ë² ë”© í›„ë³´ ê°œìˆ˜
    rerank_top_n: int = 5,    # ìµœì¢… ë°˜í™˜ ê°œìˆ˜  
    security_level: int = 1,
    source_filter: Optional[List[str]] = None,
    task_type: str = "qna",
    model_key: Optional[str] = None,
    search_type: Optional[str] = None,
) -> Dict:
    print(f"â­ [ExecuteSearch] í•¨ìˆ˜ í˜¸ì¶œ: question='{question}', topK={top_k}, rerank_topN={rerank_top_n}")
    req = RAGSearchRequest(
        query=question,
        top_k=top_k,
        user_level=security_level,
        task_type=task_type,
        model=model_key,
    )
    logger.info(f"ğŸ“ [ExecuteSearch] search_documents í˜¸ì¶œ ì „: req ìƒì„± ì™„ë£Œ")
    res = await search_documents(req, search_type_override=search_type, rerank_top_n=rerank_top_n)
    logger.info(f"ğŸ“ [ExecuteSearch] search_documents í˜¸ì¶œ í›„: ê²°ê³¼ hits ìˆ˜={len(res.get('hits', []))}")
    # Build check_file BEFORE optional source_filter so it reflects original candidates
    check_files: List[str] = []
    logger.debug(f"\n###########################\nres: {res}")
    try:
        for h in res.get("hits", []):
            # Prefer doc_id when available; fallback to path-derived filename
            doc_id_val = h.get("doc_id")
            if doc_id_val:
                check_files.append(f"{str(doc_id_val)}.pdf")
                continue
            p = Path(h.get("path", ""))
            if str(p):
                check_files.append(p.with_suffix(".pdf").name)
    except Exception:
        pass

    if source_filter and "hits" in res:
        names = {Path(n).stem for n in source_filter}
        res["hits"] = [h for h in res["hits"] if Path(h["path"]).stem in names]

    res["check_file"] = sorted(list(set(check_files)))
    return res



# -------------------------------------------------
# 4) ì‚­ì œ ê´€ë ¨ í•¨ìˆ˜ (Milvus + RDB)
# -------------------------------------------------

async def delete_collection(collection_key: str | None = None):
    COLLECTIONS = app_config["retrieval"]["milvus"]["collections"]
    _invalidate_embedder_cache()
    client = get_milvus_client()
    targets = []

    if collection_key is not None:
        name = COLLECTIONS.get(collection_key)
        targets = [name]
        doc_types = [collection_key]
    else:
        targets = list(COLLECTIONS.values())
        doc_types = list(COLLECTIONS.keys()) 

    dropped = []
    for col in targets:
        if col in client.list_collections():
            client.drop_collection(col)
            dropped.append(col)
    sql_stats = purge_documents_by_collection(doc_types)
    return {"dropped": dropped, "sql": sql_stats}


# -------------------------------------------------
# 5) ê²€ìƒ‰ ê´€ë ¨ í•¨ìˆ˜
# -------------------------------------------------


async def list_indexed_files(
    limit: int = 16384,
    offset: int = 0,
    query: Optional[str] = None,
    task_type: Optional[str] = None,
):
    limit = max(1, min(limit, 16384))
    doc_records = list_documents_by_type(ADMIN_DOC_TYPE)
    doc_map = {doc["doc_id"]: doc for doc in doc_records if doc.get("doc_id")}
    
    rows = get_list_indexed_files(collection_name=ADMIN_COLLECTION, offset=offset, limit=limit, task_type=task_type)

    items: List[Dict[str, Any]] = []
    for doc_id_val, ttype, chunk_count in rows:
        doc_meta = doc_map.get(doc_id_val or "")
        if not doc_meta:
            # doc metadataê°€ ì—†ìœ¼ë©´ ë„˜ì–´ê°
            continue
        file_name = doc_meta.get("filename") or Path(doc_meta.get("source_path") or "").name
        file_path = doc_meta.get("source_path") or ""
        sec_levels = (doc_meta.get("payload") or {}).get("security_levels", {}) or {}
        sec_level = int(sec_levels.get(ttype, doc_meta.get("security_level", 1)))
        items.append(
            {
                "taskType": ttype,
                "fileName": file_name,
                "filePath": file_path,
                "chunkCount": int(chunk_count),
                "indexedAt": doc_meta.get("updated_at"),
                "fileSize": None,
                "securityLevel": sec_level,
            }
        )

    if query:
        q = str(query)
        items = [it for it in items if q in it["fileName"]]
    return items

async def delete_files_by_names(file_names: List[str], task_type: Optional[str] = None):
    """
    íŒŒì¼ëª…(= doc_id stem) ë°°ì—´ì„ ë°›ì•„ ë²¡í„° DBì—ì„œ ì‚­ì œ.
    - task_type ê°€ None ì´ë©´ ëª¨ë“  ì‘ì—…ìœ í˜•(doc_gen/summary/qna)ì—ì„œ ì‚­ì œ (ê¸°ì¡´ ë™ì‘ê³¼ ë™ì¼)
    - task_type ê°€ ì§€ì •ë˜ë©´ í•´ë‹¹ ì‘ì—…ìœ í˜• ë ˆì½”ë“œë§Œ ì‚­ì œ
    """
    if not file_names:
        return {"deleted": 0, "requested": 0}

    try:
        from repository.documents import delete_workspace_documents_by_filenames
    except Exception:
        delete_workspace_documents_by_filenames = None

    client = get_milvus_client()
    milvus_ready = ADMIN_COLLECTION in client.list_collections()

    if milvus_ready:
        try:
            client.load_collection(collection_name=ADMIN_COLLECTION)
        except Exception:
            pass
    else:
        logger.warning("Milvus collection %s not available; skipping vector DB deletion.", ADMIN_COLLECTION)

    # ìœ íš¨í•œ task_type ì¸ì§€ ê²€ì¦
    task_filter = ""
    if task_type:
        if task_type not in TASK_TYPES:
            return {
                "deleted": 0,
                "requested": len(file_names),
                "error": f"invalid taskType: {task_type}",
            }
        task_filter = f" && task_type == '{task_type}'"

    deleted_total = 0
    per_file: dict[str, int] = {}

    doc_ids_to_remove: set[str] = set()
    name_index = _build_doc_name_index()

    for name in file_names:
        raw_name = str(name or "").strip()
        stem = Path(raw_name).stem if raw_name else ""

        doc_id_candidate = None
        for token in filter(None, [raw_name.lower(), stem.lower() if stem else None]):
            doc_id_candidate = name_index.get(token)
            if doc_id_candidate:
                break

        if not doc_id_candidate:
            try:
                base_id, _ver = parse_doc_version(stem or raw_name)
            except Exception:
                base_id = stem or raw_name
            doc_id_candidate = base_id

        if not doc_id_candidate:
            per_file[name] = per_file.get(name, 0)
            continue

        if milvus_ready:
            try:
                filt = f"doc_id == '{doc_id_candidate}'{task_filter}"
                client.delete(collection_name=ADMIN_COLLECTION, filter=filt)
                deleted_total += 1
                per_file[name] = per_file.get(name, 0) + 1
            except Exception:
                logger.exception("Failed to delete from Milvus for file: %s", name)
                per_file[name] = per_file.get(name, 0)
        else:
            per_file[name] = per_file.get(name, 0)

        if task_type:
            delete_document_vectors(doc_id_candidate, task_type)
            if not document_has_vectors(doc_id_candidate):
                if milvus_ready:
                    try:
                        client.delete(collection_name=ADMIN_COLLECTION, filter=f"doc_id == '{doc_id_candidate}'")
                    except Exception:
                        logger.exception("Failed to delete remaining Milvus vectors for doc_id=%s", doc_id_candidate)
                doc_ids_to_remove.add(doc_id_candidate)
        else:
            # ì „ì²´ ì‘ì—…ìœ í˜• ì‚­ì œ ì‹œ SQL/Milvus ëª¨ë‘ ì œê±°
            delete_document_vectors(doc_id_candidate, None)
            if milvus_ready:
                try:
                    client.delete(collection_name=ADMIN_COLLECTION, filter=f"doc_id == '{doc_id_candidate}'")
                except Exception:
                    logger.exception("Failed to delete doc_id=%s from Milvus", doc_id_candidate)
            doc_ids_to_remove.add(doc_id_candidate)

    if milvus_ready:
        # Ensure deletion is visible to subsequent queries (file lists/overview)
        try:
            logger.info("flush Milvus after deletion")
            client.flush(ADMIN_COLLECTION)
        except Exception:
            logger.exception("Failed to flush Milvus after deletion")
        # Force reload to avoid any stale cache/state on the server side
        try:
            client.release_collection(collection_name=ADMIN_COLLECTION)
        except Exception:
            pass
        try:
            client.load_collection(collection_name=ADMIN_COLLECTION)
        except Exception:
            logger.exception("Failed to reload collection after deletion")

    deleted_sql = None
    if delete_workspace_documents_by_filenames:
        try:
            # SQLì€ ì‘ì—…ìœ í˜• êµ¬ë¶„ì´ ì—†ë‹¤ê³  ê°€ì •(ê¸°ì¡´ ê·¸ëŒ€ë¡œ)
            deleted_sql = delete_workspace_documents_by_filenames(file_names)
        except Exception:
            logger.exception("Failed to delete workspace documents in SQL")
            deleted_sql = None

    if doc_ids_to_remove:
        try:
            delete_documents_by_type_and_ids(ADMIN_DOC_TYPE, list(doc_ids_to_remove))
        except Exception:
            logger.exception("Failed to delete admin document metadata for %s", doc_ids_to_remove)

    return {
        "deleted": deleted_total,  # ìš”ì²­ íŒŒì¼ ê¸°ì¤€ ì„±ê³µ ê±´ìˆ˜(ì‘ì—…ìœ í˜• ê¸°ì¤€ ë‹¨ìˆœ ì¹´ìš´íŠ¸)
        "deleted_sql": deleted_sql,
        "requested": len(file_names),
        "taskType": task_type,
        "perFile": per_file,  # íŒŒì¼ë³„ ì²˜ë¦¬í˜„í™©
    }

async def list_indexed_files_overview():
    items = await list_indexed_files(limit=16384, offset=0, query=None, task_type=None)
    # agg: task_type -> level -> count
    agg: Dict[str, Dict[int, int]] = defaultdict(lambda: defaultdict(int))
    for it in items:
        agg[it["taskType"]][int(it["securityLevel"])] += it["chunkCount"]
    # ë³´ê¸° ì¢‹ê²Œ ë³€í™˜
    overview = {
        t: {str(lv): agg[t][lv] for lv in sorted(agg[t].keys())} for t in agg.keys()
    }
    return {"overview": overview, "items": items}


# === ìƒˆ API: í‚¤ì›Œë“œ ì—†ì´ ë ˆë²¨ ì˜¤ë²„ë¼ì´ë“œ í›„ ì¸ì œìŠ¤íŠ¸ ===
class OverrideLevelsRequest(BaseModel):
    """
    ì—…ë¡œë“œ(or ê¸°ì¡´) íŒŒì¼ë“¤ì— ëŒ€í•´ ì‘ì—…ìœ í˜•ë³„ ë ˆë²¨ì„ ê°•ì œë¡œ ì„¸íŒ…í•˜ê³  ì¸ì œìŠ¤íŠ¸.
    - files: ëŒ€ìƒ íŒŒì¼ ì´ë¦„/ê²½ë¡œ(ë¹„ìš°ë©´ META ì „ì²´ ëŒ€ìƒì´ì§€ë§Œ, ë³¸ ì—”ë“œí¬ì¸íŠ¸ì—ì„œëŠ” ì—…ë¡œë“œ íŒŒì¼ë§Œ ì „ë‹¬)
    - level_for_tasks: {"qna":2,"summary":1,"doc_gen":3} (í•„ìˆ˜)
    - tasks: ì‘ì—…ìœ í˜• ì œí•œ (ë¯¸ì§€ì • ì‹œ ëª¨ë“  TASK_TYPES)
    """
    files: Optional[List[str]] = None
    level_for_tasks: Dict[str, int]
    tasks: Optional[List[str]] = None


async def override_levels_and_ingest(req: OverrideLevelsRequest):
    target_tasks = [t for t in (req.tasks or TASK_TYPES) if t in TASK_TYPES]
    if not target_tasks:
        return {"error": "ìœ íš¨í•œ ì‘ì—…ìœ í˜•ì´ ì—†ìŠµë‹ˆë‹¤. (í—ˆìš©: doc_gen|summary|qna)"}

    level_map = {t: int(max(1, lv)) for t, lv in (req.level_for_tasks or {}).items() if t in TASK_TYPES}
    if not level_map:
        return {"error": "ì ìš©í•  ë³´ì•ˆë ˆë²¨ì´ ì—†ìŠµë‹ˆë‹¤. level_for_tasks ë¥¼ ì§€ì •í•˜ì„¸ìš”."}

    documents = _load_admin_documents(req.files)
    if not documents:
        return {"updated": 0, "ingested": 0, "message": "ëŒ€ìƒ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

    updated = 0
    target_tokens: List[str] = []
    for doc in documents:
        doc_id = doc.get("doc_id")
        if not doc_id:
            continue
        payload = dict(doc.get("payload") or {})
        sec = payload.get("security_levels") or {}
        for t in target_tasks:
            if t in level_map:
                sec[t] = int(level_map[t])
        payload["security_levels"] = sec
        upsert_document(
            doc_id=doc_id,
            doc_type=ADMIN_DOC_TYPE,
            filename=doc.get("filename") or doc_id,
            source_path=doc.get("source_path"),
            security_level=_max_security_level(sec),
            payload=payload,
        )
        updated += 1
        target_tokens.append(doc_id)

    settings = get_vector_settings()
    model_key = settings.get("embeddingModel")

    res = await ingest_embeddings(
        model_key=model_key,
        target_tasks=target_tasks,
        collection_name=ADMIN_COLLECTION,
        file_keys_filter=target_tokens,
    )
    return {
        "message": "ë ˆë²¨ ì˜¤ë²„ë¼ì´ë“œ í›„ ì¸ì œìŠ¤íŠ¸ ì™„ë£Œ",
        "collection": ADMIN_COLLECTION,
        "updated_meta_entries": updated,
        "inserted_chunks": int(res.get("inserted_chunks", 0)),
        "target_count": len(target_tokens),
    }