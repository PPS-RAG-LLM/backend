# service/admin/LLM_finetuning.py
from __future__ import annotations

# âœ… UnslothëŠ” transformers/peft ë³´ë‹¤ ë¨¼ì € import
try:
    import unsloth  # noqa: F401
except Exception:
    pass

import os
# ğŸ”§ CUDA ë©”ëª¨ë¦¬ ë‹¨í¸í™” ì™„í™” (ê¶Œì¥)
# expandable_segments:TrueëŠ” ë©”ëª¨ë¦¬ ë‹¨í¸í™”ë¥¼ ì¤„ì´ê³ , max_split_size_mbëŠ” í° ë¸”ë¡ í• ë‹¹ì„ ë°©ì§€
os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "max_split_size_mb:512,expandable_segments:True,roundup_power2_divisions:16")

import json
import threading
import time
import uuid
import sqlite3
import tempfile
import shutil
from dataclasses import dataclass
from datetime import datetime, timezone, timedelta
from typing import Any, Dict, Optional
from pathlib import Path
from contextlib import contextmanager

from pydantic import BaseModel, Field, field_validator, model_validator
from utils import get_db, logger
from errors.exceptions import BadRequestError, InternalServerError

logger = logger(__name__)

import re
from urllib.parse import quote_plus
_FEEDBACK_FILE_RE = re.compile(r"^feedback_(qna|doc_gen|summary)_p(\d+)\.csv$", re.IGNORECASE)

# ===== ë””ë°”ì´ìŠ¤ ìœ í‹¸ =====
def _get_model_device(model):
    try:
        return model.get_input_embeddings().weight.device
    except Exception:
        pass
    try:
        return next(model.parameters()).device
    except Exception:
        pass
    import torch as _torch
    return _torch.device("cuda" if _torch.cuda.is_available() else "cpu")


def _clear_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ìºì‹œë¥¼ ì™„ì „íˆ ì •ë¦¬í•˜ì—¬ ë‹¨í¸í™”ë¥¼ ì¤„ì…ë‹ˆë‹¤."""
    try:
        import torch
        import gc
        if hasattr(torch, "cuda") and torch.cuda.is_available():
            # ëª¨ë“  CUDA ìºì‹œ ì •ë¦¬
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            # Python GC ì‹¤í–‰
            gc.collect()
            # ë‹¤ì‹œ í•œë²ˆ ìºì‹œ ì •ë¦¬
            torch.cuda.empty_cache()
            logger.debug("GPU memory cache cleared")
    except Exception as e:
        logger.warning(f"Failed to clear GPU memory: {e}")


# ===== ìºì‹œ/ì„ì‹œ ë””ë ‰í† ë¦¬ ê´€ë¦¬ =====
@contextmanager
def _ephemeral_cache_env():
    """í›ˆë ¨ ì¤‘ì—ë§Œ ì„ì‹œ ìºì‹œ ë””ë ‰í† ë¦¬ë¥¼ ì‚¬ìš©í•˜ê³  ëë‚˜ë©´ ì‚­ì œ"""
    keys = [
        "HF_HOME", "TRANSFORMERS_CACHE", "HF_DATASETS_CACHE",
        "XDG_CACHE_HOME", "UNSLOTH_CACHE_DIR", "TORCHINDUCTOR_CACHE_DIR",
    ]
    tmpdir = tempfile.mkdtemp(prefix="ft_cache_")
    old = {k: os.environ.get(k) for k in keys}
    try:
        for k in keys:
            os.environ[k] = tmpdir
        yield
    finally:
        for k, v in old.items():
            if v is None:
                os.environ.pop(k, None)
            else:
                os.environ[k] = v
        shutil.rmtree(tmpdir, ignore_errors=True)

# ===== Progress interval (seconds) =====
FT_PROGRESS_INTERVAL_SEC = int(os.getenv("FT_PROGRESS_INTERVAL_SEC", "2"))
# ===== Heartbeat timeout (seconds) for liveness detection =====
FT_HEARTBEAT_TIMEOUT_SEC = int(os.getenv("FT_HEARTBEAT_TIMEOUT_SEC", "300"))
# ===== OOM retry limit =====
MAX_OOM_RETRIES = int(os.getenv("FT_MAX_OOM_RETRIES", "1"))

# ===== Paths =====
BASE_BACKEND = Path(os.getenv("COREIQ_BACKEND_ROOT", str(Path(__file__).resolve().parents[2])))  # backend/
# Force DB to pps_rag.db across the process (can be overridden by env before start)
import os as _os
_os.environ.setdefault("COREIQ_DB", str(BASE_BACKEND / "storage" / "pps_rag.db"))
STORAGE_MODEL_ROOT = os.getenv("STORAGE_MODEL_ROOT", str(BASE_BACKEND / "storage" / "models"))
TRAIN_DATA_ROOT   = os.getenv("TRAIN_DATA_ROOT", str(BASE_BACKEND / "storage" / "train_data"))

# ===== SQLAlchemy ORM (Session) =====
from sqlalchemy import create_engine, select, func
from sqlalchemy.orm import sessionmaker
from storage.db_models import (
    LlmModel, FineTuneDataset, FineTuneJob as ORMJob, FineTunedModel
)
DB_URL = f"sqlite:///{os.environ.get('COREIQ_DB', str(BASE_BACKEND / 'storage' / 'pps_rag.db'))}"
_engine = create_engine(DB_URL, connect_args={"check_same_thread": False})
SessionLocal = sessionmaker(bind=_engine, autoflush=False, autocommit=False)

# ---- Portable path helpers ----
def _to_rel(p: str) -> str:
    """`p`ë¥¼ backend ë£¨íŠ¸ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜(ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜)."""
    try:
        return os.path.relpath(p, BASE_BACKEND)
    except Exception:
        return p

def _to_service_rel(p: str) -> str:
    """
    ì–´ë–¤ ê²½ë¡œë“  ìµœì¢…ì ìœ¼ë¡œ './service/<...>' í˜•íƒœë¡œ í‘œì¤€í™”.
    ì˜ˆ) /.../backend/storage/models/llm/Qwen3-8B  â†’ ./service/storage/models/llm/Qwen3-8B
        storage/models/llm/Qwen3-8B               â†’ ./service/storage/models/llm/Qwen3-8B
        ./service/storage/models/llm/Qwen3-8B     â†’ ê·¸ëŒ€ë¡œ ìœ ì§€
    """
    # ì ˆëŒ€ê²½ë¡œë©´ backend ê¸°ì¤€ ìƒëŒ€ê²½ë¡œë¡œ
    if os.path.isabs(p):
        p = os.path.relpath(p, BASE_BACKEND)
    s = p.replace("\\", "/").lstrip("./")
    if not s.startswith("service/"):
        s = f"service/{s}"
    # ì¤‘ë³µ ìŠ¬ë˜ì‹œ ì •ë¦¬
    while "//" in s:
        s = s.replace("//", "/")
    return f"./{s}"

# ===== Helpers: path resolve =====
def _resolve_model_dir(name_or_path: str) -> str:
    """Resolve a local model directory for finetuning.
    Priority: DB.llm_models.model_path (by exact name, then base-name) â†’ STORAGE_MODEL_ROOT/name.
    ì¸í„°ë„· ë‹¤ìš´ë¡œë“œëŠ” ì‹œë„í•˜ì§€ ì•ŠìŒ.
    """
    if os.path.isabs(name_or_path):
        return name_or_path

    # 1) DBì—ì„œ llm_models í…Œì´ë¸” ì¡°íšŒ
    try:
        with SessionLocal() as s:
            # ì •í™•í•œ ì´ë¦„ìœ¼ë¡œ ë¨¼ì € ì°¾ê¸°
            model = s.execute(
                select(LlmModel).where(LlmModel.name == name_or_path)
            ).scalar_one_or_none()

            if not model:
                # ì¹´í…Œê³ ë¦¬ ì ‘ë¯¸ì‚¬ ì œê±° í›„ ë‹¤ì‹œ ì°¾ê¸°
                def _strip_cat(n: str) -> str:
                    for suf in ("-qna", "-doc_gen", "-summary"):
                        if n.endswith(suf):
                            return n[: -len(suf)]
                    return n

                base_name = _strip_cat(name_or_path)
                if base_name != name_or_path:
                    model = s.execute(
                        select(LlmModel).where(LlmModel.name == base_name)
                    ).scalar_one_or_none()

            if model and model.model_path:
                p = model.model_path
                if os.path.isabs(p):
                    return p
                # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° STORAGE_MODEL_ROOT ê¸°ì¤€ìœ¼ë¡œ í•´ì„
                cand = os.path.join(STORAGE_MODEL_ROOT, p)
                if os.path.isdir(cand):
                    return cand
    except Exception as e:
        logger.warning(f"DB lookup failed for model {name_or_path}: {e}")

    # 2) Fallback to storage root
    return os.path.join(STORAGE_MODEL_ROOT, name_or_path)

def _has_model_signature(dir_path: str) -> bool:
    if not os.path.isdir(dir_path):
        return False
    sigs = [
        "config.json",
        "model.safetensors",
        "pytorch_model.bin",
        "generation_config.json",
    ]
    return any(os.path.isfile(os.path.join(dir_path, s)) for s in sigs)

def _resolve_train_path(p: str) -> str:
    if os.path.isabs(p):
        return p
    cand = os.path.join(TRAIN_DATA_ROOT, p)
    if os.path.isfile(cand):
        return cand
    try:
        for root, _, files in os.walk(TRAIN_DATA_ROOT):
            if p in files:
                return os.path.join(root, p)
    except Exception:
        pass
    return os.path.abspath(p)

# ===== Helpers: detect MXFP4 / gpt-oss =====
def _looks_like_mxfp4_model(dir_or_name: str) -> bool:
    name = (dir_or_name or "").lower()
    if "gpt-oss" in name:
        return True
    # ë¡œì»¬ í´ë”ì¼ ê²½ìš° configì—ì„œ íŒíŠ¸ ì°¾ê¸°
    try:
        cand = os.path.join(dir_or_name, "config.json")
        if os.path.isfile(cand):
            with open(cand, "r", encoding="utf-8") as f:
                cfg = json.load(f)
            txt = json.dumps(cfg).lower()
            if "mxfp4" in txt or "mx" in txt:
                return True
    except Exception:
        pass
    # ì–‘ìí™” ì„¤ì • íŒŒì¼ ì¼€ì´ìŠ¤
    for q in ("quantization_config.json", "quantize_config.json"):
        try:
            cand = os.path.join(dir_or_name, q)
            if os.path.isfile(cand):
                with open(cand, "r", encoding="utf-8") as f:
                    if "mxfp4" in f.read().lower():
                        return True
        except Exception:
            pass
    return False

# ===== Schemas =====
class FineTuneRequest(BaseModel):
    # === ê³µí†µ íƒœê·¸(í•„ìˆ˜) ===
    category: str = Field(..., description="qna | doc_gen | summary")
    subcategory: Optional[str] = Field(None, description="ì„¸ë¶€ í…ŒìŠ¤í¬. í˜„ì¬ ì£¼ë¡œ doc_genì—ì„œ ì‚¬ìš©")

    baseModelName: str
    saveModelName: str
    systemPrompt: str
    batchSize: int = 1
    epochs: int = 3
    learningRate: float = 2e-4
    overfittingPrevention: bool = True
    trainSetFile: str
    gradientAccumulationSteps: int = 16
    quantizationBits: Optional[int] = Field(
        default=None, description="QLORA ì „ìš©: ì–‘ìí™” ë¹„íŠ¸ (4 ë˜ëŠ” 8)",
    )
    tuningType: Optional[str] = Field(
        default="QLORA",
        description="íŒŒì¸íŠœë‹ ë°©ì‹: LORA | QLORA | FULL",
        pattern="^(LORA|QLORA|FULL)$",
    )
    
    @field_validator("tuningType", mode="before")
    @classmethod
    def _v_tuning_type_strip(cls, v):
        """tuningType ê°’ì˜ ì•ë’¤ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤."""
        if v is None:
            return None
        if isinstance(v, str):
            return v.strip()
        return v
    startAt: Optional[str] = Field(
        default=None, description="ì˜ˆì•½ ì‹œì‘ ISO8601 (ì˜ˆ: 2025-09-19T13:00:00)"
    )
    startNow: bool = Field(
        default=False, description="ì¦‰ì‹œ ì‹¤í–‰ ì—¬ë¶€ (True: ë°”ë¡œ ì‹œì‘, False: ì˜ˆì•½ë§Œ ë“±ë¡)"
    )

    @field_validator("category")
    @classmethod
    def _v_category(cls, v: str) -> str:
        allowed = {"qna", "doc_gen", "summary"}
        vv = (v or "").strip().lower()
        if vv not in allowed:
            raise ValueError(f"category must be one of {sorted(allowed)}")
        return vv

    @field_validator("quantizationBits", mode="before")
    @classmethod
    def _v_qbits_empty_to_none(cls, v):
        """
        í¼ì—ì„œ ë¹ˆ ë¬¸ìì—´("")ë¡œ ë„˜ì–´ì˜¤ëŠ” quantizationBitsë¥¼ Noneìœ¼ë¡œ ì²˜ë¦¬.
        FULL/LORAì—ì„œëŠ” quantizationBitsë¥¼ ë¹„ì›Œë„ ë˜ë„ë¡ í—ˆìš©í•˜ê³ ,
        QLORAì¸ ê²½ìš°ì—ë§Œ ì•„ë˜ validator/model_validatorì—ì„œ 4 ë˜ëŠ” 8ì„ ê°•ì œí•œë‹¤.
        """
        if v in ("", None):
            return None
        return v

    @field_validator("quantizationBits")
    @classmethod
    def _v_qbits_range(cls, v: Optional[int]):
        if v is None:
            return v
        if v not in (4, 8):
            raise ValueError("quantizationBits must be 4 or 8 when provided")
        return v

    @model_validator(mode="after")
    def _v_qbits_required_for_qlora(self):
        if (self.tuningType or "").upper() == "QLORA":
            if self.quantizationBits not in (4, 8):
                raise ValueError("QLORA requires quantizationBits=4 or 8")
        return self

@dataclass
class FineTuneJob:
    job_id: str
    category: str
    request: Dict[str, Any]
    status: str = "queued"  # queued | running | succeeded | failed

# ===== Common utils =====
def _now_local_str() -> str:
    """í˜„ì¬ ì‹œê°„ì„ KSTë¡œ ë°˜í™˜"""
    kst = timezone(timedelta(hours=9))
    return datetime.now(kst).strftime("%Y-%m-%d %H:%M:%S")

def _now_utc() -> datetime:
    """í˜„ì¬ ì‹œê°„ì„ UTCë¡œ ë°˜í™˜"""
    return datetime.now(timezone.utc)

def _to_kst(utc_dt: datetime) -> datetime:
    """UTC datetimeì„ KSTë¡œ ë³€í™˜"""
    if utc_dt.tzinfo is None:
        utc_dt = utc_dt.replace(tzinfo=timezone.utc)
    kst = timezone(timedelta(hours=9))
    return utc_dt.astimezone(kst)

def _ensure_output_dir(model_name: str) -> str:
    try:
        os.makedirs(STORAGE_MODEL_ROOT, exist_ok=True)
    except PermissionError as e:
        raise InternalServerError(f"permission denied creating model root '{STORAGE_MODEL_ROOT}': {e}")
    except OSError as e:
        try:
            if getattr(e, "errno", None) == 13:
                raise InternalServerError(f"permission denied creating model root '{STORAGE_MODEL_ROOT}': {e}")
        except Exception:
            pass
        raise

    out_dir = os.path.join(STORAGE_MODEL_ROOT, model_name)
    try:
        os.makedirs(out_dir, exist_ok=True)
    except PermissionError as e:
        raise InternalServerError(f"permission denied creating output dir '{out_dir}': {e}")
    except OSError as e:
        try:
            if getattr(e, "errno", None) == 13:
                raise InternalServerError(f"permission denied creating output dir '{out_dir}': {e}")
        except Exception:
            pass
        raise
    cfg_path = os.path.join(out_dir, "config.json")
    if not os.path.isfile(cfg_path):
        try:
            with open(cfg_path, "w", encoding="utf-8") as f:
                json.dump({"created_at": _now_utc().isoformat()}, f, ensure_ascii=False)
        except Exception:
            logger.exception(f"failed to write config.json inside {out_dir}")
    return out_dir

def _ensure_lora_marker(out_dir: str, tuning_type: str):
    if tuning_type.upper() in ("LORA", "QLORA"):
        marker = os.path.join(out_dir, "adapter_config.json")
        if not os.path.isfile(marker):
            try:
                with open(marker, "w", encoding="utf-8") as f:
                    json.dump({"peft_type": "LORA", "tuning": tuning_type.upper()}, f, ensure_ascii=False)
            except Exception:
                logger.exception(f"failed to create LoRA marker {marker}")

def _append_log(log_path: str, line: str):
    try:
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(line.rstrip("\n") + "\n")
    except Exception:
        # Log but do not interrupt main flow
        logger.exception(f"failed to append log to {log_path}")

# ----- Error helper -----
def _write_error(out_dir: str, message: str):
    """Write error message to a dedicated error.txt inside output dir."""
    try:
        os.makedirs(out_dir, exist_ok=True)
        err_path = os.path.join(out_dir, "error.txt")
        with open(err_path, "a", encoding="utf-8") as f:
            ts = _now_utc().isoformat()
            f.write(f"[{ts}] {message}\n")
    except Exception:
        logger.exception(f"failed to write error file under {out_dir}")


# ===== DB ops =====
def _insert_dataset_if_needed(conn, path: str, category: str) -> int:
    # ==== ORM ë²„ì „ ====
    rel_path = _to_rel(path)
    with SessionLocal() as s:
        exist = s.execute(
            select(FineTuneDataset).where(FineTuneDataset.path == rel_path)
        ).scalar_one_or_none()
        if exist:
            return int(exist.id)
        row = FineTuneDataset(
            name=os.path.basename(path),
            category=category,
            path=rel_path,
            record_count=None,
        )
        s.add(row)
        s.commit()
        s.refresh(row)
        return int(row.id)

def _insert_job(conn, category: str, req: FineTuneRequest, job_id: str, save_name_with_suffix: str, dataset_id: int,
                initial_status: str = "queued", scheduled_at: Optional[str] = None) -> int:
    # ==== ORM ë²„ì „ ====
    reserve_now = _now_local_str()
    train_path = _resolve_train_path(req.trainSetFile)
    train_rel_path = _to_rel(train_path)
    hyper = {
        "systemPrompt": req.systemPrompt,
        "batchSize": req.batchSize,
        "epochs": req.epochs,
        "learningRate": req.learningRate,
        "overfittingPrevention": req.overfittingPrevention,
        "tuningType": (req.tuningType or "QLORA").upper(),
        "baseModelName": req.baseModelName,
        "saveModelName": save_name_with_suffix,
        "trainSetFile": train_rel_path,
        "reserveDate": reserve_now,
        "category": category,
        "subcategory": (req.subcategory or None),
        "gradientAccumulationSteps": req.gradientAccumulationSteps,
        "quantizationBits": req.quantizationBits,
    }
    metrics = {"hyperparameters": hyper}
    if scheduled_at:
        metrics["scheduledAt"] = scheduled_at
        metrics["learningProgress"] = 0

    with SessionLocal() as s:
        row = ORMJob(
            provider_job_id=job_id,
            dataset_id=dataset_id,
            status=initial_status,
            started_at=None,  # ì‹¤ì œ ì‹œì‘ ì‹œì ì— ì„¤ì •
            metrics=json.dumps(metrics, ensure_ascii=False),
        )
        s.add(row)
        s.commit()
        s.refresh(row)
        return int(row.id)

def _update_job_status(conn, job_id: str, status: str, progress: int | None = None, rough: int | None = None, extras: dict | None = None, _retries: int = 3):
    """
    Update job status and optional metrics. Progress percentage is now persisted.
    - If the fine_tune_jobs.metrics column doesn't exist, fall back to updating only status.
    """
    for attempt in range(_retries):
        try:
            cur = conn.cursor()
            has_metrics_col = _has_column(conn, "fine_tune_jobs", "metrics")

            metrics = {}
            if has_metrics_col:
                try:
                    cur.execute("SELECT metrics FROM fine_tune_jobs WHERE provider_job_id=?", (job_id,))
                    row = cur.fetchone()
                    if row and row["metrics"]:
                        try:
                            metrics = json.loads(row["metrics"]) or {}
                        except Exception:
                            metrics = {}
                except Exception:
                    metrics = {}

            # ì§„í–‰ë¥  ë°˜ì˜: ìš”ì²­ê°’ì„ ê·¸ëŒ€ë¡œ ì €ì¥(ì‚¬ìš©ì ìš”ì²­ëŒ€ë¡œ max ì ìš© ì œê±°)
            if progress is not None:
                metrics["learningProgress"] = int(progress)

            if rough is not None:
                metrics["roughScore"] = int(rough)

            if extras:
                try:
                    metrics.update(extras)
                except Exception:
                    pass

            # Update heartbeat timestamp for liveness detection
            try:
                metrics["heartbeatAt"] = _now_utc().isoformat()
            except Exception:
                pass

            if has_metrics_col:
                cur.execute(
                    "UPDATE fine_tune_jobs SET status=?, metrics=? WHERE provider_job_id=?",
                    (status, json.dumps(metrics, ensure_ascii=False), job_id),
                )
            else:
                cur.execute(
                    "UPDATE fine_tune_jobs SET status=? WHERE provider_job_id=?",
                    (status, job_id),
                )
            conn.commit()
            try:
                # Additional: persist status change to train.log for easier tracing
                out_dir = _resolve_out_dir_by_job(conn, job_id)
                if out_dir:
                    log_path = os.path.join(out_dir, "train.log")
                    msg = f"status={status} progress={metrics.get('learningProgress','-')} rough={metrics.get('roughScore','-')} extras={extras or {}}"
                    _append_log(log_path, f"[{_now_utc().isoformat()}] {msg}")
            except Exception:
                pass
            return
        except sqlite3.OperationalError as e:
            if "locked" in str(e).lower() and attempt < _retries - 1:
                time.sleep(0.2 * (attempt + 1))
                continue
            try:
                logger.error(f"update_job_status operational error: {e}")
            except Exception:
                pass
            raise

def _has_column(conn, table: str, column: str) -> bool:
    cur = conn.cursor()
    cur.execute(f"PRAGMA table_info({table})")
    cols = [r[1] for r in cur.fetchall()]
    return column in cols

def _ensure_ftm_rouge_column(conn):
    try:
        if not _has_column(conn, "fine_tuned_models", "rouge1_f1"):
            cur = conn.cursor()
            cur.execute("ALTER TABLE fine_tuned_models ADD COLUMN rouge1_f1 FLOAT")
            conn.commit()
    except Exception:
        # ignore if cannot add (older sqlite)
        pass

def _finish_job_success(conn, job_id: str, model_name: str, category: str, tuning_type: str, final_rouge: Optional[float] = None, subcategory: Optional[str]=None):
    """
    íŒŒì¸íŠœë‹ ì™„ë£Œì‹œ ê²°ê³¼ë¥¼ DBì— ë°˜ì˜í•œë‹¤.
    - llm_models:
        - type: FULL | LORA | QLORA (ëŒ€ë¬¸ì)
        - LORA/QLORA:  model_path=ì–´ëŒ‘í„° í´ë”(ìƒëŒ€), mather_path=ë² ì´ìŠ¤ í´ë”(ìƒëŒ€)
        - FULL:        model_path=í†µì§œ ì €ì¥ í´ë”(ìƒëŒ€), mather_path=NULL
    - fine_tuned_models:
        - type ë™ì¼(ëŒ€ë¬¸ì)
        - rouge1_f1: ìµœì¢… ROUGE ì €ì¥
    """
    # âœ… í•­ìƒ './service/storage/models/llm/<...>' ë¡œ í‘œì¤€í™”
    rel_model_path = _to_service_rel(os.path.join(STORAGE_MODEL_ROOT, model_name))
    mdl_type = (tuning_type or "QLORA").upper()

    with SessionLocal() as s:
        # --- job/metricsì—ì„œ baseModelName ì¶”ì¶œ ---
        job_row = s.execute(select(ORMJob).where(ORMJob.provider_job_id == job_id)).scalar_one_or_none()
        base_model_name = None
        if job_row and job_row.metrics:
            try:
                mt = json.loads(job_row.metrics) or {}
                base_model_name = (mt.get("hyperparameters") or {}).get("baseModelName")
            except Exception:
                base_model_name = None

        # job ìƒíƒœ ì—…ë°ì´íŠ¸
        if job_row:
            job_row.status = "succeeded"
            job_row.finished_at = _now_utc()
            s.add(job_row)

        # --- ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ ì¤€ë¹„ (ìƒëŒ€ ì €ì¥) ---
        base_model_id = None
        base_model_rel_path = None
        if base_model_name:
            abs_base = _resolve_model_dir(base_model_name)   # ë¬¼ë¦¬ ê²½ë¡œ
            base_model_rel_path = _to_service_rel(abs_base)  # ./service/storage/models/llm/<...>
            base_row = s.execute(select(LlmModel).where(LlmModel.name == base_model_name)).scalar_one_or_none()
            if not base_row:
                base_row = LlmModel(
                    provider="huggingface",
                    name=base_model_name,
                    revision=0,
                    model_path=None,       # BASEëŠ” ì–´ëŒ‘í„° ì—†ìŒ
                    mather_path=base_model_rel_path,
                    category="all",
                    # subcategoryëŠ” BASEì—” ì ìš©í•˜ì§€ ì•ŠìŒ
                    type="BASE",
                    is_default=False,
                    is_active=True,
                    trained_at=None,
                )
                s.add(base_row)
                s.flush()
            base_model_id = int(base_row.id)

        # --- FT ê²°ê³¼ ëª¨ë¸(LlmModel) upsert ---
        m = s.execute(select(LlmModel).where(LlmModel.name == model_name)).scalar_one_or_none()
        if m is None:
            m = LlmModel(
                provider="huggingface",
                name=model_name,
                revision=0,
                category=category,
                # ì„œë¸Œì¹´í…Œê³ ë¦¬ ì œì•½ ì œê±°(ë©”íƒ€ë§Œ ë³´ìœ ), ê³¼ê±° ë°ì´í„° í˜¸í™˜ì„ ìœ„í•´ Noneìœ¼ë¡œ ì €ì¥
                type=mdl_type,
                is_default=False,
                is_active=True,
                trained_at=_now_utc(),
            )

        # íƒ€ì…ë³„ ê²½ë¡œ ì €ì¥ ì •ì±…
        if mdl_type in ("LORA", "QLORA"):
            # ì–´ëŒ‘í„° ê²½ë¡œëŠ” model_path, ë² ì´ìŠ¤ëŠ” mather_path
            m.model_path = rel_model_path
            m.mather_path = base_model_rel_path
        else:  # FULL
            m.model_path = rel_model_path  # í†µì§œ ì €ì¥ í´ë”
            m.mather_path = None

        m.category = category
        m.is_active = True
        m.trained_at = _now_utc()
        s.add(m)
        s.flush()  # m.id í™•ë³´

        # --- FineTunedModel insert ---
        ftm = FineTunedModel(
            model_id=m.id,
            job_id=job_row.id if job_row else None,
            provider_model_id=model_name,
            lora_weights_path=(rel_model_path if mdl_type in ("LORA", "QLORA") else None),
            type=mdl_type,                    # ëŒ€ë¬¸ì ì €ì¥
            is_active=True,
            base_model_id=base_model_id,
            base_model_path=base_model_rel_path,
        )
        try:
            setattr(ftm, "rouge1_f1", (final_rouge if final_rouge is not None else None))
        except Exception:
            pass

        s.add(ftm)
        s.commit()

def _resolve_out_dir_by_job(conn, job_id: str) -> Optional[str]:
    cur = conn.cursor()
    # First try portable path: metrics only
    cur.execute("SELECT metrics FROM fine_tune_jobs WHERE provider_job_id=?", (job_id,))
    row = cur.fetchone()
    save_name = None
    if row and row["metrics"]:
        try:
            mt = json.loads(row["metrics"]) or {}
            hp = mt.get("hyperparameters") or {}
            save_name = hp.get("saveModelName")
        except Exception:
            save_name = None
    # If still missing, try optional hyperparameters column
    if not save_name:
        try:
            cur.execute("SELECT hyperparameters FROM fine_tune_jobs WHERE provider_job_id=?", (job_id,))
            r2 = cur.fetchone()
            if r2 and r2["hyperparameters"]:
                try:
                    hp = json.loads(r2["hyperparameters"]) or {}
                    save_name = hp.get("saveModelName")
                except Exception:
                    save_name = None
        except Exception:
            # column may not exist in this DB
            pass
    if not save_name:
        return None
    return os.path.join(STORAGE_MODEL_ROOT, save_name)


# ===== ì‹œë®¬ë ˆì´í„° ì œê±°ë¨ =====

# ===== Training (inline, real) =====
def _run_training_inline(job: FineTuneJob, save_name_with_suffix: str):
    """
    Unsloth gpt-oss(20B) ë…¸íŠ¸ë¶ íë¦„ì„ ë°˜ì˜í•œ ê²½ëŸ‰/ì•ˆì • íŒŒì´í”„ë¼ì¸.
    - gpt-oss(MXFP4) â†’ Unsloth FastLanguageModel + 4bit LoRA
    - ê·¸ ì™¸ QLORA â†’ BitsAndBytes 4bit + LoRA
    - LORA/FULL ê²½ë¡œëŠ” ê¸°ì¡´ ë¡œì§ ìœ ì§€ (í•„ìš” ì‹œ ë™ì¼ íŒ¨í„´ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥)
    """
    import gc
    conn = get_db()
    try:
        _update_job_status(conn, job.job_id, "running", progress=0)

        out_dir = _ensure_output_dir(save_name_with_suffix)
        tuning_type = (job.request.get("tuningType") or "QLORA").upper()
        if tuning_type in ("LORA", "QLORA"):
            _ensure_lora_marker(out_dir, tuning_type)
        log_path = os.path.join(out_dir, "train.log")
        try:
            with open(log_path, "w", encoding="utf-8") as _tmp:
                _tmp.write("")
        except Exception:
            pass
        _append_log(log_path, f"[{_now_utc().isoformat()}] job {job.job_id} started (INLINE)")
        logger.info(
            f"Fine-tuning started jobId={job.job_id} type={tuning_type} base={job.request.get('baseModelName')} "
            f"save={save_name_with_suffix} data={job.request.get('trainSetFile')}"
        )

        # âœ… ì„ì‹œ ìºì‹œ í´ë”
        tmpdir = tempfile.mkdtemp(prefix="ft_cache_")
        cache_keys = ["HF_HOME", "TRANSFORMERS_CACHE", "HF_DATASETS_CACHE", "XDG_CACHE_HOME", "UNSLOTH_CACHE_DIR", "TORCHINDUCTOR_CACHE_DIR"]
        old_cache = {k: os.environ.get(k) for k in cache_keys}
        for k in cache_keys:
            os.environ[k] = tmpdir

        # ===== Imports =====
        try:
            from unsloth import FastLanguageModel  # type: ignore
            import pandas as pd
            import torch
            from transformers import (
                AutoModelForCausalLM,
                AutoTokenizer,
                TrainingArguments,
                Trainer,
                BitsAndBytesConfig,
                TrainerCallback,
            )
        except Exception as e:
            _append_log(log_path, f"[{_now_utc().isoformat()}] import error: {e}")
            _update_job_status(conn, job.job_id, "failed", extras={"error": f"import error: {e}"})
            logger.error(f"Fine-tuning failed jobId={job.job_id} error=import error: {e}")
            return

        # ===== ë°ì´í„° ì ì¬/ì „ì²˜ë¦¬ =====
        system_prompt = job.request.get("systemPrompt") or "You are Qwen, a helpful assistant."
        def build_prompt(context: str, question: str) -> str:
            return f"{context.strip()}\n{system_prompt}\nQuestion: {question.strip()}"

        csv_path = _resolve_train_path(job.request.get("trainSetFile"))
        encodings_to_try = ["utf-8", "utf-8-sig", "cp949", "euc-kr", "latin1"]
        df = None
        for enc in encodings_to_try:
            try:
                df = pd.read_csv(csv_path, encoding=enc, dtype=str).fillna("")
                break
            except Exception:
                continue
        if df is None:
            try:
                import chardet
                with open(csv_path, "rb") as f:
                    raw = f.read(10000)
                enc = chardet.detect(raw).get("encoding") or "utf-8"
                df = pd.read_csv(csv_path, encoding=enc, dtype=str, on_bad_lines="skip").fillna("")
            except Exception as e:
                _append_log(log_path, f"[{_now_utc().isoformat()}] csv load failed: {e}")
                _update_job_status(conn, job.job_id, "failed", extras={"error": f"csv load failed: {e}"})
                logger.error(f"Fine-tuning failed jobId={job.job_id} error=csv load failed: {e}")
                return

        conversations = [{
            "conversations": [
                {"from": "user", "value": build_prompt(r.get("ChunkContext",""), r.get("Question",""))},
                {"from": "assistant", "value": r.get("Answer","")},
            ]
        } for _, r in df.iterrows()]

        # ===== 7:3 ê³ ì • split (random_state=42) =====
        total = len(conversations)
        eval_size = max(1, int(round(total * 0.3))) if total >= 2 else 0
        if eval_size > 0:
            import random as _R
            idx = list(range(total))
            rng = _R.Random(42)
            rng.shuffle(idx)
            eval_idx = set(idx[:eval_size])
            train_data = [conversations[i] for i in idx[eval_size:]]
            eval_data  = [conversations[i] for i in idx[:eval_size]]
        else:
            train_data, eval_data = conversations, []
        _append_log(log_path, f"[{_now_utc().isoformat()}] split: total={total} eval={eval_size} train={len(train_data)}")

        class RagDataset(torch.utils.data.Dataset):
            def __init__(self, data, tokenizer, max_len=4096):
                self.data = data
                self.tk = tokenizer
                self.max_len = max_len
            def __len__(self): return len(self.data)
            def __getitem__(self, i):
                dialog = self.data[i]["conversations"]
                messages = [
                    {"role": "system", "content": "You are Qwen, a helpful assistant."},
                    {"role": "user", "content": dialog[0]["value"]},
                    {"role": "assistant", "content": dialog[1]["value"]},
                ]
                full = self.tk.apply_chat_template(messages, tokenize=False)
                enc = self.tk(full, max_length=self.max_len, truncation=True, padding="max_length", return_tensors="pt")
                input_ids = enc.input_ids[0]
                assist = self.tk.apply_chat_template(messages[:-1], tokenize=False, add_generation_prompt=True)
                prefix_len = len(self.tk(assist).input_ids)
                labels = input_ids.clone()
                labels[:prefix_len] = -100
                return {"input_ids": input_ids, "attention_mask": enc.attention_mask[0], "labels": labels}

        base_folder = _resolve_model_dir(job.request.get("baseModelName"))
        if not _has_model_signature(base_folder):
            msg = f"base model not found: {base_folder}"
            _append_log(log_path, f"[{_now_utc().isoformat()}] {msg}")
            _update_job_status(conn, job.job_id, "failed", extras={"error": msg})
            logger.error(f"Fine-tuning failed jobId={job.job_id} error={msg}")
            return

        model_path = base_folder
        output_dir = os.path.join(STORAGE_MODEL_ROOT, save_name_with_suffix)
        is_mxfp4 = _looks_like_mxfp4_model(model_path) or _looks_like_mxfp4_model(job.request.get("baseModelName"))

        # ===== ëª¨ë¸ ë¡œë”© ì „ ë©”ëª¨ë¦¬ ì •ë¦¬ =====
        # _clear_gpu_memory()

        # ===== ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ =====
        # gpt-oss(MXFP4) â†’ Unsloth
        if tuning_type == "QLORA" and is_mxfp4:
            max_len = int(job.request.get("max_len", 3072))  # ğŸ’¡ ê¸°ë³¸ 3072ë¡œ ì‚´ì§ ë‚®ì¶° OOM ì˜ˆë°©
            # ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€ë¥¼ ìœ„í•´ ë¡œë”© ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
            # _clear_gpu_memory()
            model, tokenizer = FastLanguageModel.from_pretrained(
                model_name=model_path,
                dtype=None,                    # H100 â†’ bf16 ìë™
                max_seq_length=max_len,
                load_in_4bit=True,
                full_finetuning=False,
                trust_remote_code=True,
                local_files_only=True,
            )
            # ë¡œë”© í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
            # _clear_gpu_memory()
            # Unsloth ëª¨ë²”ì‚¬ë¡€: í•™ìŠµ ìµœì í™” í™œì„±í™”
            try:
                model = FastLanguageModel.for_training(model)  # ì¼ë¶€ ë²„ì „ì—ì„  in-place. ë°˜í™˜ê°’ í˜¸í™˜.
            except Exception:
                pass
            try:
                model = FastLanguageModel.get_peft_model(
                    model,
                    r=64,
                    lora_alpha=16,
                    lora_dropout=0.05,
                    bias="none",
                )
            except Exception:
                from peft import LoraConfig, get_peft_model  # type: ignore
                lora_cfg = LoraConfig(r=64, lora_alpha=16, lora_dropout=0.05, bias="none", task_type="CAUSAL_LM")
                model = get_peft_model(model, lora_cfg)

        elif tuning_type == "QLORA":
            # ì¼ë°˜ QLORA
            from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training  # type: ignore
            bnb = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16,
            )
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, local_files_only=True)
            if tokenizer.pad_token_id is None:
                if getattr(tokenizer, "eos_token_id", None) is not None:
                    tokenizer.pad_token_id = tokenizer.eos_token_id
                else:
                    tokenizer.add_special_tokens({"pad_token": "<|pad|>"})
                    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("<|pad|>")
            # ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€ë¥¼ ìœ„í•´ ë¡œë”© ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
            # _clear_gpu_memory()
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                trust_remote_code=True,
                device_map="auto",
                quantization_config=bnb,
                local_files_only=True,
            )
            model.gradient_checkpointing_enable()
            model = prepare_model_for_kbit_training(model)
            # ë¡œë”© í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
            # _clear_gpu_memory()
            lora_targets = [
                "q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj",
                "down_proj","w1","w2","c_proj","c_attn"
            ]
            from itertools import chain
            target_modules = sorted({
                n.split(".")[-1]
                for n, _ in model.named_modules()
                if any(k in n for k in lora_targets)
            })
            lora_cfg = LoraConfig(
                r=64, lora_alpha=16, target_modules=(target_modules or None),
                lora_dropout=0.05, bias="none", task_type="CAUSAL_LM",
            )
            model = get_peft_model(model, lora_cfg)
            max_len = int(job.request.get("max_len", 4096))
        elif tuning_type == "LORA":
            from peft import LoraConfig, get_peft_model  # type: ignore
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, local_files_only=True)
            if tokenizer.pad_token_id is None:
                if getattr(tokenizer, "eos_token_id", None) is not None:
                    tokenizer.pad_token_id = tokenizer.eos_token_id
                else:
                    tokenizer.add_special_tokens({"pad_token": "<|pad|>"})
                    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("<|pad|>")
            # ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€ë¥¼ ìœ„í•´ ë¡œë”© ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
            # _clear_gpu_memory()
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                trust_remote_code=True,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                local_files_only=True,
            )
            model.gradient_checkpointing_enable()
            # ë¡œë”© í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
            # _clear_gpu_memory()
            targets = [
                "q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj",
                "down_proj","w1","w2","c_proj","c_attn"
            ]
            target_modules = sorted({ n.split(".")[-1] for n,_ in model.named_modules() if any(k in n for k in targets) })
            lora_cfg = LoraConfig(r=64, lora_alpha=16, target_modules=(target_modules or None), lora_dropout=0.05, bias="none", task_type="CAUSAL_LM")
            model = get_peft_model(model, lora_cfg)
            max_len = int(job.request.get("max_len", 4096))
        else:  # FULL
            # ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€ë¥¼ ìœ„í•´ ë¡œë”© ì „ ë©”ëª¨ë¦¬ ì •ë¦¬
            # _clear_gpu_memory()
            tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, local_files_only=True)
            if tokenizer.pad_token_id is None:
                if getattr(tokenizer, "eos_token_id", None) is not None:
                    tokenizer.pad_token_id = tokenizer.eos_token_id
                else:
                    tokenizer.add_special_tokens({"pad_token": "<|pad|>"})
                    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("<|pad|>")
            model = AutoModelForCausalLM.from_pretrained(
                model_path, trust_remote_code=True, device_map="auto", torch_dtype=torch.bfloat16, local_files_only=True,
            )
            model.gradient_checkpointing_enable()
            for p in model.parameters(): p.requires_grad = True
            # ë¡œë”© í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
            # _clear_gpu_memory()
            # FULL íŒŒì¸íŠœë‹ì€ ë©”ëª¨ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•˜ë¯€ë¡œ ê¸°ë³¸ max_lenì„ ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •
            max_len = int(job.request.get("max_len", 2048))  # ê¸°ë³¸ê°’ì„ 4096ì—ì„œ 2048ë¡œ ê°ì†Œ

        # ===== ë°ì´í„°ì…‹ ìƒì„± =====
        train_ds = RagDataset(train_data, tokenizer, max_len=max_len)
        eval_ds  = RagDataset(eval_data,  tokenizer, max_len=max_len) if eval_size > 0 else None
        use_eval = eval_ds is not None and len(eval_data) > 0

        # ===== TrainingArguments (ë²„ì „ í˜¸í™˜ í‚¤ ìë™ ë§¤í•‘) =====
        from inspect import signature as _sig
        def _supported_args(cls): return set(_sig(cls.__init__).parameters.keys())
        def _put_kw(supported: set, kw: dict, key: str, value, *aliases: str):
            for k in (key, *aliases):
                if k in supported:
                    kw[k] = value
                    return k
            return None

        optim_name = os.getenv("FT_OPTIM", ("adamw_torch" if tuning_type in ("FULL","LORA") else "paged_adamw_8bit"))
        save_strategy_val = "epoch" if use_eval else "no"
        eval_strategy_val = "epoch" if use_eval else "no"

        _ta = dict(
            output_dir=output_dir,
            num_train_epochs=job.request.get("epochs", 3),
            per_device_train_batch_size=job.request.get("batchSize", 1),
            gradient_accumulation_steps=job.request.get("gradientAccumulationSteps", 8),
            learning_rate=job.request.get("learningRate", 2e-4),
            bf16=True,
            logging_steps=10,
            report_to="none",
            optim=optim_name,
            seed=42,
            data_seed=42,
            save_total_limit=2,
            warmup_ratio=0.05,
        )
        sup = _supported_args(TrainingArguments)
        _put_kw(sup, _ta, "save_strategy", save_strategy_val)
        _put_kw(sup, _ta, "evaluation_strategy", eval_strategy_val, "eval_strategy")
        _put_kw(sup, _ta, "gradient_checkpointing", True)
        _put_kw(sup, _ta, "lr_scheduler_type", "cosine")
        _put_kw(sup, _ta, "save_safetensors", True)
        if use_eval:
            _put_kw(sup, _ta, "load_best_model_at_end", True)
            _put_kw(sup, _ta, "metric_for_best_model", "eval_loss")
            _put_kw(sup, _ta, "greater_is_better", False)
            _put_kw(sup, _ta, "per_device_eval_batch_size", max(1, job.request.get("batchSize", 1)))

        training_args = TrainingArguments(**_ta)

        # ===== ì½œë°± =====
        class ProgressCallback(TrainerCallback):
            def __init__(self, job_id: str, every_steps: int | None = None):
                self.job_id = job_id
                self.every_steps = every_steps if every_steps else max(1, int(os.getenv("FT_PROGRESS_EVERY_STEPS", "1")))
                self.total_steps = None
            def on_train_begin(self, args, state, control, **kw):
                c = get_db(); _update_job_status(c, self.job_id, "running", progress=0); c.close()
            def on_train_begin_dataloader(self, args, state, control, **kw):
                try:
                    if state.max_steps and state.max_steps > 0:
                        self.total_steps = int(state.max_steps)
                except Exception:
                    pass
            def on_step_end(self, args, state, control, **kw):
                try:
                    total = self.total_steps or (state.max_steps or 0)
                    if not total or state.global_step % self.every_steps != 0: return
                    pct = int(min(100, max(0, round((state.global_step/total)*100))))
                    c = get_db(); _update_job_status(c, self.job_id, "running", progress=pct); c.close()
                except Exception:
                    pass
            def on_train_end(self, args, state, control, **kw):
                c = get_db(); _update_job_status(c, self.job_id, "running", progress=100); c.close()

        class LogCallback(TrainerCallback):
            def on_log(self, args, state, control, logs=None, **kw):
                if logs and "loss" in logs:
                    _append_log(log_path, f"[{_now_utc().isoformat()}] step={state.global_step} loss={logs['loss']}")

        def _build_trainer():
            return Trainer(
                model=model,
                args=training_args,
                train_dataset=train_ds,
                eval_dataset=eval_ds,
                tokenizer=tokenizer,  # FutureWarningì€ ë¬´ì‹œ ê°€ëŠ¥ (ì¶”í›„ processing_classë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜)
                callbacks=[ProgressCallback(job.job_id), LogCallback()],
            )

        trainer = _build_trainer()

        # ===== í•™ìŠµ + OOM ì„¸ì´í”„ ì¬ì‹œë„ =====
        import math
        try:
            _append_log(log_path, f"[{_now_utc().isoformat()}] training started...")
            trainer.train()
        except RuntimeError as re:
            if "out of memory" in str(re).lower():
                # ğŸ”» ë°°ì¹˜/ì‹œí€€ìŠ¤ ë™ì‹œ ì¶•ì†Œ + **ì´ì „ Trainer/ê·¸ë˜í”„ ì™„ì „ ì •ë¦¬**
                old_bs = training_args.per_device_train_batch_size
                new_bs = max(1, old_bs // 2)
                new_len = max(1024, int(max_len * 0.75))
                _append_log(log_path, f"[{_now_utc().isoformat()}] OOM â†’ retry with batch={new_bs}, max_len={new_len}")
                # ë©”ëª¨ë¦¬ í•´ì œ
                try:
                    del trainer
                    # _clear_gpu_memory()
                except Exception:
                    pass
                # ì¬êµ¬ì„±
                training_args.per_device_train_batch_size = new_bs
                train_ds = RagDataset(train_data, tokenizer, max_len=new_len)
                eval_ds  = RagDataset(eval_data,  tokenizer, max_len=new_len) if use_eval else None
                trainer = Trainer(
                    model=model,
                    args=training_args,
                    train_dataset=train_ds,
                    eval_dataset=eval_ds,
                    tokenizer=tokenizer,
                    callbacks=[ProgressCallback(job.job_id), LogCallback()],
                )
                trainer.train()
            else:
                raise

        # ===== ê°„ë‹¨ ROUGE-1 (ì˜µì…”ë„) =====
        def _rouge1_f1(ref: str, cand: str) -> float:
            rw, cw = ref.split(), cand.split()
            if not rw or not cw: return 0.0
            m = sum(1 for w in cw if w in rw)
            p = m/len(cw) if cw else 0.0
            r = m/len(rw) if rw else 0.0
            return 0.0 if (p+r)==0 else 2*(p*r)/(p+r)

        final_rouge = None
        if use_eval:
            model.eval()
            preds, refs = [], []
            device = _get_model_device(model)
            for item in eval_ds:
                inp_ids = item["input_ids"].unsqueeze(0).to(device)
                attn = item["attention_mask"].unsqueeze(0).to(device)
                try:
                    with torch.inference_mode():
                        out_ids = model.generate(
                            inp_ids,
                            attention_mask=attn,
                            max_new_tokens=128,
                            do_sample=False,
                        )[0]
                except Exception:
                    continue
                preds.append(tokenizer.decode(out_ids, skip_special_tokens=True))
                ref_ids = item["labels"]
                ref_ids = torch.where(ref_ids == -100, torch.tensor(tokenizer.pad_token_id), ref_ids)
                refs.append(tokenizer.decode(ref_ids, skip_special_tokens=True))
            scores = [_rouge1_f1(r, p) for r, p in zip(refs, preds)]
            if scores: final_rouge = sum(scores) / len(scores)
            _update_job_status(conn, job.job_id, "running", rough=int((final_rouge or 0)*100), extras={"rouge1F1": final_rouge})

        # ===== ì €ì¥ =====
        def _save_stage(stage: str, pct: int):
            _append_log(log_path, f"[{_now_utc().isoformat()}] save:{stage} {pct}%")
            c = get_db(); _update_job_status(c, job.job_id, "running", extras={"saveStage": stage, "saveProgress": pct}); c.close()

        _save_stage("start", 5)
        _append_log(log_path, f"[{_now_utc().isoformat()}] saving model...")

        if tuning_type in ("LORA", "QLORA"):
            try:
                model.save_pretrained(output_dir)     # ì–´ëŒ‘í„°ë§Œ ì €ì¥
                _save_stage("adapter", 70)
                tokenizer.save_pretrained(output_dir)
                _save_stage("tokenizer", 90)
                _append_log(log_path, f"[{_now_utc().isoformat()}] saved adapters â†’ {output_dir}")
            except Exception as e:
                trainer.save_model(output_dir)
                _save_stage("model", 70)
                tokenizer.save_pretrained(output_dir)
                _save_stage("tokenizer", 90)
                _append_log(log_path, f"[{_now_utc().isoformat()}] fallback save (trainer.save_model): {e}")
            # MXFP4 ë³‘í•© ì €ì¥ì€ ì„ íƒ(ê¸°ë³¸ ë¹„í™œì„±)
            if is_mxfp4 and os.getenv("FT_UNSLOTH_MERGE_SAVE","0") == "1":
                try:
                    model.save_pretrained_merged(output_dir, tokenizer, save_method="mxfp4")
                    _append_log(log_path, f"[{_now_utc().isoformat()}] merged MXFP4 saved â†’ {output_dir}")
                except Exception as e:
                    _append_log(log_path, f"[{_now_utc().isoformat()}] MXFP4 merge save failed: {e}, adapters only.")
        else:
            trainer.save_model(output_dir)
            _save_stage("model", 70)
            tokenizer.save_pretrained(output_dir)
            _save_stage("tokenizer", 90)

        _save_stage("done", 100)

        _finish_job_success(
            conn, job.job_id, save_name_with_suffix, job.category, tuning_type, final_rouge, subcategory=job.request.get("subcategory"),
        )
        logger.info(f"Fine-tuning succeeded jobId={job.job_id} save={save_name_with_suffix} type={tuning_type}")
        _append_log(log_path, f"[{_now_utc().isoformat()}] job {job.job_id} succeeded")

    except Exception as e:
        logger.error(f"Fine-tuning failed jobId={job.job_id} error={e}")
        try:
            _append_log(os.path.join(STORAGE_MODEL_ROOT, save_name_with_suffix, "train.log"), f"[{_now_utc().isoformat()}] error: {e}")
        except Exception:
            pass
        c = get_db()
        try:
            _update_job_status(c, job.job_id, "failed", progress=0, extras={"error": str(e)})
        finally:
            c.close()
        try:
            cur = conn.cursor()
            cur.execute("UPDATE fine_tune_jobs SET status=? WHERE provider_job_id=?", ("failed", job.job_id))
            conn.commit()
        except Exception:
            pass
    finally:
        # ìì› ì •ë¦¬
        try:
            del trainer
        except Exception:
            pass
        try:
            del model, tokenizer
        except Exception:
            pass
        try:
            import torch
            gc.collect()
            if hasattr(torch, "cuda") and torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
        except Exception:
            pass
        try:
            for k, v in old_cache.items():
                if v is None: os.environ.pop(k, None)
                else: os.environ[k] = v
            shutil.rmtree(tmpdir, ignore_errors=True)
        except Exception:
            pass
        conn.close()

# ===== Public APIs =====
def _log_to_save_name(save_name_with_suffix: str, message: str):
    try:
        out_dir = _ensure_output_dir(save_name_with_suffix)
        log_path = os.path.join(out_dir, "train.log")
        _append_log(log_path, f"[{_now_utc().isoformat()}] {message}")
    except Exception:
        pass

def start_fine_tuning(category: str, body: FineTuneRequest) -> Dict[str, Any]:
    # startNowì™€ startAt ë™ì‹œ ì§€ì • ë°©ì§€
    if body.startNow and body.startAt:
        raise BadRequestError("startNowì™€ startAtì€ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ)")

    # body.categoryë¥¼ ìµœì¢… ì‹ ë¢°(í•˜ìœ„í˜¸í™˜ì„ ìœ„í•´ ì¸ìˆ˜ categoryëŠ” fallback)
    category = (body.category or category).lower()
    suffix = (body.tuningType or "QLORA").upper()
    # ì¹´í…Œê³ ë¦¬ê¹Œì§€ í¬í•¨í•˜ì—¬ ì €ì¥ í´ë”/ëª¨ë¸ëª…ì„ êµ¬ë¶„ (ì˜ˆ: name-QLORA-qna)
    save_name_with_suffix = f"{body.saveModelName}-{suffix}-{category}"
    _ensure_output_dir(save_name_with_suffix)

    # ì¤‘ë³µ ì‹¤í–‰ ì°¨ë‹¨ (ë½ íŒŒì¼)
    import fcntl
    out_dir = _ensure_output_dir(save_name_with_suffix)
    lock_path = os.path.join(out_dir, ".run.lock")
    lock_f = open(lock_path, "w")
    try:
        fcntl.flock(lock_f, fcntl.LOCK_EX | fcntl.LOCK_NB)
        lock_f.write(f"locked at {_now_utc().isoformat()}\n")
        lock_f.flush()
    except BlockingIOError:
        lock_f.close()
        raise BadRequestError(f"another fine-tune is already running for {save_name_with_suffix}")

    base_dir = _resolve_model_dir(body.baseModelName)
    if not _has_model_signature(base_dir):
        msg = f"base model not found: {base_dir}"
        logger.error(msg)
        _log_to_save_name(save_name_with_suffix, msg)
        raise BadRequestError(msg)

    job_id = f"ft-job-{uuid.uuid4().hex[:12]}"

    conn = get_db()
    try:
        train_path = _resolve_train_path(body.trainSetFile)
        dataset_id = _insert_dataset_if_needed(conn, train_path, category)
        # ì˜ˆì•½ ì‹œê°„ íŒŒì‹±
        scheduled_at_iso = None
        delay_sec = 0.0
        if body.startAt:
            try:
                scheduled_dt = datetime.fromisoformat(body.startAt)
                # timezone ì •ë³´ê°€ ì—†ìœ¼ë©´ KSTë¡œ ê°€ì •
                if scheduled_dt.tzinfo is None:
                    kst = timezone(timedelta(hours=9))
                    scheduled_dt = scheduled_dt.replace(tzinfo=kst)
                now_dt = _now_utc()
                delta = (scheduled_dt - now_dt).total_seconds()
                if delta > 1.0:
                    delay_sec = float(delta)
                    scheduled_at_iso = scheduled_dt.isoformat()
            except Exception:
                scheduled_at_iso = None
                delay_sec = 0.0

        _insert_job(
            conn, category, body, job_id, save_name_with_suffix, dataset_id,
            initial_status=("scheduled" if delay_sec > 1.0 else "queued"),
            scheduled_at=scheduled_at_iso,
        )
    except Exception as e:
        _log_to_save_name(save_name_with_suffix, f"db insert failed: {e}")
        logger.error(f"fine-tuning init failed: {e}")
        try:
            conn.close()
        except Exception:
            pass
        raise InternalServerError(f"fine-tuning init failed: {e}")
    finally:
        try:
            conn.close()
        except Exception:
            pass

    job = FineTuneJob(job_id=job_id, category=category, request=body.model_dump())

    # ğŸ”¹ ì¦‰ì‹œ ì‹¤í–‰ì´ë©´ ì˜ˆì•½(startAt)ì€ **ë¬´ì‹œ**í•´ì„œ ì¤‘ë³µ ì‹¤í–‰ì„ ì›ì²œ ì°¨ë‹¨
    if body.startNow:
        def _launch():
            _run_training_inline(job, save_name_with_suffix)
        t = threading.Thread(target=_launch, daemon=True)
        t.start()
        logger.info(
            f"Fine-tuning started immediately jobId={job.job_id} category={category} base={body.baseModelName} "
            f"save={save_name_with_suffix} tuning={body.tuningType or 'QLORA'}"
        )
        return {"jobId": job_id, "started": True}
    else:
        # ì˜ˆì•½ ì‹¤í–‰ ë˜ëŠ” íì—ë§Œ ë“±ë¡
        if body.startAt:
            # ì˜ˆì•½ ì‹¤í–‰
            def _launch():
                _run_training_inline(job, save_name_with_suffix)

            # ì˜ˆì•½ ë”œë ˆì´ ì¬ê³„ì‚°
            delay_sec = 0.0
            try:
                scheduled_dt = datetime.fromisoformat(body.startAt)
                # timezone ì •ë³´ê°€ ì—†ìœ¼ë©´ KSTë¡œ ê°€ì •
                if scheduled_dt.tzinfo is None:
                    kst = timezone(timedelta(hours=9))
                    scheduled_dt = scheduled_dt.replace(tzinfo=kst)
                now_dt = _now_utc()
                delta = (scheduled_dt - now_dt).total_seconds()
                if delta > 1.0:
                    delay_sec = float(delta)
            except Exception:
                delay_sec = 0.0

            if delay_sec > 1.0:
                t = threading.Timer(delay_sec, _launch)
                t.daemon = True
                t.start()
                logger.info(
                    f"Fine-tuning scheduled jobId={job.job_id} at {scheduled_dt.isoformat()} category={category} base={body.baseModelName} save={save_name_with_suffix}"
                )
            else:
                # ì˜ˆì•½ ì‹œê°„ì´ ì´ë¯¸ ì§€ë‚¬ìœ¼ë©´ ì¦‰ì‹œ ì‹¤í–‰
                t = threading.Thread(target=_launch, daemon=True)
                t.start()
                logger.info(
                    f"Fine-tuning started (scheduled time passed) jobId={job.job_id} category={category} base={body.baseModelName} save={save_name_with_suffix}"
                )
        else:
            # íì—ë§Œ ë“±ë¡ (ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ë‚˜ì¤‘ì— ì‹¤í–‰)
            try:
                conn2 = get_db()
                _update_job_status(conn2, job_id, "queued", extras={"reserved": True, "reservedAt": _now_local_str()})
            finally:
                try:
                    conn2.close()
                except Exception:
                    pass
            logger.info(
                f"Fine-tuning queued (not started) jobId={job.job_id} category={category} base={body.baseModelName} save={save_name_with_suffix}"
            )

    return {"jobId": job_id, "started": bool(body.startNow)}

def get_fine_tuning_status(job_id: str) -> Dict[str, Any]:
    conn = get_db()
    try:
        cur = conn.cursor()
        cur.execute("SELECT provider_job_id, status, metrics FROM fine_tune_jobs WHERE provider_job_id=?", (job_id,))
        row = cur.fetchone()
        if not row:
            return {"error": "job not found", "jobId": job_id}
        metrics = {}
        if row["metrics"]:
            try:
                metrics = json.loads(row["metrics"]) or {}
            except Exception:
                metrics = {}

        # Liveness: if status is running but heartbeat is stale, flip to failed
        row_status = row["status"]
        try:
            if row_status == "running":
                hb = metrics.get("heartbeatAt")
                if hb:
                    from dateutil import parser as dtparser  # type: ignore
                    last = dtparser.parse(hb)
                    now = _now_utc()
                    if (now - last).total_seconds() > FT_HEARTBEAT_TIMEOUT_SEC:
                        c2 = get_db()
                        try:
                            _update_job_status(c2, job_id, "failed", extras={"error": "stale heartbeat"})
                        finally:
                            c2.close()
                        row_status = "failed"
        except Exception:
            pass

        return {
            "jobId": row["provider_job_id"],
            "status": row_status,
            "learningProgress": int(metrics.get("learningProgress", 0)),
            "roughScore": int(metrics.get("roughScore", 0)),
            "rouge1F1": metrics.get("rouge1F1"),
            "saveProgress": int(metrics.get("saveProgress", 0)),
            "saveStage": metrics.get("saveStage"),
            "error": metrics.get("error"),
        }
    finally:
        conn.close()

def list_feedback_datasets() -> dict:
    """
    ./storage/train_data ì•ˆì—ì„œ íŒŒì¼ëª… íŒ¨í„´
    'feedback_{task}_p{prompt}.csv'ì— ë§¤ì¹­ë˜ëŠ” ëª¨ë“  CSVë¥¼ í…ŒìŠ¤í¬ë³„ë¡œ ë°˜í™˜.
    ë°˜í™˜ ê²½ë¡œëŠ” ìƒëŒ€ê²½ë¡œ('./storage/train_data/...')ë¥¼ ì œê³µí•œë‹¤.
    """
    REL_ROOT = "./storage/train_data"

    try:
        names = sorted(os.listdir(TRAIN_DATA_ROOT))
    except FileNotFoundError:
        names = []

    entries = []
    for name in names:
        m = _FEEDBACK_FILE_RE.match(name)
        if not m:
            continue
        task = m.group(1).lower()
        prompt = int(m.group(2))
        abs_path = os.path.join(TRAIN_DATA_ROOT, name)
        if not os.path.isfile(abs_path):
            continue
        st = os.stat(abs_path)
        mtime_dt = datetime.fromtimestamp(st.st_mtime)
        entries.append({
            "task": task,                               # qna | doc_gen | summary
            "file": name,                               # ex) feedback_qna_p0.csv
            "prompt": prompt,                           # ì •ìˆ˜ pê°’
            "bytes": st.st_size,                        # íŒŒì¼ í¬ê¸°
            "mtime": mtime_dt.strftime("%Y-%m-%d %H:%M:%S"),
            "mtime_iso": mtime_dt.isoformat(),
            "path": f"{REL_ROOT}/{name}",               # ìƒëŒ€ê²½ë¡œ (Docker ê³ ë ¤)
            "downloadUrl": f"/v1/admin/llm/feedback-datasets?file={quote_plus(name)}",
        })

    groups = {"qna": [], "doc_gen": [], "summary": []}
    for e in entries:
        groups[e["task"]].append(e)
    for k in groups:
        groups[k].sort(key=lambda x: x["prompt"])

    return {
        "root": REL_ROOT,
        "pattern": "feedback_{task}_p{prompt}.csv",
        "total": len(entries),
        "counts": {k: len(v) for k, v in groups.items()},
        "groups": groups,
    }

def resolve_feedback_download(file: str) -> tuple[str, str]:
    """
    ë‹¤ìš´ë¡œë“œìš© íŒŒì¼ ê²€ì¦/í•´ê²°:
    - basenameë§Œ í—ˆìš© (ê²½ë¡œ íƒˆì¶œ ë°©ì§€)
    - íŒŒì¼ëª… íŒ¨í„´ í™•ì¸
    - ./storage/train_data ë‚´ë¶€ ì¡´ì¬ í™•ì¸
    ì„±ê³µ ì‹œ: (abs_path, filename) ë°˜í™˜
    """
    if os.path.basename(file) != file:
        raise BadRequestError("basenameë§Œ í—ˆìš©í•©ë‹ˆë‹¤.")
    m = _FEEDBACK_FILE_RE.match(file)
    if not m:
        raise BadRequestError("ì˜ëª»ëœ íŒŒì¼ëª… í˜•ì‹ì…ë‹ˆë‹¤. (feedback_{task}_p{n}.csv)")
    abs_path = os.path.join(TRAIN_DATA_ROOT, file)
    if not os.path.isfile(abs_path):
        # ë¼ìš°í„°ì—ì„œ 404ë¡œ ë§¤í•‘í•˜ê¸° ìœ„í•´ í‘œì¤€ ì˜ˆì™¸ ì‚¬ìš©
        raise FileNotFoundError(f"not found in {TRAIN_DATA_ROOT}: {file}")
    return abs_path, file